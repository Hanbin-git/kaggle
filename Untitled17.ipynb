{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1AxpdiDrZkEHwJQOjznN5LtpU6qrxT07Y",
      "authorship_tag": "ABX9TyOZCLWh3ks6XgWB82Qutam/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hanbin-git/kaggle/blob/main/Untitled17.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 라이브러리 설치  (재시작되면 다시 설치해야 함)\n",
        "!pip install -q wandb timm==0.9.12 torchvision --upgrade\n",
        "!pip install -q transformers==4.41.2\n"
      ],
      "metadata": {
        "id": "CsJ7vyz-c-r4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Drive 마운트 + 데이터 unzip\n",
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os, zipfile, shutil\n",
        "SRC_ZIP=\"/content/drive/MyDrive/open.zip\"; DST_DIR=\"/content/open\"\n",
        "if os.path.exists(DST_DIR): shutil.rmtree(DST_DIR)\n",
        "shutil.copy(SRC_ZIP, \"/content/open.zip\")\n",
        "with zipfile.ZipFile(\"/content/open.zip\") as z: z.extractall(DST_DIR)\n"
      ],
      "metadata": {
        "id": "uK5TyM3GdOyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yaml_text = \"\"\"\n",
        "SEED: 42\n",
        "IMG_SIZE: 456\n",
        "BATCH_SIZE: 48\n",
        "EPOCHS: 40\n",
        "LEARNING_RATE: 0.0003\n",
        "patience: 8\n",
        "model: \"tf_efficientnet_b5\"\n",
        "ema_decay: 0.9997\n",
        "stochastic_depth: 0.2\n",
        "train_root: \"/content/open/train\"\n",
        "test_root : \"/content/open/test\"\n",
        "\"\"\"\n",
        "with open(\"config.yaml\", \"w\") as f:\n",
        "    f.write(yaml_text)\n",
        "print(\"✅ config.yaml 저장 완료\")\n"
      ],
      "metadata": {
        "id": "_P9UuX_fd8BJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"dataload.py\", \"w\") as f:\n",
        "    f.write('''\n",
        "import os, yaml, random, math\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler\n",
        "from torchvision.transforms import v2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "\n",
        "# ───────── config 로드 ─────────\n",
        "with open(\"config.yaml\") as cf:\n",
        "    CFG = yaml.safe_load(cf)\n",
        "\n",
        "# ───────── 시드 고정 ─────────\n",
        "def seed_everything(seed:int):\n",
        "    random.seed(seed); np.random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    torch.manual_seed(seed); torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# ───────── Dataset 정의 ─────────\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, is_test=False):\n",
        "        self.root_dir, self.transform, self.is_test = root_dir, transform, is_test\n",
        "        self.samples = []\n",
        "        if is_test:\n",
        "            for f in sorted(os.listdir(root_dir)):\n",
        "                if f.lower().endswith(('.jpg','.jpeg','.png')):\n",
        "                    self.samples.append((os.path.join(root_dir, f),))\n",
        "        else:\n",
        "            self.classes = sorted(os.listdir(root_dir))\n",
        "            self.class_to_idx = {c:i for i,c in enumerate(self.classes)}\n",
        "            for cls in self.classes:\n",
        "                cls_dir = os.path.join(root_dir, cls)\n",
        "                for f in os.listdir(cls_dir):\n",
        "                    if f.lower().endswith(('.jpg','.jpeg','.png')):\n",
        "                        self.samples.append((os.path.join(cls_dir, f), self.class_to_idx[cls]))\n",
        "\n",
        "    def __len__(self): return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.is_test:\n",
        "            path = self.samples[idx][0]\n",
        "            img  = Image.open(path).convert(\"RGB\")\n",
        "            return self.transform(img) if self.transform else img\n",
        "        path, label = self.samples[idx]\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        if self.transform: img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "# ───────── Transform ─────────\n",
        "def get_transforms():\n",
        "    train_tf = v2.Compose([\n",
        "        v2.ToImage(), v2.ToDtype(torch.float32, scale=True),\n",
        "        v2.RandomResizedCrop((CFG[\"IMG_SIZE\"], CFG[\"IMG_SIZE\"]), scale=(0.8,1.0)),\n",
        "        v2.RandomHorizontalFlip(),\n",
        "        v2.ColorJitter(0.3,0.3,0.3,0.1),\n",
        "        v2.RandAugment(num_ops=2, magnitude=7),\n",
        "        v2.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
        "    ])\n",
        "    val_tf = v2.Compose([\n",
        "        v2.ToImage(), v2.ToDtype(torch.float32, scale=True),\n",
        "        v2.Resize((CFG[\"IMG_SIZE\"], CFG[\"IMG_SIZE\"])),\n",
        "        v2.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
        "    ])\n",
        "    return train_tf, val_tf\n",
        "\n",
        "# ───────── Loader ─────────\n",
        "def get_loaders():\n",
        "    seed_everything(CFG[\"SEED\"])\n",
        "\n",
        "    full = CustomImageDataset(CFG[\"train_root\"])\n",
        "    labels = [y for _, y in full.samples]\n",
        "    tr_idx, val_idx = train_test_split(range(len(labels)),\n",
        "                                       test_size=0.2,\n",
        "                                       stratify=labels,\n",
        "                                       random_state=CFG[\"SEED\"])\n",
        "\n",
        "    tr_tf, val_tf = get_transforms()\n",
        "    train_ds = Subset(CustomImageDataset(CFG[\"train_root\"], tr_tf), tr_idx)\n",
        "    val_ds   = Subset(CustomImageDataset(CFG[\"train_root\"], val_tf), val_idx)\n",
        "\n",
        "    # ---- 불균형 보정 Sampler\n",
        "    cls_cnt   = np.bincount([labels[i] for i in tr_idx])\n",
        "    cls_wt    = 1. / (cls_cnt + 1e-6)\n",
        "    sample_wt = [cls_wt[labels[i]] for i in tr_idx]\n",
        "    sampler   = WeightedRandomSampler(sample_wt, num_samples=len(tr_idx), replacement=True)\n",
        "\n",
        "    # ---- DataLoader 파라미터\n",
        "    n_workers   = CFG.get(\"NUM_WORKERS\", 8)\n",
        "    prefetch    = CFG.get(\"PREFETCH\", 4)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds, batch_size=CFG[\"BATCH_SIZE\"], sampler=sampler,\n",
        "        num_workers=n_workers, pin_memory=True, persistent_workers=True,\n",
        "        prefetch_factor=prefetch\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_ds, batch_size=CFG[\"BATCH_SIZE\"], shuffle=False,\n",
        "        num_workers=max(1, n_workers//2), pin_memory=True,\n",
        "        persistent_workers=True, prefetch_factor=prefetch\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        CustomImageDataset(CFG[\"test_root\"], val_tf, is_test=True),\n",
        "        batch_size=CFG[\"BATCH_SIZE\"], shuffle=False,\n",
        "        num_workers=max(1, n_workers//2), pin_memory=True,\n",
        "        persistent_workers=True, prefetch_factor=prefetch\n",
        "    )\n",
        "    return train_loader, val_loader, test_loader, full.classes\n",
        "''')\n"
      ],
      "metadata": {
        "id": "VGHUPFuKeYqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train.py\n",
        "import os, yaml, wandb, torch, timm, gc\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from transformers import get_cosine_schedule_with_warmup\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from dataload import get_loaders, CFG\n",
        "\n",
        "# ──────────────────────── 0. 환경 설정 ────────────────────────\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if device.type == \"cuda\":\n",
        "    print(f\"🚀 Using GPU : {torch.cuda.get_device_name(0)}\")\n",
        "    torch.backends.cudnn.benchmark = True   # 속도 ↑\n",
        "    torch.cuda.empty_cache()                # ↳ 캐시 정리\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "# ──────────────────────── 1. 모델 정의 ────────────────────────\n",
        "class BaseModel(nn.Module):\n",
        "    def __init__(self, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.backbone = timm.create_model(\n",
        "            CFG[\"model\"],\n",
        "            pretrained=True,\n",
        "            num_classes=num_classes,\n",
        "            drop_path_rate=CFG.get(\"stochastic_depth\", 0.0)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)\n",
        "\n",
        "# ──────────────────────── 2. 학습 함수 ────────────────────────\n",
        "def main():\n",
        "    run = wandb.init(project=\"effb5-color\", config=CFG)\n",
        "    train_loader, val_loader, _, class_names = get_loaders()\n",
        "    model = BaseModel(len(class_names)).to(device)\n",
        "\n",
        "    # 파라미터 그룹 설정: 분류 헤드(10×lr)\n",
        "    head, body = [], []\n",
        "    for n, p in model.named_parameters():\n",
        "        (head if \"classifier\" in n or \"head\" in n else body).append(p)\n",
        "    optimizer = optim.AdamW([\n",
        "        {\"params\": body, \"lr\": CFG[\"LEARNING_RATE\"]},\n",
        "        {\"params\": head, \"lr\": CFG[\"LEARNING_RATE\"] * 10}\n",
        "    ], weight_decay=0.05)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    scaler    = GradScaler()\n",
        "\n",
        "    # 학습률 스케줄러\n",
        "    total_steps = len(train_loader) * CFG[\"EPOCHS\"]\n",
        "    scheduler   = get_cosine_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        int(total_steps * 0.1),         # warm-up 10 %\n",
        "        total_steps\n",
        "    )\n",
        "\n",
        "    # 파라미터 수 출력\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    train_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"📊 Params: {total_params:,} (trainable {train_params:,})\")\n",
        "\n",
        "    best_loss, bad_epochs = float(\"inf\"), 0\n",
        "\n",
        "    for epoch in range(CFG[\"EPOCHS\"]):\n",
        "        # ─── Train ────────────────────────────────────────────\n",
        "        model.train(); running = 0\n",
        "        for imgs, labels in tqdm(train_loader, desc=f\"[Train {epoch+1}]\"):\n",
        "            imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            with autocast():\n",
        "                loss = criterion(model(imgs), labels)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer); scaler.update()\n",
        "            scheduler.step()\n",
        "            running += loss.item()\n",
        "        train_loss = running / len(train_loader)\n",
        "\n",
        "        # ─── Validation ──────────────────────────────────────\n",
        "        model.eval(); val_loss = 0; correct = total = 0\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in tqdm(val_loader, desc=f\"[Val   {epoch+1}]\"):\n",
        "                imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "                with autocast():\n",
        "                    outs  = model(imgs)\n",
        "                    loss  = criterion(outs, labels)\n",
        "                val_loss += loss.item()\n",
        "                correct  += (outs.argmax(1) == labels).sum().item()\n",
        "                total    += labels.size(0)\n",
        "        val_loss /= len(val_loader)\n",
        "        val_acc   = 100 * correct / total\n",
        "\n",
        "        # ─── GPU 메모리 모니터링 ──────────────────────────────\n",
        "        if device.type == \"cuda\":\n",
        "            mem_alloc   = torch.cuda.memory_allocated() / 1024**2\n",
        "            peak_alloc  = torch.cuda.max_memory_allocated() / 1024**2\n",
        "            print(f\"🧠 GPU Mem  : {mem_alloc:.1f} MB (peak {peak_alloc:.1f} MB)\")\n",
        "\n",
        "        # ─── 로깅 & EarlyStopping ────────────────────────────\n",
        "        wandb.log({\"epoch\": epoch+1,\n",
        "                   \"train_loss\": train_loss,\n",
        "                   \"val_loss\":   val_loss,\n",
        "                   \"val_acc\":    val_acc,\n",
        "                   \"gpu_mem_MB\": mem_alloc if device.type==\"cuda\" else 0})\n",
        "\n",
        "        if val_loss < best_loss:                     # improvement\n",
        "            best_loss, bad_epochs = val_loss, 0\n",
        "            torch.save(model.state_dict(), \"best_model.pth\")\n",
        "            print(f\"📦  Best model saved (val_loss={val_loss:.4f})\")\n",
        "        else:\n",
        "            bad_epochs += 1\n",
        "            print(f\"⚠️  No improvement {bad_epochs}/{CFG['patience']}\")\n",
        "            if bad_epochs >= CFG[\"patience\"]:\n",
        "                print(f\"🛑 Early Stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    # 훈련 종료 후 캐시 정리\n",
        "    if device.type == \"cuda\":\n",
        "        torch.cuda.empty_cache(); gc.collect()\n",
        "        print(\"♻️  GPU cache cleared\")\n",
        "\n",
        "    run.finish()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "0khwp472gFkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. 학습 실행  ← 주석 제거!\n",
        "%cd /content          # 주석 없이 정확히\n",
        "import os; os.environ[\"WANDB_MODE\"]=\"offline\"   # WandB 로그인 생략\n",
        "from train import main\n",
        "main()\n"
      ],
      "metadata": {
        "id": "cw1Aq4OSecgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) 추론 & 제출 파일 생성\n",
        "!python inference.py\n",
        "!head submission_tta.csv    # 결과 미리보기\n"
      ],
      "metadata": {
        "id": "lotQRmv_gJZo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}