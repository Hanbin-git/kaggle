{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1HuAO5Kys89GsFnFfOFan853-bSVE68l-",
      "authorship_tag": "ABX9TyPZZ6aAGeCHaxXouY8aYo7W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hanbin-git/kaggle/blob/main/feature_color%EB%A1%9C_%EC%A0%84%EB%9E%B5_%EC%88%98%EC%A0%952025_06_09.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import zipfile\n",
        "\n",
        "# 1. Driveì—ì„œ ë¡œì»¬ë¡œ ë³µì‚¬ (ë¹ ë¦„)\n",
        "shutil.copy(\"/content/drive/MyDrive/open.zip\", \"/content/open.zip\")\n",
        "\n",
        "# 2. ì••ì¶• í’€ê¸°\n",
        "with zipfile.ZipFile(\"/content/open.zip\", \"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"/content/\")\n",
        "\n",
        "# 3. í™•ì¸\n",
        "!ls /content/train\n"
      ],
      "metadata": {
        "id": "VGLGwzmFf9U8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53668e8f-8c12-4b5c-c2bb-8bf94c3fd970"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì»¨í‹°ë„¨íƒˆ_10ì„¸ëŒ€_2017_2019\t     A_í´ëž˜ìŠ¤_W177_2020_2025\n",
            "ì–´ì½”ë“œ_10ì„¸ëŒ€_2018_2022\t\t     B_í´ëž˜ìŠ¤_W246_2013_2018\n",
            "1ì‹œë¦¬ì¦ˆ_F20_2013_2015\t\t     ë‰´_ìŠ¤íƒ€ì¼_ì½”ëž€ë„_C_2017_2019\n",
            "1ì‹œë¦¬ì¦ˆ_F20_2016_2019\t\t     í”„ë¦¬ìš°ìŠ¤_C_2018_2020\n",
            "1ì‹œë¦¬ì¦ˆ_F40_2020_2024\t\t     ë‰´_CC_2012_2016\n",
            "ê·¸ëžœë“œì¹´ë‹ˆë°œ_2006_2010\t\t     CLA_í´ëž˜ìŠ¤_C117_2014_2019\n",
            "2008_2015_2017\t\t\t     CLA_í´ëž˜ìŠ¤_C118_2020_2025\n",
            "ì—ì¿ ìŠ¤_ì‹ í˜•_2010_2015\t\t     CLE_í´ëž˜ìŠ¤_C236_2024_2025\n",
            "íŒŒë‚˜ë©”ë¼_2010_2016\t\t     CLS_í´ëž˜ìŠ¤_C257_2019_2023\n",
            "ë‰´_ì œíƒ€_2011_2016\t\t     CLS_í´ëž˜ìŠ¤_W218_2012_2017\n",
            "ë‰´_ì¹´ì´ì—”_2011_2018\t\t     ì•„ë°˜ë–¼_í•˜ì´ë¸Œë¦¬ë“œ_CN7_2021_2023\n",
            "ì—‘ì„¼íŠ¸_ì‹ í˜•_2011_2019\t\t     ì•„ë°˜ë–¼_CN7_2021_2023\n",
            "ìŠ¤íŒŒí¬_2012_2015\t\t     ë”_ë‰´_ì•„ë°˜ë–¼_CN7_2023_2025\n",
            "ì¿ í¼_ì»¨íŠ¸ë¦¬ë§¨_2012_2015\t\t     CT6_2016_2018\n",
            "ì˜¬_ë‰´_ëª¨ë‹_2012_2015\t\t     C_í´ëž˜ìŠ¤_W204_2008_2015\n",
            "ì•„ë² ì˜¤_2012_2016\t\t     C_í´ëž˜ìŠ¤_W205_2015_2021\n",
            "ë§ë¦¬ë¶€_2012_2016\t\t     C_í´ëž˜ìŠ¤_W206_2022_2024\n",
            "ë‰´_í‹°êµ¬ì•ˆ_2012_2016\t\t     ì œë„¤ì‹œìŠ¤_DH_2014_2016\n",
            "ë ˆì´_2012_2017\t\t\t     ì˜ë‚˜íƒ€_DN8_2020_2023\n",
            "ì˜¬ëž€ë„_2012_2018\t\t     ì˜ë‚˜íƒ€_ë””_ì—£ì§€_DN8_2024_2025\n",
            "ë”_ë‰´_íŒŒì‚¬íŠ¸_2012_2019\t\t     e_íŠ¸ë¡ _2020_2023\n",
            "íŠ¸ëž™ìŠ¤_2013_2016\t\t     E_PACE_2018_2020\n",
            "ì½°íŠ¸ë¡œí¬ë¥´í…Œ_2014_2016\t\t     EQ900_2016_2018\n",
            "ë”_ë‰´_ì•„ë°˜ë–¼_2014_2016\t\t     EQA_H243_2021_2024\n",
            "ë§ˆì¹¸_2014_2018\t\t\t     EQE_V295_2022_2024\n",
            "ê·¸ëžœë“œ_ì²´ë¡œí‚¤_2014_2020\t\t     EQS_V297_2022_2023\n",
            "ê¸°ë¸”ë¦¬_2014_2023\t\t     ë‰´_ES300h_2013_2015\n",
            "ë”_ë‰´_ëª¨ë‹_2015_2016\t\t     ë‰´_ES300h_2016_2018\n",
            "ë ˆë‹ˆê²Œì´ë“œ_2015_2017\t\t     ES300h_7ì„¸ëŒ€_2019_2026\n",
            "ì˜¬_ë‰´_ì˜ë Œí† _2015_2017\t\t     ë””_ì˜¬ë‰´ë‹ˆë¡œEV_2023_2024\n",
            "ì•„ìŠ¬ëž€_2015_2018\t\t     ë”_ê¸°ì•„_ë ˆì´_EV_2024_2025\n",
            "í‹°ë³¼ë¦¬_2015_2018\t\t     EV6_2022_2024\n",
            "ë””ìŠ¤ì»¤ë²„ë¦¬_ìŠ¤í¬ì¸ _2015_2019\t     EV9_2024_2025\n",
            "ì˜¬_ë‰´_ì¹´ë‹ˆë°œ_2015_2019\t\t     E_í´ëž˜ìŠ¤_W212_2010_2016\n",
            "ì—ìŠ¤ì»¬ë ˆì´ë“œ_2015_2020\t\t     E_í´ëž˜ìŠ¤_W213_2017_2020\n",
            "ë¨¸ìŠ¤íƒ±_2015_2023\t\t     E_í´ëž˜ìŠ¤_W213_2021_2023\n",
            "ìµìŠ¤í”Œë¡œëŸ¬_2016_2017\t\t     E_í´ëž˜ìŠ¤_W214_2024_2025\n",
            "ê·¸ëžœë“œ_ìŠ¤íƒ€ë ‰ìŠ¤_2016_2018\t     F150_2004_2021\n",
            "ì‹¼íƒ€íŽ˜_ë”_í”„ë¼ìž„_2016_2018\t     F_PACE_2017_2019\n",
            "ë”_ë„¥ìŠ¤íŠ¸_ìŠ¤íŒŒí¬_2016_2018\t     G4_ë ‰ìŠ¤í„´_2018_2020\n",
            "ë”_ë‰´_ë§¥ìŠ¤í¬ë£¨ì¦ˆ_2016_2018\t     G70_2018_2020\n",
            "ë”_ë‰´_ì½”ëž€ë„_ìŠ¤í¬ì¸ _2016_2018\t     ë”_ë‰´_G70_2021_2025\n",
            "ë ˆì¸ì§€ë¡œë²„_ì´ë³´í¬_2016_2019\t     G80_2017_2020\n",
            "ì•„ì´ì˜¤ë‹‰_í•˜ì´ë¸Œë¦¬ë“œ_2016_2019\t     ë”_ì˜¬ë‰´G80_2021_2024\n",
            "í‹°ë³¼ë¦¬_ì—ì–´_2016_2019\t\t     ë‰´_G80_2025_2026\n",
            "ìž„íŒ”ë¼_2016_2019\t\t     G80_RG3_2021_2023\n",
            "ì¿ í¼_ì»¨íŠ¸ë¦¬ë§¨_2016_2024\t\t     G80_RG3_2025\n",
            "ì¿ í¼_ì»¨ë²„í„°ë¸”_2016_2024\t\t     G90_2019_2022\n",
            "ì¿ í¼_í´ëŸ½ë§¨_2016_2024\t\t     G90_RS4_2022_2025\n",
            "ì•Œí‹°ë§ˆ_2017_2018\t\t     GLA_í´ëž˜ìŠ¤_H247_2020_2025\n",
            "ì˜¬_ë‰´_ì¹´ë§ˆë¡œ_2017_2018\t\t     GLA_í´ëž˜ìŠ¤_X156_2015_2019\n",
            "ì˜¬_ë‰´_ë§ë¦¬ë¶€_2017_2018\t\t     GLB_í´ëž˜ìŠ¤_X247_2020_2023\n",
            "ë‹ˆë¡œ_2017_2019\t\t\t     GLC_í´ëž˜ìŠ¤_X253_2017_2019\n",
            "ë”_ë‰´_ëª¨í•˜ë¹„_2017_2019\t\t     GLC_í´ëž˜ìŠ¤_X253_2020_2022\n",
            "ì½°íŠ¸ë¡œí¬ë¥´í…Œ_2017_2022\t\t     GLC_í´ëž˜ìŠ¤_X253_2023\n",
            "ë¥´ë°˜ë–¼_2017_2022\t\t     GLC_í´ëž˜ìŠ¤_X254_2023_2025\n",
            "ë”_ë‰´_íŠ¸ëž™ìŠ¤_2017_2022\t\t     GLE_í´ëž˜ìŠ¤_W166_2016_2018\n",
            "ë ˆì¸ì§€ë¡œë²„_ë²¨ë¼_2018_2019\t     GLE_í´ëž˜ìŠ¤_W167_2019_2024\n",
            "ìµìŠ¤í”Œë¡œëŸ¬_2018_2019\t\t     GLS_í´ëž˜ìŠ¤_X166_2017_2019\n",
            "í‹°ë³¼ë¦¬_ì•„ë¨¸_2018_2019\t\t     GLS_í´ëž˜ìŠ¤_X167_2020_2024\n",
            "ì˜ë‚˜íƒ€_ë‰´_ë¼ì´ì¦ˆ_2018_2019\t     ê·¸ëžœì €_GN7_2023_2025\n",
            "ìŠ¤í† ë‹‰_2018_2020\t\t     ì»¨í‹°ë„¨íƒˆ_GT_2ì„¸ëŒ€_2012_2017\n",
            "ìŠ¤íŒ…ì–´_2018_2020\t\t     ì»¨í‹°ë„¨íƒˆ_GT_3ì„¸ëŒ€_2018_2023\n",
            "ì½”ë‚˜_2018_2020\t\t\t     íŒŒì‚¬íŠ¸_GT_B8_2018_2022\n",
            "ë”_ë‰´_ì˜ë Œí† _2018_2020\t\t     GV70_2021_2023\n",
            "ë ‰ìŠ¤í„´_ìŠ¤í¬ì¸ _2018_2021\t\t     ì¼ë ‰íŠ¸ë¦¬íŒŒì´ë“œ_GV70_2022_2024\n",
            "ë”_ë‰´_ê·¸ëžœë“œ_ìŠ¤íƒ€ë ‰ìŠ¤_2018_2021      GV80_2020_2022\n",
            "ë”_ë‰´_ë ˆì´_2018_2022\t\t     ë‰´_GV80_2024_2025\n",
            "í‹°êµ¬ì•ˆ_ì˜¬ìŠ¤íŽ˜ì´ìŠ¤_2018_2023\t     GV80_2024_2025\n",
            "ì•„í…Œì˜¨_2018_2023\t\t     G_í´ëž˜ìŠ¤_W463_2009_2017\n",
            "ë„¥ì˜_2018_2024\t\t\t     G_í´ëž˜ìŠ¤_W463b_2019_2025\n",
            "ë ‰ìŠ¤í„´_ìŠ¤í¬ì¸ _ì¹¸_2019_2020\t     ê·¸ëžœì €_HG_2011_2014\n",
            "ë”_ë‰´_ì¹´ë‹ˆë°œ_2019_2020\t\t     ê·¸ëžœì €_HG_2015_2017\n",
            "ë§ˆì¹¸_2019_2021\t\t\t     i30_PD_2017_2018\n",
            "íŒ°ë¦¬ì„¸ì´ë“œ_2019_2022\t\t     i4_2022_2024\n",
            "ìŠ¤í¬í‹°ì§€_ë”_ë³¼ë“œ_2019_2022\t     ê·¸ëžœì €_IG_2017_2019\n",
            "ë”_ë‰´_ë§ë¦¬ë¶€_2019_2022\t\t     ë”_ë‰´_ê·¸ëžœì €_IG_2020_2023\n",
            "ë”_ë‰´_ìŠ¤íŒŒí¬_2019_2022\t\t     iX_2022_2024\n",
            "ë ˆë‹ˆê²Œì´ë“œ_2019_2023\t\t     ì˜¬_ë‰´_ëª¨ë‹_JA_2017_2020\n",
            "ë·°í‹°í’€_ì½”ëž€ë„_2019_2024\t\t     ëª¨ë‹_ì–´ë°˜_JA_2021_2023\n",
            "ë”_ë‰´_ì•„ì´ì˜¤ë‹‰_í•˜ì´ë¸Œë¦¬ë“œ_2020\t     ë”_ë‰´_ëª¨ë‹_JA_2024_2025\n",
            "ì½œë¡œë¼ë„_2020_2020\t\t     ëž­ê¸€ëŸ¬_JK_2009_2017\n",
            "ì½”ì„¸ì–´_2020_2022\t\t     ëž­ê¸€ëŸ¬_JL_2018_2024\n",
            "ë”_ë‰´_ë‹ˆë¡œ_2020_2022\t\t     ë²¨ë¡œìŠ¤í„°_JS_2018_2020\n",
            "íŠ¸ëž˜ë²„ìŠ¤_2020_2023\t\t     ê¸€ëž˜ë””ì—ì´í„°_JT_2020_2023\n",
            "ì…€í† ìŠ¤_2020_2023\t\t     K3_2013_2015\n",
            "ë² ë¦¬_ë‰´_í‹°ë³¼ë¦¬_2020_2023\t     ë”_ë‰´_K3_2016_2018\n",
            "ëª¨í•˜ë¹„_ë”_ë§ˆìŠ¤í„°_2020_2024\t     ì˜¬_ë‰´_K3_2019_2021\n",
            "ë² ë‰´_2020_2024\t\t\t     ë”_ë‰´_K3_2ì„¸ëŒ€_2022_2024\n",
            "íŠ¸ë ˆì¼ë¸”ë ˆì´ì €_2021_2022\t     K5_2ì„¸ëŒ€_2016_2018\n",
            "í‹°ë³¼ë¦¬_ì—ì–´_2021_2022\t\t     ë”_ë‰´_K5_2ì„¸ëŒ€_2019_2020\n",
            "ë¦¬ì–¼_ë‰´_ì½œë¡œë¼ë„_2021_2022\t     K5_3ì„¸ëŒ€_í•˜ì´ë¸Œë¦¬ë“œ_2020_2022\n",
            "ìŠ¤íŒ…ì–´_ë§ˆì´ìŠ¤í„°_2021_2023\t     K5_í•˜ì´ë¸Œë¦¬ë“œ_3ì„¸ëŒ€_2020_2023\n",
            "ë”_ì˜¬ë‰´íˆ¬ì‹¼_í•˜ì´ë¸Œë¦¬ë“œ_2021_2023     K5_3ì„¸ëŒ€_2020_2023\n",
            "ë”_ë‰´_ì‹¼íƒ€íŽ˜_2021_2023\t\t     ë”_ë‰´_K5_í•˜ì´ë¸Œë¦¬ë“œ_3ì„¸ëŒ€_2023_2025\n",
            "ë”_ë‰´_ì½”ë‚˜_2021_2023\t\t     ë”_ë‰´_K5_3ì„¸ëŒ€_2024_2025\n",
            "íƒ€ì´ì¹¸_2021_2025\t\t     ë”_ë‰´_K7_2013_2016\n",
            "ë”_ë‰´_ë ‰ìŠ¤í„´_ìŠ¤í¬ì¸ _ì¹¸_2021_2025     ì˜¬_ë‰´_K7_2016_2019\n",
            "ë”_ë‰´_ë ‰ìŠ¤í„´_ìŠ¤í¬ì¸ _2021_2025\t     ì˜¬_ë‰´_K7_í•˜ì´ë¸Œë¦¬ë“œ_2017_2019\n",
            "ì˜¬_ë‰´_ë ‰ìŠ¤í„´_2021_2025\t\t     K7_í”„ë¦¬ë¯¸ì–´_í•˜ì´ë¸Œë¦¬ë“œ_2020_2021\n",
            "ìºìŠ¤í¼_2022_2024\t\t     K7_í”„ë¦¬ë¯¸ì–´_2020_2021\n",
            "ë§ˆì¹¸_2022_2024\t\t\t     K8_í•˜ì´ë¸Œë¦¬ë“œ_2022_2024\n",
            "ë””_ì˜¬_ë‰´_ìŠ¤í¬í‹°ì§€_2022_2024\t     K8_2022_2024\n",
            "ìŠ¤íƒ€ë¦¬ì•„_2022_2025\t\t     ë”_K9_2019_2021\n",
            "ë””_ì˜¬ë‰´ë‹ˆë¡œ_2022_2025\t\t     ë”_ë‰´_K9_2ì„¸ëŒ€_2022_2025\n",
            "ë”_ë‰´_ê¸°ì•„_ë ˆì´_2022_2025\t     ì²´ë¡œí‚¤_KL_2019_2023\n",
            "ë””_ì˜¬_ë‰´_ë‹ˆë¡œ_2022_2025\t\t     ë””íŽœë”_L663_2020_2025\n",
            "íŠ¸ë ˆì¼ë¸”ë ˆì´ì €_2023\t\t     LF_ì˜ë‚˜íƒ€_2015_2017\n",
            "ë”_ë‰´_íŒ°ë¦¬ì„¸ì´ë“œ_2023_2024\t     íŒ°ë¦¬ì„¸ì´ë“œ_LX3_2025\n",
            "í† ë ˆìŠ¤_2023_2025\t\t     M2_F87_2016_2021\n",
            "ë””_ì˜¬ë‰´ê·¸ëžœì €_2023_2025\t\t     M4_F82_2015_2020\n",
            "ë””_ì˜¬ë‰´ì½”ë‚˜_2023_2025\t\t     M5_F90_2018_2023\n",
            "ë”_ë‰´_ì…€í† ìŠ¤_2023_2025\t\t     ì•„ë°˜ë–¼_MD_2011_2014\n",
            "íŠ¸ëž™ìŠ¤_í¬ë¡œìŠ¤ì˜¤ë²„_2024_2025\t     MKC_2015_2018\n",
            "ë””_ì˜¬ë‰´ì‹¼íƒ€íŽ˜_2024_2025\t\t     ë‰´_MKZ_2017_2020\n",
            "ê·¸ëž‘_ì½œë ˆì˜¤ìŠ¤_2025\t\t     ì‹¼íƒ€íŽ˜_MX5_2024_2025\n",
            "ë ˆì¸ì§€ë¡œë²„_ìŠ¤í¬ì¸ _2ì„¸ëŒ€_2013_2017    ì•„ë°˜ë–¼_N_2022_2023\n",
            "ë ˆì¸ì§€ë¡œë²„_ìŠ¤í¬ì¸ _2ì„¸ëŒ€_2018_2022    New_XF_2012_2015\n",
            "ì»´íŒ¨ìŠ¤_2ì„¸ëŒ€_2018_2022\t\t     íˆ¬ì‹¼_NX4_2021_2023\n",
            "ë ˆì¸ì§€ë¡œë²„_ì´ë³´í¬_2ì„¸ëŒ€_2020_2022    ë”_ë‰´_íˆ¬ì‹¼_NX4_2023_2025\n",
            "ë””ìŠ¤ì»¤ë²„ë¦¬_ìŠ¤í¬ì¸ _2ì„¸ëŒ€_2020_2025    ì¹´ì´ì—”_PO536_2019_2023\n",
            "ì—ë¹„ì—ì´í„°_2ì„¸ëŒ€_2020_2025\t     Q30_2017_2019\n",
            "ë ˆì¸ì§€ë¡œë²„_ì´ë³´í¬_2ì„¸ëŒ€_2023_2024    Q3_F3_2020_2024\n",
            "ì•¡í‹°ì–¸_2ì„¸ëŒ€_2025\t\t     Q50_2014_2017\n",
            "2ì‹œë¦¬ì¦ˆ_ê·¸ëž€ì¿ íŽ˜_F44_2020_2024\t     Q5_FY_2020\n",
            "2ì‹œë¦¬ì¦ˆ_ì•¡í‹°ë¸Œ_íˆ¬ì–´ëŸ¬_F45_2019_2021  Q5_FY_2021_2024\n",
            "2ì‹œë¦¬ì¦ˆ_ì•¡í‹°ë¸Œ_íˆ¬ì–´ëŸ¬_U06_2022_2024  Q7_4M_2016_2019\n",
            "3008_2ì„¸ëŒ€_2018_2023\t\t     Q7_4M_2020_2023\n",
            "íŒŒì¼ëŸ¿_3ì„¸ëŒ€_2016_2018\t\t     Q8_4M_2020_2025\n",
            "ëª¨ë¸_3_2019_2022\t\t     QM3_2014_2017\n",
            "íˆ¬ì•„ë ‰_3ì„¸ëŒ€_2020_2023\t\t     ë‰´QM3_2018_2019\n",
            "ëª¨ë¸_3_2024_2025\t\t     ë‰´_QM5_2012_2014\n",
            "3ì‹œë¦¬ì¦ˆ_E90_2005_2012\t\t     QM6_2017_2019\n",
            "3ì‹œë¦¬ì¦ˆ_F30_2013_2018\t\t     ë”_ë‰´_QM6_2020_2023\n",
            "3ì‹œë¦¬ì¦ˆ_G20_2019_2022\t\t     ë‰´_QM6_2021_2023\n",
            "3ì‹œë¦¬ì¦ˆ_G20_2023_2025\t\t     ë”_ë‰´_QM6_2024_2025\n",
            "3ì‹œë¦¬ì¦ˆ_GT_F34_2014_2021\t     QX60_2016_2018\n",
            "ë””ìŠ¤ì»¤ë²„ë¦¬_4_2010_2016\t\t     ë‰´ì˜ë Œí† _R_2013_2014\n",
            "ë ˆì¸ì§€ë¡œë²„_4ì„¸ëŒ€_2014_2017\t     ë”_ë‰´ìŠ¤í¬í‹°ì§€R_2014_2016\n",
            "ëª¬ë°ì˜¤_4ì„¸ëŒ€_2015_2020\t\t     RAV4_2016_2018\n",
            "í”„ë¦¬ìš°ìŠ¤_4ì„¸ëŒ€_2016_2018\t     RAV4_5ì„¸ëŒ€_2019_2024\n",
            "ìŠ¤í¬í‹°ì§€_4ì„¸ëŒ€_2016_2018\t     S60_3ì„¸ëŒ€_2020_2024\n",
            "ë ˆì¸ì§€ë¡œë²„_4ì„¸ëŒ€_2018_2022\t     S90_2017_2020\n",
            "í”„ë¦¬ìš°ìŠ¤_4ì„¸ëŒ€_2019_2022\t     S90_2021_2025\n",
            "ì¹´ë‹ˆë°œ_4ì„¸ëŒ€_2021\t\t     SM3_ë„¤ì˜¤_2015_2019\n",
            "ì˜ë Œí† _4ì„¸ëŒ€_2021_2023\t\t     ë‰´_SM5_ìž„í”„ë ˆì…˜_2008_2010\n",
            "ì‹œì—ë‚˜_4ì„¸ëŒ€_2021_2024\t\t     ë‰´_SM5_í”Œëž˜í‹°ë„˜_2013_2014\n",
            "ì¹´ë‹ˆë°œ_4ì„¸ëŒ€_2022_2023\t\t     SM5_ë…¸ë°”_2015_2019\n",
            "ë”_ë‰´_ì¹´ë‹ˆë°œ_4ì„¸ëŒ€_2024_2025\t     SM6_2016_2020\n",
            "ë”_ë‰´_ì˜ë Œí† _4ì„¸ëŒ€_2024_2025\t     ë”_ë‰´_SM6_2021_2024\n",
            "ë¼ë¸Œ4_4ì„¸ëŒ€_2013_2018\t\t     SM7_ë‰´ì•„íŠ¸_2008_2011\n",
            "ë¼ë¸Œ4_5ì„¸ëŒ€_2019_2024\t\t     SM7_ë…¸ë°”_2015_2019\n",
            "4ì‹œë¦¬ì¦ˆ_F32_2014_2020\t\t     S_í´ëž˜ìŠ¤_W221_2006_2013\n",
            "4ì‹œë¦¬ì¦ˆ_G22_2021_2023\t\t     S_í´ëž˜ìŠ¤_W222_2014_2020\n",
            "4ì‹œë¦¬ì¦ˆ_G22_2024_2025\t\t     S_í´ëž˜ìŠ¤_W223_2021_2025\n",
            "5008_2ì„¸ëŒ€_2018_2019\t\t     ì½”ë‚˜_SX2_2023_2025\n",
            "5008_2ì„¸ëŒ€_2021_2024\t\t     ê·¸ëžœì €TG_2007_2008\n",
            "ë””ìŠ¤ì»¤ë²„ë¦¬_5_2017_2020\t\t     ì˜¬_ë‰´_íˆ¬ì‹¼_TL_2016_2018\n",
            "ì—ìŠ¤ì»¬ë ˆì´ë“œ_5ì„¸ëŒ€_2021_2024\t     ì˜¬_ë‰´_íˆ¬ì‹¼_TL_2019_2020\n",
            "ì•„ì´ì˜¤ë‹‰5_2022_2023\t\t     ì‹¼íƒ€íŽ˜_TM_2019_2020\n",
            "ë””ìŠ¤ì»¤ë²„ë¦¬_5_2022_2024\t\t     UX250h_2019_2024\n",
            "ìŠ¤í¬í‹°ì§€_5ì„¸ëŒ€_2022_2024\t     V40_2015_2018\n",
            "ë ˆì¸ì§€ë¡œë²„_5ì„¸ëŒ€_2023_2024\t     V60_í¬ë¡œìŠ¤ì»¨íŠ¸ë¦¬_2ì„¸ëŒ€_2020_2025\n",
            "5ì‹œë¦¬ì¦ˆ_F10_2010_2016\t\t     V90_í¬ë¡œìŠ¤ì»¨íŠ¸ë¦¬_2018_2024\n",
            "5ì‹œë¦¬ì¦ˆ_G30_2017_2023\t\t     ë‰´_ì²´ì–´ë§¨_W_2012_2016\n",
            "5ì‹œë¦¬ì¦ˆ_G60_2024_2025\t\t     ê·¸ëžœë“œ_ì²´ë¡œí‚¤_WL_2021_2023\n",
            "5ì‹œë¦¬ì¦ˆ_GT_F07_2010_2017\t     X1_F48_2016_2019\n",
            "ìµìŠ¤í”Œë¡œëŸ¬_6ì„¸ëŒ€_2020_2025\t     X1_F48_2020_2022\n",
            "ì•„ì´ì˜¤ë‹‰6_2023_2025\t\t     X1_U11_2023_2024\n",
            "6ì‹œë¦¬ì¦ˆ_F12_2011_2018\t\t     X2_F39_2018_2023\n",
            "6ì‹œë¦¬ì¦ˆ_GT_G32_2018_2020\t     X3_G01_2018_2021\n",
            "6ì‹œë¦¬ì¦ˆ_GT_G32_2021_2024\t     X3_G01_2022_2024\n",
            "ë°•ìŠ¤í„°_718_2017_2024\t\t     X4_F26_2015_2018\n",
            "718_ì¹´ì´ë§¨_2017_2024\t\t     X4_G02_2019_2021\n",
            "718_ë°•ìŠ¤í„°_2017_2024\t\t     X4_G02_2022_2025\n",
            "ê³¨í”„_7ì„¸ëŒ€_2013_2016\t\t     X5_F15_2014_2018\n",
            "7ì‹œë¦¬ì¦ˆ_F01_2009_2015\t\t     X5_G05_2019_2023\n",
            "7ì‹œë¦¬ì¦ˆ_G11_2016_2018\t\t     X5_G05_2024_2025\n",
            "7ì‹œë¦¬ì¦ˆ_G11_2019_2022\t\t     X6_F16_2015_2019\n",
            "7ì‹œë¦¬ì¦ˆ_G70_2023_2025\t\t     X6_G06_2020_2023\n",
            "8ì‹œë¦¬ì¦ˆ_G15_2020_2024\t\t     X6_G06_2024_2025\n",
            "911_2003_2019\t\t\t     X7_G07_2019_2022\n",
            "911_992_2020_2024\t\t     X7_G07_2023_2025\n",
            "íŒŒë‚˜ë©”ë¼_971_2017_2023\t\t     XC40_2019_2022\n",
            "A4_B9_2016_2019\t\t\t     XC60_2ì„¸ëŒ€_2018_2021\n",
            "A4_B9_2020_2024\t\t\t     XC60_2ì„¸ëŒ€_2022_2025\n",
            "A5_F5_2019_2024\t\t\t     XC90_2ì„¸ëŒ€_2017_2019\n",
            "ë‰´_A6_2012_2014\t\t\t     XC90_2ì„¸ëŒ€_2020_2025\n",
            "ë‰´_A6_2015_2018\t\t\t     XE_2016_2019\n",
            "A6_C8_2019_2025\t\t\t     XF_X260_2016_2020\n",
            "A7_2012_2016\t\t\t     XJ_8ì„¸ëŒ€_2010_2019\n",
            "A7_4K_2020_2024\t\t\t     XM3_2020_2023\n",
            "A8_D5_2018_2023\t\t\t     XM3_2024\n",
            "ì•„ë°˜ë–¼_AD_2016_2018\t\t     ìº ë¦¬_XV70_2018_2024\n",
            "ë”_ë‰´_ì•„ë°˜ë–¼_AD_2019_2020\t     ëª¨ë¸_Y_2021_2025\n",
            "All_New_XJ_2016_2019\t\t     YFì˜ë‚˜íƒ€_2009_2012\n",
            "AMG_GT_2016_2024\t\t     YFì˜ë‚˜íƒ€_í•˜ì´ë¸Œë¦¬ë“œ_2011_2015\n",
            "A_í´ëž˜ìŠ¤_W176_2015_2018\t\t     Z4_G29_2019_2025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import timm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import StratifiedKFold\n"
      ],
      "metadata": {
        "id": "eyLBp4j3i6F1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train ê²½ë¡œ\n",
        "train_dir = '/content/train'\n",
        "\n",
        "# test ê²½ë¡œ\n",
        "test_dir = '/content/test'\n",
        "\n",
        "# sample_submission (ì¶”ë¡  ë•Œ ì‚¬ìš©)\n",
        "sample_submission_path = '/content/sample_submission.csv'\n"
      ],
      "metadata": {
        "id": "Ybtf0x5si9CE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "def show_memory_status():\n",
        "    allocated = torch.cuda.memory_allocated() / (1024 ** 2)  # MB ë‹¨ìœ„\n",
        "    reserved = torch.cuda.memory_reserved() / (1024 ** 2)    # MB ë‹¨ìœ„\n",
        "    print(f\"ðŸ“Š í˜„ìž¬ GPU ë©”ëª¨ë¦¬ ìƒíƒœ: Allocated = {allocated:.2f} MB | Reserved = {reserved:.2f} MB\")\n",
        "\n",
        "# í˜„ìž¬ CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸\n",
        "if torch.cuda.is_available():\n",
        "    print(\"ðŸ” ì´ˆê¸°í™” ì „ GPU ë©”ëª¨ë¦¬ ìƒíƒœ:\")\n",
        "    show_memory_status()\n",
        "\n",
        "    # GPU ìºì‹œ ë° ë©”ëª¨ë¦¬ ì´ˆê¸°í™”\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.ipc_collect()\n",
        "    gc.collect()\n",
        "\n",
        "    print(\"\\nðŸ§¹ GPU ë©”ëª¨ë¦¬ ì´ˆê¸°í™” ì™„ë£Œ\")\n",
        "    print(\"ðŸ” ì´ˆê¸°í™” í›„ GPU ë©”ëª¨ë¦¬ ìƒíƒœ:\")\n",
        "    show_memory_status()\n",
        "else:\n",
        "    print(\"âŒ CUDA ì‚¬ìš© ë¶ˆê°€\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxuQLgBAmkIm",
        "outputId": "30be1cb4-99e2-4581-f7d0-936f0473cbeb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ” ì´ˆê¸°í™” ì „ GPU ë©”ëª¨ë¦¬ ìƒíƒœ:\n",
            "ðŸ“Š í˜„ìž¬ GPU ë©”ëª¨ë¦¬ ìƒíƒœ: Allocated = 0.00 MB | Reserved = 0.00 MB\n",
            "\n",
            "ðŸ§¹ GPU ë©”ëª¨ë¦¬ ì´ˆê¸°í™” ì™„ë£Œ\n",
            "ðŸ” ì´ˆê¸°í™” í›„ GPU ë©”ëª¨ë¦¬ ìƒíƒœ:\n",
            "ðŸ“Š í˜„ìž¬ GPU ë©”ëª¨ë¦¬ ìƒíƒœ: Allocated = 0.00 MB | Reserved = 0.00 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "# âœ… ì „ì²´ JPG íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸° (Train)\n",
        "file_list = glob.glob('/content/train/*/*.jpg')\n",
        "\n",
        "# âœ… í´ëž˜ìŠ¤ëª… ì¶”ì¶œ (í´ë”ëª… ê¸°ì¤€)\n",
        "def extract_class_name_jpg(path):\n",
        "    return os.path.basename(os.path.dirname(path))\n",
        "\n",
        "class_names = sorted(set(extract_class_name_jpg(f) for f in file_list))\n",
        "class_to_idx = {cls: idx for idx, cls in enumerate(class_names)}\n",
        "\n",
        "print(f\"âœ… í´ëž˜ìŠ¤ ìˆ˜: {len(class_to_idx)}\")  # 396ê°œ ë‚˜ì™€ì•¼ ì •ìƒ\n",
        "\n",
        "# âœ… ë¼ë²¨ ìƒì„±\n",
        "labels = [class_to_idx[extract_class_name_jpg(f)] for f in file_list]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktvVaB62kWsK",
        "outputId": "66206994-a232-4bc0-ea09-d83b82099290"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… í´ëž˜ìŠ¤ ìˆ˜: 396\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "# âœ… labeling ê¸°ì¤€ (ê°„ë‹¨ rule ê¸°ë°˜ v2 ì‚¬ìš© ì˜ˆì‹œ)\n",
        "def simple_pose_label_v2(img_path):\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    w, h = img.size\n",
        "\n",
        "    # Aspect ratio\n",
        "    aspect_ratio = w / h\n",
        "\n",
        "    # ì¤‘ì•™ crop â†’ ì¤‘ì•™ ë°ê¸°\n",
        "    crop = img.crop((w * 0.3, h * 0.5, w * 0.7, h * 0.8))\n",
        "    crop_np = np.array(crop).mean()\n",
        "\n",
        "    # ìƒë‹¨ crop â†’ ìƒë‹¨ ë°ê¸°\n",
        "    crop_top = img.crop((w * 0.3, h * 0.1, w * 0.7, h * 0.4))\n",
        "    crop_top_np = np.array(crop_top).mean()\n",
        "\n",
        "    # Simple rule v2\n",
        "    if crop_np > crop_top_np + 15 and aspect_ratio > 1.1:\n",
        "        return 'Rear'\n",
        "    elif crop_top_np > crop_np + 15 and aspect_ratio > 0.9:\n",
        "        return 'Front'\n",
        "    elif 0.9 < aspect_ratio < 1.1:\n",
        "        return 'Side'\n",
        "    else:\n",
        "        return 'Unknown'\n",
        "\n",
        "# âœ… ì „ì²´ labeling â†’ Train ì „ìš©\n",
        "def run_pose_labeling_train(img_paths):\n",
        "    records = []\n",
        "\n",
        "    for path in tqdm(img_paths, desc='Pose labeling Train'):\n",
        "        fname = os.path.basename(path)\n",
        "        label = simple_pose_label_v2(path)\n",
        "        # class_name ì¶”ì¶œ (í´ë”ëª…)\n",
        "        class_name = extract_class_name_jpg(path)\n",
        "        records.append({\n",
        "            'filename': fname,\n",
        "            'class': class_name,\n",
        "            'pose': label\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "# âœ… ì‹¤í–‰\n",
        "df_train_pose = run_pose_labeling_train(file_list)\n",
        "\n",
        "# âœ… ì €ìž¥\n",
        "SAVE_CSV_PATH = '/content/drive/MyDrive/team_models/pose_labels_for_train.csv'\n",
        "df_train_pose.to_csv(SAVE_CSV_PATH, index=False)\n",
        "\n",
        "print(f\"\\nâœ… Pose labeling (Train) ì™„ë£Œ â†’ CSV ì €ìž¥ë¨: {SAVE_CSV_PATH}\")\n",
        "print(df_train_pose['pose'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VE8FCPKkqZL",
        "outputId": "9c6868a1-ab7e-4c6d-a429-1f669a014cfe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pose labeling Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33137/33137 [01:36<00:00, 344.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Pose labeling (Train) ì™„ë£Œ â†’ CSV ì €ìž¥ë¨: /content/drive/MyDrive/team_models/pose_labels_for_train.csv\n",
            "pose\n",
            "Front      16134\n",
            "Rear        8769\n",
            "Unknown     7600\n",
            "Side         634\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# âœ… íŒŒì¼ ê²½ë¡œ\n",
        "POSE_LABEL_CSV = '/content/drive/MyDrive/team_models/pose_labels_for_train.csv'\n",
        "\n",
        "# âœ… CSV ë¡œë“œ\n",
        "df_pose = pd.read_csv(POSE_LABEL_CSV)\n",
        "\n",
        "# âœ… ìš°ì„  ê¸°ì¡´ pose ì»¬ëŸ¼ ë³µì‚¬\n",
        "df_pose['manual_pose'] = df_pose['pose']\n",
        "\n",
        "# âœ… ì ìš© ìˆœì„œ (ì¤‘ìš”!)\n",
        "# âœ… ì™„ì „ ë…¸ì´ì¦ˆ\n",
        "manual_noise_list = [\n",
        "    '5ì‹œë¦¬ì¦ˆ_G60_2024_2025_0010.jpg',\n",
        "    '6ì‹œë¦¬ì¦ˆ_GT_G32_2018_2020_0018.jpg',\n",
        "    '7ì‹œë¦¬ì¦ˆ_G11_2016_2018_0040.jpg',\n",
        "    '911_992_2020_2024_0030.jpg',\n",
        "    'E_í´ëž˜ìŠ¤_W212_2010_2016_0022.jpg',\n",
        "    'K5_2ì„¸ëŒ€_2016_2018_0007.jpg',\n",
        "    'F150_2004_2021_0018.jpg',\n",
        "    'G_í´ëž˜ìŠ¤_W463b_2019_2025_0030.jpg',\n",
        "    'GLE_í´ëž˜ìŠ¤_W167_2019_2024_0068.jpg',\n",
        "    'Q5_FY_2021_2024_0032.jpg',\n",
        "    'Q30_2017_2019_0075.jpg',\n",
        "    'Q50_2014_2017_0031.jpg',\n",
        "    'SM7_ë‰´ì•„íŠ¸_2008_2011_0053.jpg',\n",
        "    'X3_G01_2022_2024_0029.jpg',\n",
        "    'XF_X260_2016_2020_0023.jpg',\n",
        "    'ë‰´_ES300h_2013_2015_0000.jpg',\n",
        "    'ë‰´_G80_2025_2026_0042.jpg',\n",
        "    'ë‰´_G80_2025_2026_0043.jpg',\n",
        "    'ë‰´_SM5_ìž„í”„ë ˆì…˜_2008_2010_0033.jpg',\n",
        "    'ë”_ê¸°ì•„_ë ˆì´_EV_2024_2025_0078.jpg',\n",
        "    'ë”_ë‰´_K3_2ì„¸ëŒ€_2022_2024_0001.jpg',\n",
        "    'ë”_ë‰´_ê·¸ëžœë“œ_ìŠ¤íƒ€ë ‰ìŠ¤_2018_2021_0078.jpg',\n",
        "    'ë”_ë‰´_ê·¸ëžœë“œ_ìŠ¤íƒ€ë ‰ìŠ¤_2018_2021_0079.jpg',\n",
        "    'ë”_ë‰´_ê·¸ëžœë“œ_ìŠ¤íƒ€ë ‰ìŠ¤_2018_2021_0080.jpg',\n",
        "    'ë”_ë‰´_ì•„ë°˜ë–¼_2014_2016_0031.jpg',\n",
        "    'ë”_ë‰´_íŒŒì‚¬íŠ¸_2012_2019_0067.jpg',\n",
        "    'ë ˆë‹ˆê²Œì´ë“œ_2019_2023_0041.jpg',\n",
        "    'ë°•ìŠ¤í„°_718_2017_2024_0011.jpg',\n",
        "    'ì‹¼íƒ€íŽ˜_TM_2019_2020_0009.jpg',\n",
        "    'ì•„ë°˜ë–¼_MD_2011_2014_0081.jpg',\n",
        "    'ì•„ë°˜ë–¼_N_2022_2023_0064.jpg',\n",
        "    'ìµìŠ¤í”Œë¡œëŸ¬_2016_2017_0072.jpg',\n",
        "    'ì½°íŠ¸ë¡œí¬ë¥´í…Œ_2017_2022_0074.jpg',\n",
        "    'í”„ë¦¬ìš°ìŠ¤_4ì„¸ëŒ€_2019_2022_0052.jpg',\n",
        "    'ì•„ë°˜ë–¼_N_2022_2023_0035.jpg'  # âœ… ì¶”ê°€ë¡œ ë„£ìœ¼ì‹  ê²ƒ!\n",
        "]\n",
        "\n",
        "# âœ… ì°¨ëŸ‰ ë‚´ë¶€\n",
        "manual_inside_list = [\n",
        "    'E_í´ëž˜ìŠ¤_W212_2010_2016_0069.jpg',\n",
        "    'ES300h_7ì„¸ëŒ€_2019_2026_0028.jpg',\n",
        "    'G_í´ëž˜ìŠ¤_W463_2009_2017_0011.jpg',\n",
        "    'GLB_í´ëž˜ìŠ¤_X247_2020_2023_0008.jpg',\n",
        "    'GLS_í´ëž˜ìŠ¤_X167_2020_2024_0013.jpg',\n",
        "    'K3_2013_2015_0045.jpg',\n",
        "    'K5_3ì„¸ëŒ€_2020_2023_0081.jpg',\n",
        "    'Q7_4M_2020_2023_0011.jpg',\n",
        "    'RAV4_5ì„¸ëŒ€_2019_2024_0020.jpg',\n",
        "    'S_í´ëž˜ìŠ¤_W223_2021_2025_0008.jpg',\n",
        "    'S_í´ëž˜ìŠ¤_W223_2021_2025_0071.jpg',\n",
        "    'X4_F26_2015_2018_0068.jpg',\n",
        "    'ê·¸ëžœë“œ_ì²´ë¡œí‚¤_WL_2021_2023_0018.jpg',\n",
        "    'ë ˆì´_2012_2017_0063.jpg',\n",
        "    'ë ˆì¸ì§€ë¡œë²„_5ì„¸ëŒ€_2023_2024_0030.jpg',\n",
        "    'ë ˆì¸ì§€ë¡œë²„_ìŠ¤í¬ì¸ _2ì„¸ëŒ€_2018_2022_0014.jpg',\n",
        "    'ë ˆì¸ì§€ë¡œë²„_ìŠ¤í¬ì¸ _2ì„¸ëŒ€_2018_2022_0017.jpg',\n",
        "    'ë§ˆì¹¸_2019_2021_0035.jpg',\n",
        "    'ë¨¸ìŠ¤íƒ±_2015_2023_0086.jpg',\n",
        "    'ì•„ë°˜ë–¼_MD_2011_2014_0009.jpg',\n",
        "    'ì•„ë°˜ë–¼_MD_2011_2014_0082.jpg',\n",
        "    'ì»¨í‹°ë„¨íƒˆ_GT_3ì„¸ëŒ€_2018_2023_0007.jpg',\n",
        "    'íƒ€ì´ì¹¸_2021_2025_0065.jpg',\n",
        "    'íŒŒë‚˜ë©”ë¼_2010_2016_0000.jpg',\n",
        "    'íŒŒë‚˜ë©”ë¼_2010_2016_0036.jpg',\n",
        "    '3ì‹œë¦¬ì¦ˆ_F30_2013_2018_0036.jpg',\n",
        "    '4ì‹œë¦¬ì¦ˆ_F32_2014_2020_0027.jpg',\n",
        "    '5ì‹œë¦¬ì¦ˆ_G60_2024_2025_0056.jpg',\n",
        "    '7ì‹œë¦¬ì¦ˆ_F01_2009_2015_0029.jpg',\n",
        "    '7ì‹œë¦¬ì¦ˆ_F01_2009_2015_0044.jpg',\n",
        "    '911_992_2020_2024_0006.jpg',\n",
        "    'C_í´ëž˜ìŠ¤_W204_2008_2015_0068.jpg',\n",
        "    'CLS_í´ëž˜ìŠ¤_C257_2019_2023_0021.jpg'\n",
        "]\n",
        "\n",
        "# âœ… íŠ¸ë í¬ ì—´ë¦¼\n",
        "manual_trunk_list = [\n",
        "    'Q30_2017_2019_0074.jpg',\n",
        "    'ê¸€ëž˜ë””ì—ì´í„°_JT_2020_2023_0075.jpg',\n",
        "    'ë‰´_CC_2012_2016_0001.jpg',\n",
        "    'ë‰´_CC_2012_2016_0002.jpg',\n",
        "    'ë”_ë‰´_ì½”ë‚˜_2021_2023_0081.jpg',\n",
        "    '2ì‹œë¦¬ì¦ˆ_ì•¡í‹°ë¸Œ_íˆ¬ì–´ëŸ¬_U06_2022_2024_0004.jpg',\n",
        "    'A8_D5_2018_2023_0084.jpg'\n",
        "]\n",
        "\n",
        "# âœ… ê²°ê³¼ í™•ì¸\n",
        "print(df_pose['manual_pose'].value_counts())\n",
        "\n",
        "# âœ… ì €ìž¥\n",
        "SAVE_MANUAL_CSV = '/content/drive/MyDrive/team_models/pose_labels_for_train_manual_applied.csv'\n",
        "df_pose.to_csv(SAVE_MANUAL_CSV, index=False)\n",
        "\n",
        "print(f\"\\nâœ… ìžë™ ì ìš© ì™„ë£Œ â†’ CSV ì €ìž¥ë¨: {SAVE_MANUAL_CSV}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTEs2TzCrbZH",
        "outputId": "c3adfe01-acf5-4e57-f025-93f3e2e6451e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "manual_pose\n",
            "Front      16134\n",
            "Rear        8769\n",
            "Unknown     7600\n",
            "Side         634\n",
            "Name: count, dtype: int64\n",
            "\n",
            "âœ… ìžë™ ì ìš© ì™„ë£Œ â†’ CSV ì €ìž¥ë¨: /content/drive/MyDrive/team_models/pose_labels_for_train_manual_applied.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# âœ… ì¤€ë¹„ëœ Pose Label CSV ê²½ë¡œ\n",
        "POSE_CSV_PATH = '/content/drive/MyDrive/team_models/pose_labels_for_train_manual_applied.csv'\n",
        "\n",
        "# âœ… ê¸°ì¡´ pose label ë¡œë“œ\n",
        "df_pose = pd.read_csv(POSE_CSV_PATH)\n",
        "\n",
        "# âœ… í˜„ìž¬ file_list ë¡œë“œ (train image list)\n",
        "train_file_list = glob.glob('/content/train/*/*.jpg')\n",
        "\n",
        "# âœ… \"íŒŒì¼ëª…\" ê¸°ì¤€ìœ¼ë¡œ DataFrame ë§Œë“¤ê¸°\n",
        "df_train_files = pd.DataFrame({\n",
        "    'filepath': train_file_list,\n",
        "    'filename': [os.path.basename(p) for p in train_file_list],\n",
        "    'class': [os.path.basename(os.path.dirname(p)) for p in train_file_list]\n",
        "})\n",
        "\n",
        "# âœ… Merge â†’ pose ë¶™ì´ê¸°\n",
        "df_train_merged = pd.merge(df_train_files, df_pose[['filename', 'manual_pose']], on='filename', how='left')\n",
        "\n",
        "# âœ… Null ì²´í¬\n",
        "assert df_train_merged['manual_pose'].isnull().sum() == 0, \"ðŸš¨ pose ë§¤ì¹­ ì•ˆëœ ì´ë¯¸ì§€ ìžˆìŒ!\"\n",
        "\n",
        "print(f\"âœ… Pose merge ì™„ë£Œ â†’ ì´ {len(df_train_merged)}ê°œ\")\n",
        "\n",
        "# âœ… One-hot Encoding (ì¶”ì²œ ì‚¬ìš©)\n",
        "df_pose_ohe = pd.get_dummies(df_train_merged['manual_pose'], prefix='pose')\n",
        "\n",
        "# âœ… ìµœì¢… DF\n",
        "df_train_final = pd.concat([df_train_merged, df_pose_ohe], axis=1)\n",
        "\n",
        "# âœ… ì €ìž¥\n",
        "SAVE_TRAIN_FINAL = '/content/drive/MyDrive/team_models/train_with_pose_feature.csv'\n",
        "df_train_final.to_csv(SAVE_TRAIN_FINAL, index=False)\n",
        "\n",
        "print(f\"\\nâœ… ìµœì¢… Train + Pose feature ì €ìž¥ ì™„ë£Œ â†’ {SAVE_TRAIN_FINAL}\")\n",
        "print(f\"ðŸ‘‰ One-hot columns: {df_pose_ohe.columns.tolist()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SC6keyKvklC1",
        "outputId": "150cba73-6322-46ac-8a4e-48887fbc51a8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Pose merge ì™„ë£Œ â†’ ì´ 33137ê°œ\n",
            "\n",
            "âœ… ìµœì¢… Train + Pose feature ì €ìž¥ ì™„ë£Œ â†’ /content/drive/MyDrive/team_models/train_with_pose_feature.csv\n",
            "ðŸ‘‰ One-hot columns: ['pose_Front', 'pose_Rear', 'pose_Side', 'pose_Unknown']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "class CarImagePoseDataset(Dataset):\n",
        "    def __init__(self, file_list, class_to_idx, df_pose, transform=None,\n",
        "                 use_aspect=False, use_color=False, use_pose=False):\n",
        "\n",
        "        self.file_list = file_list\n",
        "        self.class_to_idx = class_to_idx\n",
        "        self.df_pose = df_pose.set_index('filename')  # ë¹ ë¥¸ lookup ìœ„í•´ index ì„¤ì •\n",
        "        self.transform = transform\n",
        "        self.use_aspect = use_aspect\n",
        "        self.use_color = use_color\n",
        "        self.use_pose = use_pose\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.file_list[idx]\n",
        "\n",
        "        # Load image\n",
        "        image_pil = Image.open(path).convert(\"RGB\")\n",
        "\n",
        "        # Aspect Ratio\n",
        "        width, height = image_pil.size\n",
        "        aspect_ratio = np.array([width / height], dtype=np.float32)\n",
        "\n",
        "        # Color Mean\n",
        "        image_cv2 = cv2.cvtColor(np.array(image_pil), cv2.COLOR_RGB2BGR)\n",
        "        color_mean = image_cv2.mean(axis=(0, 1))\n",
        "        color_mean = color_mean[::-1]\n",
        "        color_mean = np.array(color_mean / 255.0, dtype=np.float32)\n",
        "\n",
        "        # Transform\n",
        "        if self.transform:\n",
        "            image = self.transform(image_pil)\n",
        "        else:\n",
        "            image = transforms.ToTensor()(image_pil)\n",
        "\n",
        "        # Label\n",
        "        class_name = os.path.basename(os.path.dirname(path))\n",
        "        label = self.class_to_idx[class_name]\n",
        "\n",
        "        # Filename\n",
        "        fname = os.path.basename(path)\n",
        "\n",
        "        # Pose feature (4-dim vector)\n",
        "        pose_vec = np.array([\n",
        "            self.df_pose.loc[fname, 'pose_Front'],\n",
        "            self.df_pose.loc[fname, 'pose_Rear'],\n",
        "            self.df_pose.loc[fname, 'pose_Side'],\n",
        "            self.df_pose.loc[fname, 'pose_Unknown']\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "        # Return modes\n",
        "        features = []\n",
        "\n",
        "        if self.use_aspect:\n",
        "            features.append(torch.tensor(aspect_ratio))\n",
        "\n",
        "        if self.use_color:\n",
        "            features.append(torch.tensor(color_mean))\n",
        "\n",
        "        if self.use_pose:\n",
        "            features.append(torch.tensor(pose_vec))\n",
        "\n",
        "        if features:\n",
        "            return image, *features, label\n",
        "        else:\n",
        "            return image, label\n"
      ],
      "metadata": {
        "id": "9a4nTFQ8siE9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import timm\n",
        "import torch\n",
        "\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, use_aspect, use_color, use_pose, num_classes):\n",
        "        super(CustomModel, self).__init__()\n",
        "\n",
        "        self.use_aspect = use_aspect\n",
        "        self.use_color = use_color\n",
        "        self.use_pose = use_pose\n",
        "\n",
        "        # âœ… EfficientNet-B5 backbone\n",
        "        self.backbone = timm.create_model('efficientnet_b5', pretrained=True, num_classes=0)  # feature extractor\n",
        "        backbone_out_features = self.backbone.num_features\n",
        "\n",
        "        # âœ… Meta feature dimension ê³„ì‚°\n",
        "        meta_features_dim = 0\n",
        "        if self.use_aspect:\n",
        "            meta_features_dim += 1  # aspect ratio 1ê°œ\n",
        "        if self.use_color:\n",
        "            meta_features_dim += 3  # color_mean (R, G, B) 3ê°œ\n",
        "        if self.use_pose:\n",
        "            meta_features_dim += 4  # pose one-hot (Front, Rear, Side, Unknown)\n",
        "\n",
        "        # âœ… Classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(backbone_out_features + meta_features_dim, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, image, aspect_ratio=None, color_mean=None, pose_feature=None):\n",
        "        # âœ… EfficientNet feature\n",
        "        x = self.backbone(image)\n",
        "\n",
        "        # âœ… Meta features concat\n",
        "        aux_list = []\n",
        "\n",
        "        if self.use_aspect:\n",
        "            aux_list.append(aspect_ratio)\n",
        "\n",
        "        if self.use_color:\n",
        "            aux_list.append(color_mean)\n",
        "\n",
        "        if self.use_pose:\n",
        "            aux_list.append(pose_feature)\n",
        "\n",
        "        if aux_list:\n",
        "            aux_features = torch.cat(aux_list, dim=1)\n",
        "            x = torch.cat([x, aux_features], dim=1)\n",
        "\n",
        "        # âœ… Final classifier\n",
        "        out = self.classifier(x)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "-XGDJI0ls9Pk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# EfficientNet-B5 ê¶Œìž¥ ìž…ë ¥ í¬ê¸° (456x456)\n",
        "IMG_SIZE = 456\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n"
      ],
      "metadata": {
        "id": "Xvj8_cOctJJs"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "class CarImageDataset(Dataset):\n",
        "    def __init__(self, file_list, class_to_idx, df_pose, transform=None, use_aspect=False, use_color=False, use_pose=False):\n",
        "        self.file_list = file_list\n",
        "        self.class_to_idx = class_to_idx\n",
        "        self.df_pose = df_pose\n",
        "        self.transform = transform\n",
        "        self.use_aspect = use_aspect\n",
        "        self.use_color = use_color\n",
        "        self.use_pose = use_pose\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.file_list[idx]\n",
        "\n",
        "        # ðŸš— Load Image\n",
        "        image_pil = Image.open(path).convert(\"RGB\")\n",
        "\n",
        "        # ðŸš— Aspect Ratio\n",
        "        width, height = image_pil.size\n",
        "        aspect_ratio = np.array([width / height], dtype=np.float32)\n",
        "\n",
        "        # ðŸš— Color Mean\n",
        "        image_cv2 = cv2.cvtColor(np.array(image_pil), cv2.COLOR_RGB2BGR)\n",
        "        color_mean = image_cv2.mean(axis=(0, 1))\n",
        "        color_mean = color_mean[::-1]\n",
        "        color_mean = np.array(color_mean / 255.0, dtype=np.float32)\n",
        "\n",
        "        # ðŸš— Pose One-hot â†’ df_pose lookup\n",
        "        fname = os.path.basename(path)\n",
        "        row = self.df_pose[self.df_pose['filename'] == fname]\n",
        "\n",
        "        if len(row) == 0:\n",
        "            pose_onehot = np.array([0, 0, 0, 1], dtype=np.float32)  # Unknown fallback\n",
        "        else:\n",
        "            pose = row['pose'].values[0]\n",
        "            pose_onehot = np.zeros(4, dtype=np.float32)\n",
        "            if pose == 'Front':\n",
        "                pose_onehot[0] = 1\n",
        "            elif pose == 'Rear':\n",
        "                pose_onehot[1] = 1\n",
        "            elif pose == 'Side':\n",
        "                pose_onehot[2] = 1\n",
        "            else:\n",
        "                pose_onehot[3] = 1  # Unknown\n",
        "\n",
        "        # ðŸš— Transform\n",
        "        if self.transform:\n",
        "            image = self.transform(image_pil)\n",
        "        else:\n",
        "            image = transforms.ToTensor()(image_pil)\n",
        "\n",
        "        # ðŸš— Label\n",
        "        class_name = os.path.basename(os.path.dirname(path))\n",
        "        label = self.class_to_idx[class_name]\n",
        "\n",
        "        # ðŸš— Return\n",
        "        meta_list = []\n",
        "\n",
        "        if self.use_aspect:\n",
        "            meta_list.append(torch.tensor(aspect_ratio))\n",
        "        if self.use_color:\n",
        "            meta_list.append(torch.tensor(color_mean))\n",
        "        if self.use_pose:\n",
        "            meta_list.append(torch.tensor(pose_onehot))\n",
        "\n",
        "        if meta_list:\n",
        "            return image, *meta_list, label\n",
        "        else:\n",
        "            return image, label\n"
      ],
      "metadata": {
        "id": "sZcoNxFctUDd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# âœ… ë””ë°”ì´ìŠ¤ ì„¤ì • (GPU ìš°ì„ )\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"âœ… Device ì‚¬ìš©: {device}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLt3GPdlxLPD",
        "outputId": "8dc95b55-4017-450f-a220-29bcf31db23b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Device ì‚¬ìš©: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "# âœ… StratifiedKFold ì •ì˜\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# âœ… ì‹¤í—˜ëª…\n",
        "EXPERIMENT = \"C_pose\"\n",
        "\n",
        "use_aspect = False\n",
        "use_color = True\n",
        "use_pose = True\n",
        "\n",
        "print(f\"\\nðŸš€ ì‹¤í—˜ ì„¤ì •: {EXPERIMENT} (Aspect={use_aspect}, Color={use_color}, Pose={use_pose})\\n\")\n",
        "\n",
        "# âœ… 5-Fold ë£¨í”„ ì‹œìž‘\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(file_list, labels)):\n",
        "    print(f\"\\n==============================\")\n",
        "    print(f\"ðŸ” Fold {fold + 1} / 5\")\n",
        "    print(f\"==============================\\n\")\n",
        "\n",
        "    # âœ… Foldë³„ split\n",
        "    train_files = [file_list[i] for i in train_idx]\n",
        "    val_files = [file_list[i] for i in val_idx]\n",
        "\n",
        "    # âœ… Foldë³„ Dataset & DataLoader\n",
        "    train_dataset = CarImageDataset(train_files, class_to_idx, df_pose, train_transform, use_aspect, use_color, use_pose)\n",
        "    val_dataset = CarImageDataset(val_files, class_to_idx, df_pose, val_transform, use_aspect, use_color, use_pose)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
        "\n",
        "    # âœ… Foldë³„ model / criterion / optimizer ì´ˆê¸°í™”\n",
        "    model = CustomModel(use_aspect=use_aspect, use_color=use_color, use_pose=use_pose, num_classes=396)\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "\n",
        "    # âœ… EarlyStopping ë³€ìˆ˜ ì´ˆê¸°í™”\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 3\n",
        "    patience_counter = 0\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    # âœ… Epoch ë£¨í”„\n",
        "    for epoch in range(1, 31):\n",
        "        print(f\"\\nðŸ“Œ Fold {fold+1} | Epoch {epoch}\")\n",
        "\n",
        "        # === í•™ìŠµ ===\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "\n",
        "        loop = tqdm(train_loader, desc=f\"Train Fold {fold+1}\", leave=False)\n",
        "        for batch in loop:\n",
        "            X = batch[0].to(device)\n",
        "            meta_list = batch[1:-1]\n",
        "            y = batch[-1].to(device)\n",
        "\n",
        "            # Meta ì¤€ë¹„\n",
        "            meta_args = {}\n",
        "            cnt = 0\n",
        "            if use_aspect:\n",
        "                meta_args['aspect_ratio'] = meta_list[cnt].to(device)\n",
        "                cnt += 1\n",
        "            if use_color:\n",
        "                meta_args['color_mean'] = meta_list[cnt].to(device)\n",
        "                cnt += 1\n",
        "            if use_pose:\n",
        "                meta_args['pose_feature'] = meta_list[cnt].to(device)\n",
        "                cnt += 1\n",
        "\n",
        "            outputs = model(X, **meta_args)\n",
        "\n",
        "            loss = criterion(outputs, y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * X.size(0)\n",
        "            train_correct += (outputs.argmax(1) == y).sum().item()\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "        train_acc = train_correct / len(train_loader.dataset)\n",
        "\n",
        "        # === ê²€ì¦ ===\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "\n",
        "        val_loop = tqdm(val_loader, desc=f\"Valid Fold {fold+1}\", leave=False)\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loop:\n",
        "                X = batch[0].to(device)\n",
        "                meta_list = batch[1:-1]\n",
        "                y = batch[-1].to(device)\n",
        "\n",
        "                meta_args = {}\n",
        "                cnt = 0\n",
        "                if use_aspect:\n",
        "                    meta_args['aspect_ratio'] = meta_list[cnt].to(device)\n",
        "                    cnt += 1\n",
        "                if use_color:\n",
        "                    meta_args['color_mean'] = meta_list[cnt].to(device)\n",
        "                    cnt += 1\n",
        "                if use_pose:\n",
        "                    meta_args['pose_feature'] = meta_list[cnt].to(device)\n",
        "                    cnt += 1\n",
        "\n",
        "                outputs = model(X, **meta_args)\n",
        "\n",
        "                loss = criterion(outputs, y)\n",
        "                val_loss += loss.item() * X.size(0)\n",
        "                val_correct += (outputs.argmax(1) == y).sum().item()\n",
        "                val_loop.set_postfix(loss=loss.item())\n",
        "\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        val_acc = val_correct / len(val_loader.dataset)\n",
        "\n",
        "        # === ë¡œê·¸ ì¶œë ¥ ===\n",
        "        print(f\"âœ… Fold {fold+1} | Epoch {epoch} | Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f}\")\n",
        "        print(f\"âœ… Fold {fold+1} | Epoch {epoch} | Val   Loss: {val_loss:.4f} | Acc: {val_acc:.4f}\")\n",
        "\n",
        "        # === EarlyStopping ===\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            save_path = f\"/content/drive/MyDrive/team_models/EffNetB5_{EXPERIMENT}_fold{fold+1}.pth\"\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(f\"ðŸ“¦ Best model saved for Fold {fold+1}!\")\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"âš ï¸ EarlyStopping patience: {patience_counter}/{patience}\")\n",
        "            if patience_counter >= patience:\n",
        "                print(\"â›” Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "    # âœ… Fold ëë‚˜ê³  Best ëª¨ë¸ ë¡œë“œ\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    print(f\"âœ… Fold {fold+1} Best model loaded.\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-r1EJPrSwXcS",
        "outputId": "82acac27-b8fa-47fe-c208-3ef27fa43913"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸš€ ì‹¤í—˜ ì„¤ì •: C_pose (Aspect=False, Color=True, Pose=True)\n",
            "\n",
            "\n",
            "==============================\n",
            "ðŸ” Fold 1 / 5\n",
            "==============================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“Œ Fold 1 | Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 1 | Epoch 1 | Train Loss: 3.2506 | Acc: 0.4787\n",
            "âœ… Fold 1 | Epoch 1 | Val   Loss: 0.6897 | Acc: 0.8612\n",
            "ðŸ“¦ Best model saved for Fold 1!\n",
            "\n",
            "ðŸ“Œ Fold 1 | Epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 1 | Epoch 2 | Train Loss: 0.5051 | Acc: 0.8977\n",
            "âœ… Fold 1 | Epoch 2 | Val   Loss: 0.2809 | Acc: 0.9200\n",
            "ðŸ“¦ Best model saved for Fold 1!\n",
            "\n",
            "ðŸ“Œ Fold 1 | Epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 1 | Epoch 3 | Train Loss: 0.2636 | Acc: 0.9286\n",
            "âœ… Fold 1 | Epoch 3 | Val   Loss: 0.2432 | Acc: 0.9270\n",
            "ðŸ“¦ Best model saved for Fold 1!\n",
            "\n",
            "ðŸ“Œ Fold 1 | Epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 1 | Epoch 4 | Train Loss: 0.1972 | Acc: 0.9428\n",
            "âœ… Fold 1 | Epoch 4 | Val   Loss: 0.1978 | Acc: 0.9378\n",
            "ðŸ“¦ Best model saved for Fold 1!\n",
            "\n",
            "ðŸ“Œ Fold 1 | Epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 1 | Epoch 5 | Train Loss: 0.1670 | Acc: 0.9495\n",
            "âœ… Fold 1 | Epoch 5 | Val   Loss: 0.1760 | Acc: 0.9466\n",
            "ðŸ“¦ Best model saved for Fold 1!\n",
            "\n",
            "ðŸ“Œ Fold 1 | Epoch 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 1 | Epoch 6 | Train Loss: 0.1490 | Acc: 0.9541\n",
            "âœ… Fold 1 | Epoch 6 | Val   Loss: 0.1889 | Acc: 0.9460\n",
            "âš ï¸ EarlyStopping patience: 1/3\n",
            "\n",
            "ðŸ“Œ Fold 1 | Epoch 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 1 | Epoch 7 | Train Loss: 0.1366 | Acc: 0.9580\n",
            "âœ… Fold 1 | Epoch 7 | Val   Loss: 0.1719 | Acc: 0.9519\n",
            "ðŸ“¦ Best model saved for Fold 1!\n",
            "\n",
            "ðŸ“Œ Fold 1 | Epoch 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 1 | Epoch 8 | Train Loss: 0.1250 | Acc: 0.9598\n",
            "âœ… Fold 1 | Epoch 8 | Val   Loss: 0.1646 | Acc: 0.9526\n",
            "ðŸ“¦ Best model saved for Fold 1!\n",
            "\n",
            "ðŸ“Œ Fold 1 | Epoch 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 1 | Epoch 9 | Train Loss: 0.1148 | Acc: 0.9623\n",
            "âœ… Fold 1 | Epoch 9 | Val   Loss: 0.1550 | Acc: 0.9561\n",
            "ðŸ“¦ Best model saved for Fold 1!\n",
            "\n",
            "ðŸ“Œ Fold 1 | Epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 1 | Epoch 10 | Train Loss: 0.1100 | Acc: 0.9657\n",
            "âœ… Fold 1 | Epoch 10 | Val   Loss: 0.1668 | Acc: 0.9567\n",
            "âš ï¸ EarlyStopping patience: 1/3\n",
            "\n",
            "ðŸ“Œ Fold 1 | Epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 1 | Epoch 11 | Train Loss: 0.0990 | Acc: 0.9674\n",
            "âœ… Fold 1 | Epoch 11 | Val   Loss: 0.1704 | Acc: 0.9575\n",
            "âš ï¸ EarlyStopping patience: 2/3\n",
            "\n",
            "ðŸ“Œ Fold 1 | Epoch 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 1 | Epoch 12 | Train Loss: 0.0932 | Acc: 0.9703\n",
            "âœ… Fold 1 | Epoch 12 | Val   Loss: 0.1410 | Acc: 0.9582\n",
            "ðŸ“¦ Best model saved for Fold 1!\n",
            "\n",
            "ðŸ“Œ Fold 1 | Epoch 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 1 | Epoch 13 | Train Loss: 0.0961 | Acc: 0.9689\n",
            "âœ… Fold 1 | Epoch 13 | Val   Loss: 0.1444 | Acc: 0.9615\n",
            "âš ï¸ EarlyStopping patience: 1/3\n",
            "\n",
            "ðŸ“Œ Fold 1 | Epoch 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 1 | Epoch 14 | Train Loss: 0.0810 | Acc: 0.9725\n",
            "âœ… Fold 1 | Epoch 14 | Val   Loss: 0.1478 | Acc: 0.9609\n",
            "âš ï¸ EarlyStopping patience: 2/3\n",
            "\n",
            "ðŸ“Œ Fold 1 | Epoch 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 1 | Epoch 15 | Train Loss: 0.0847 | Acc: 0.9732\n",
            "âœ… Fold 1 | Epoch 15 | Val   Loss: 0.1636 | Acc: 0.9573\n",
            "âš ï¸ EarlyStopping patience: 3/3\n",
            "â›” Early stopping triggered.\n",
            "âœ… Fold 1 Best model loaded.\n",
            "\n",
            "\n",
            "==============================\n",
            "ðŸ” Fold 2 / 5\n",
            "==============================\n",
            "\n",
            "\n",
            "ðŸ“Œ Fold 2 | Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 2 | Epoch 1 | Train Loss: 3.3691 | Acc: 0.4543\n",
            "âœ… Fold 2 | Epoch 1 | Val   Loss: 0.7773 | Acc: 0.8544\n",
            "ðŸ“¦ Best model saved for Fold 2!\n",
            "\n",
            "ðŸ“Œ Fold 2 | Epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 2 | Epoch 2 | Train Loss: 0.5291 | Acc: 0.8930\n",
            "âœ… Fold 2 | Epoch 2 | Val   Loss: 0.2946 | Acc: 0.9151\n",
            "ðŸ“¦ Best model saved for Fold 2!\n",
            "\n",
            "ðŸ“Œ Fold 2 | Epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 2 | Epoch 3 | Train Loss: 0.2692 | Acc: 0.9255\n",
            "âœ… Fold 2 | Epoch 3 | Val   Loss: 0.2196 | Acc: 0.9282\n",
            "ðŸ“¦ Best model saved for Fold 2!\n",
            "\n",
            "ðŸ“Œ Fold 2 | Epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 2 | Epoch 4 | Train Loss: 0.2101 | Acc: 0.9381\n",
            "âœ… Fold 2 | Epoch 4 | Val   Loss: 0.2063 | Acc: 0.9327\n",
            "ðŸ“¦ Best model saved for Fold 2!\n",
            "\n",
            "ðŸ“Œ Fold 2 | Epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 2 | Epoch 5 | Train Loss: 0.1752 | Acc: 0.9457\n",
            "âœ… Fold 2 | Epoch 5 | Val   Loss: 0.1946 | Acc: 0.9431\n",
            "ðŸ“¦ Best model saved for Fold 2!\n",
            "\n",
            "ðŸ“Œ Fold 2 | Epoch 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 2 | Epoch 6 | Train Loss: 0.1511 | Acc: 0.9521\n",
            "âœ… Fold 2 | Epoch 6 | Val   Loss: 0.1937 | Acc: 0.9454\n",
            "ðŸ“¦ Best model saved for Fold 2!\n",
            "\n",
            "ðŸ“Œ Fold 2 | Epoch 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 2 | Epoch 7 | Train Loss: 0.1379 | Acc: 0.9558\n",
            "âœ… Fold 2 | Epoch 7 | Val   Loss: 0.2179 | Acc: 0.9410\n",
            "âš ï¸ EarlyStopping patience: 1/3\n",
            "\n",
            "ðŸ“Œ Fold 2 | Epoch 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 2 | Epoch 8 | Train Loss: 0.1228 | Acc: 0.9599\n",
            "âœ… Fold 2 | Epoch 8 | Val   Loss: 0.1682 | Acc: 0.9496\n",
            "ðŸ“¦ Best model saved for Fold 2!\n",
            "\n",
            "ðŸ“Œ Fold 2 | Epoch 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 2 | Epoch 9 | Train Loss: 0.1203 | Acc: 0.9617\n",
            "âœ… Fold 2 | Epoch 9 | Val   Loss: 0.1705 | Acc: 0.9517\n",
            "âš ï¸ EarlyStopping patience: 1/3\n",
            "\n",
            "ðŸ“Œ Fold 2 | Epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 2 | Epoch 10 | Train Loss: 0.1044 | Acc: 0.9655\n",
            "âœ… Fold 2 | Epoch 10 | Val   Loss: 0.1599 | Acc: 0.9553\n",
            "ðŸ“¦ Best model saved for Fold 2!\n",
            "\n",
            "ðŸ“Œ Fold 2 | Epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 2 | Epoch 11 | Train Loss: 0.1058 | Acc: 0.9653\n",
            "âœ… Fold 2 | Epoch 11 | Val   Loss: 0.1936 | Acc: 0.9504\n",
            "âš ï¸ EarlyStopping patience: 1/3\n",
            "\n",
            "ðŸ“Œ Fold 2 | Epoch 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 2 | Epoch 12 | Train Loss: 0.1058 | Acc: 0.9664\n",
            "âœ… Fold 2 | Epoch 12 | Val   Loss: 0.1649 | Acc: 0.9596\n",
            "âš ï¸ EarlyStopping patience: 2/3\n",
            "\n",
            "ðŸ“Œ Fold 2 | Epoch 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 2 | Epoch 13 | Train Loss: 0.0953 | Acc: 0.9688\n",
            "âœ… Fold 2 | Epoch 13 | Val   Loss: 0.1588 | Acc: 0.9553\n",
            "ðŸ“¦ Best model saved for Fold 2!\n",
            "\n",
            "ðŸ“Œ Fold 2 | Epoch 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 2 | Epoch 14 | Train Loss: 0.0890 | Acc: 0.9716\n",
            "âœ… Fold 2 | Epoch 14 | Val   Loss: 0.1664 | Acc: 0.9568\n",
            "âš ï¸ EarlyStopping patience: 1/3\n",
            "\n",
            "ðŸ“Œ Fold 2 | Epoch 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 2 | Epoch 15 | Train Loss: 0.0828 | Acc: 0.9737\n",
            "âœ… Fold 2 | Epoch 15 | Val   Loss: 0.1459 | Acc: 0.9599\n",
            "ðŸ“¦ Best model saved for Fold 2!\n",
            "\n",
            "ðŸ“Œ Fold 2 | Epoch 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 2 | Epoch 16 | Train Loss: 0.0777 | Acc: 0.9751\n",
            "âœ… Fold 2 | Epoch 16 | Val   Loss: 0.1611 | Acc: 0.9553\n",
            "âš ï¸ EarlyStopping patience: 1/3\n",
            "\n",
            "ðŸ“Œ Fold 2 | Epoch 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 2 | Epoch 17 | Train Loss: 0.0827 | Acc: 0.9734\n",
            "âœ… Fold 2 | Epoch 17 | Val   Loss: 0.1611 | Acc: 0.9612\n",
            "âš ï¸ EarlyStopping patience: 2/3\n",
            "\n",
            "ðŸ“Œ Fold 2 | Epoch 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 2 | Epoch 18 | Train Loss: 0.0675 | Acc: 0.9785\n",
            "âœ… Fold 2 | Epoch 18 | Val   Loss: 0.1447 | Acc: 0.9624\n",
            "ðŸ“¦ Best model saved for Fold 2!\n",
            "\n",
            "ðŸ“Œ Fold 2 | Epoch 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 2 | Epoch 19 | Train Loss: 0.0768 | Acc: 0.9746\n",
            "âœ… Fold 2 | Epoch 19 | Val   Loss: 0.1633 | Acc: 0.9599\n",
            "âš ï¸ EarlyStopping patience: 1/3\n",
            "\n",
            "ðŸ“Œ Fold 2 | Epoch 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 2 | Epoch 20 | Train Loss: 0.0691 | Acc: 0.9785\n",
            "âœ… Fold 2 | Epoch 20 | Val   Loss: 0.1468 | Acc: 0.9656\n",
            "âš ï¸ EarlyStopping patience: 2/3\n",
            "\n",
            "ðŸ“Œ Fold 2 | Epoch 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 2 | Epoch 21 | Train Loss: 0.0679 | Acc: 0.9779\n",
            "âœ… Fold 2 | Epoch 21 | Val   Loss: 0.1461 | Acc: 0.9645\n",
            "âš ï¸ EarlyStopping patience: 3/3\n",
            "â›” Early stopping triggered.\n",
            "âœ… Fold 2 Best model loaded.\n",
            "\n",
            "\n",
            "==============================\n",
            "ðŸ” Fold 3 / 5\n",
            "==============================\n",
            "\n",
            "\n",
            "ðŸ“Œ Fold 3 | Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 3 | Epoch 1 | Train Loss: 3.2986 | Acc: 0.4768\n",
            "âœ… Fold 3 | Epoch 1 | Val   Loss: 0.7558 | Acc: 0.8604\n",
            "ðŸ“¦ Best model saved for Fold 3!\n",
            "\n",
            "ðŸ“Œ Fold 3 | Epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 3 | Epoch 2 | Train Loss: 0.5096 | Acc: 0.8985\n",
            "âœ… Fold 3 | Epoch 2 | Val   Loss: 0.3435 | Acc: 0.9039\n",
            "ðŸ“¦ Best model saved for Fold 3!\n",
            "\n",
            "ðŸ“Œ Fold 3 | Epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 3 | Epoch 3 | Train Loss: 0.2536 | Acc: 0.9312\n",
            "âœ… Fold 3 | Epoch 3 | Val   Loss: 0.2391 | Acc: 0.9244\n",
            "ðŸ“¦ Best model saved for Fold 3!\n",
            "\n",
            "ðŸ“Œ Fold 3 | Epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 3 | Epoch 4 | Train Loss: 0.2003 | Acc: 0.9414\n",
            "âœ… Fold 3 | Epoch 4 | Val   Loss: 0.2137 | Acc: 0.9369\n",
            "ðŸ“¦ Best model saved for Fold 3!\n",
            "\n",
            "ðŸ“Œ Fold 3 | Epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 3 | Epoch 5 | Train Loss: 0.1723 | Acc: 0.9476\n",
            "âœ… Fold 3 | Epoch 5 | Val   Loss: 0.1877 | Acc: 0.9415\n",
            "ðŸ“¦ Best model saved for Fold 3!\n",
            "\n",
            "ðŸ“Œ Fold 3 | Epoch 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 3 | Epoch 6 | Train Loss: 0.1457 | Acc: 0.9540\n",
            "âœ… Fold 3 | Epoch 6 | Val   Loss: 0.1924 | Acc: 0.9427\n",
            "âš ï¸ EarlyStopping patience: 1/3\n",
            "\n",
            "ðŸ“Œ Fold 3 | Epoch 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 3 | Epoch 7 | Train Loss: 0.1340 | Acc: 0.9579\n",
            "âœ… Fold 3 | Epoch 7 | Val   Loss: 0.1996 | Acc: 0.9389\n",
            "âš ï¸ EarlyStopping patience: 2/3\n",
            "\n",
            "ðŸ“Œ Fold 3 | Epoch 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 3 | Epoch 8 | Train Loss: 0.1268 | Acc: 0.9587\n",
            "âœ… Fold 3 | Epoch 8 | Val   Loss: 0.1635 | Acc: 0.9526\n",
            "ðŸ“¦ Best model saved for Fold 3!\n",
            "\n",
            "ðŸ“Œ Fold 3 | Epoch 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 3 | Epoch 9 | Train Loss: 0.1064 | Acc: 0.9661\n",
            "âœ… Fold 3 | Epoch 9 | Val   Loss: 0.1652 | Acc: 0.9501\n",
            "âš ï¸ EarlyStopping patience: 1/3\n",
            "\n",
            "ðŸ“Œ Fold 3 | Epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 3 | Epoch 10 | Train Loss: 0.1038 | Acc: 0.9661\n",
            "âœ… Fold 3 | Epoch 10 | Val   Loss: 0.2045 | Acc: 0.9531\n",
            "âš ï¸ EarlyStopping patience: 2/3\n",
            "\n",
            "ðŸ“Œ Fold 3 | Epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 3 | Epoch 11 | Train Loss: 0.1110 | Acc: 0.9639\n",
            "âœ… Fold 3 | Epoch 11 | Val   Loss: 0.1842 | Acc: 0.9513\n",
            "âš ï¸ EarlyStopping patience: 3/3\n",
            "â›” Early stopping triggered.\n",
            "âœ… Fold 3 Best model loaded.\n",
            "\n",
            "\n",
            "==============================\n",
            "ðŸ” Fold 4 / 5\n",
            "==============================\n",
            "\n",
            "\n",
            "ðŸ“Œ Fold 4 | Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 4 | Epoch 1 | Train Loss: 3.2348 | Acc: 0.4814\n",
            "âœ… Fold 4 | Epoch 1 | Val   Loss: 0.7080 | Acc: 0.8524\n",
            "ðŸ“¦ Best model saved for Fold 4!\n",
            "\n",
            "ðŸ“Œ Fold 4 | Epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 4 | Epoch 2 | Train Loss: 0.5055 | Acc: 0.8971\n",
            "âœ… Fold 4 | Epoch 2 | Val   Loss: 0.2851 | Acc: 0.9188\n",
            "ðŸ“¦ Best model saved for Fold 4!\n",
            "\n",
            "ðŸ“Œ Fold 4 | Epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fold 4 | Epoch 3 | Train Loss: 0.2639 | Acc: 0.9279\n",
            "âœ… Fold 4 | Epoch 3 | Val   Loss: 0.2400 | Acc: 0.9301\n",
            "ðŸ“¦ Best model saved for Fold 4!\n",
            "\n",
            "ðŸ“Œ Fold 4 | Epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Fold 4:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1635/1657 [06:18<00:05,  4.33it/s, loss=0.256]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… ì„¤ì •\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import timm\n",
        "\n",
        "# âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
        "IMG_SIZE = 456\n",
        "NUM_CLASSES = 396\n",
        "TEST_DIR = \"/content/test\"\n",
        "SAMPLE_SUB_PATH = \"/content/sample_submission.csv\"\n",
        "exp_name = \"C\"\n",
        "\n",
        "# âœ… ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"âœ… Using device: {device}\")\n",
        "\n",
        "# âœ… ìƒ˜í”Œ ì œì¶œ íŒŒì¼\n",
        "sample = pd.read_csv(SAMPLE_SUB_PATH)\n",
        "column_names = sample.columns.tolist()[1:]  # 'ID' ì œì™¸\n",
        "\n",
        "# âœ… Transform ê³ ì •\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# âœ… Test Dataset (Colorë§Œ ì‚¬ìš©)\n",
        "class TestDataset_ColorOnly(Dataset):\n",
        "    def __init__(self, img_root, transform=None):\n",
        "        self.file_list = sorted([os.path.join(img_root, f) for f in os.listdir(img_root) if f.endswith('.jpg')])\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.file_list[idx]\n",
        "        image_pil = Image.open(path).convert(\"RGB\")\n",
        "\n",
        "        # Color Mean\n",
        "        image_cv2 = cv2.cvtColor(np.array(image_pil), cv2.COLOR_RGB2BGR)\n",
        "        color_mean = image_cv2.mean(axis=(0, 1))\n",
        "        color_mean = color_mean[::-1]\n",
        "        color_mean = np.array(color_mean / 255.0, dtype=np.float32)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image_pil)\n",
        "        else:\n",
        "            image = transforms.ToTensor()(image_pil)\n",
        "\n",
        "        fname = os.path.basename(path).replace(\".jpg\", \"\")\n",
        "        return image, torch.tensor(color_mean), fname\n",
        "\n",
        "# âœ… CustomModel (ì „ëžµ Cìš©)\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, use_aspect, use_color, use_pose, num_classes):\n",
        "        super(CustomModel, self).__init__()\n",
        "\n",
        "        self.use_aspect = use_aspect\n",
        "        self.use_color = use_color\n",
        "        self.use_pose = use_pose\n",
        "\n",
        "        self.backbone = timm.create_model('efficientnet_b5', pretrained=True, num_classes=0)\n",
        "        backbone_out_features = self.backbone.num_features\n",
        "\n",
        "        meta_features_dim = 0\n",
        "        if self.use_aspect:\n",
        "            meta_features_dim += 1\n",
        "        if self.use_color:\n",
        "            meta_features_dim += 3\n",
        "        if self.use_pose:\n",
        "            meta_features_dim += 4\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(backbone_out_features + meta_features_dim, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, image, aspect_ratio=None, color_mean=None, pose_feature=None):\n",
        "        x = self.backbone(image)\n",
        "\n",
        "        aux_list = []\n",
        "        if self.use_aspect:\n",
        "            aux_list.append(aspect_ratio)\n",
        "        if self.use_color:\n",
        "            aux_list.append(color_mean)\n",
        "        if self.use_pose:\n",
        "            aux_list.append(pose_feature)\n",
        "\n",
        "        if aux_list:\n",
        "            aux_features = torch.cat(aux_list, dim=1)\n",
        "            x = torch.cat([x, aux_features], dim=1)\n",
        "\n",
        "        out = self.classifier(x)\n",
        "        return out\n",
        "\n",
        "# âœ… ëª¨ë¸ ê²½ë¡œ\n",
        "FOLD_MODEL_PATHS = [\n",
        "    f\"/content/drive/MyDrive/team_models/EffNetB5_{exp_name}_fold1.pth\",\n",
        "    f\"/content/drive/MyDrive/team_models/EffNetB5_{exp_name}_fold2.pth\",\n",
        "    f\"/content/drive/MyDrive/team_models/EffNetB5_{exp_name}_fold3.pth\",\n",
        "    f\"/content/drive/MyDrive/team_models/EffNetB5_{exp_name}_fold4.pth\",\n",
        "    f\"/content/drive/MyDrive/team_models/EffNetB5_{exp_name}_fold5.pth\",\n",
        "]\n",
        "\n",
        "# âœ… DataLoader\n",
        "test_dataset = TestDataset_ColorOnly(TEST_DIR, test_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "# âœ… ì•™ìƒë¸” ê²°ê³¼\n",
        "ensemble_outputs = []\n",
        "\n",
        "# âœ… ì¶”ë¡  ë£¨í”„\n",
        "for fold_idx, model_path in enumerate(FOLD_MODEL_PATHS):\n",
        "    print(f\"\\nðŸš€ Inference with Fold {fold_idx + 1} Model: {model_path}\")\n",
        "\n",
        "    model = CustomModel(use_aspect=False, use_color=True, use_pose=False, num_classes=NUM_CLASSES)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    fold_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, color_mean, fnames in tqdm(test_loader, desc=f\"ðŸ” Fold {fold_idx + 1} Inference\"):\n",
        "            imgs, color_mean = imgs.to(device), color_mean.to(device)\n",
        "\n",
        "            outputs = model(imgs, color_mean=color_mean)\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            fold_probs.append(probs.cpu().numpy())\n",
        "\n",
        "    fold_probs = np.concatenate(fold_probs, axis=0)\n",
        "    ensemble_outputs.append(fold_probs)\n",
        "\n",
        "# âœ… ì•™ìƒë¸” í‰ê· \n",
        "ensemble_outputs = np.stack(ensemble_outputs, axis=0)\n",
        "mean_outputs = np.mean(ensemble_outputs, axis=0)\n",
        "\n",
        "# âœ… ê²°ê³¼ ì €ìž¥\n",
        "results = []\n",
        "for idx, path in enumerate(test_dataset.file_list):\n",
        "    fname = os.path.basename(path).replace(\".jpg\", \"\")\n",
        "    row = {\"ID\": fname}\n",
        "    row.update({class_name: mean_outputs[idx, i] for i, class_name in enumerate(column_names)})\n",
        "    results.append(row)\n",
        "\n",
        "submission_df = pd.DataFrame(results)\n",
        "submission_df = submission_df[[\"ID\"] + column_names]\n",
        "\n",
        "# âœ… ì €ìž¥\n",
        "SAVE_SUB_PATH = f\"/content/drive/MyDrive/team_models/submission_fold5_ensemble_{exp_name}_IMG{IMG_SIZE}.csv\"\n",
        "submission_df.to_csv(SAVE_SUB_PATH, index=False)\n",
        "\n",
        "print(f\"\\nâœ… ìµœì¢… ì œì¶œìš© CSV ì €ìž¥ ì™„ë£Œ: {SAVE_SUB_PATH}\")\n"
      ],
      "metadata": {
        "id": "IH87wo41Gn3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_3hGyrOFSWR1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}