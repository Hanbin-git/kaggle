{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 87793,
          "databundleVersionId": 12276181,
          "sourceType": "competition"
        },
        {
          "sourceId": 10855324,
          "sourceType": "datasetVersion",
          "datasetId": 6742586
        },
        {
          "sourceId": 11118830,
          "sourceType": "datasetVersion",
          "datasetId": 6933267
        },
        {
          "sourceId": 224830487,
          "sourceType": "kernelVersion"
        }
      ],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Randomness",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hanbin-git/kaggle/blob/main/RNA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "Bk4B5NutV4Br"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "stanford_rna_3d_folding_path = kagglehub.competition_download('stanford-rna-3d-folding')\n",
        "metric_usalign_path = kagglehub.dataset_download('metric/usalign')\n",
        "geraseva_protenix_checkpoints_path = kagglehub.dataset_download('geraseva/protenix-checkpoints')\n",
        "metric_ribonanza_tm_score_path = kagglehub.notebook_output_download('metric/ribonanza-tm-score')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "VomZYivsV4Bs"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_TYPE='protenix'\n",
        "VALIDATION=True"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-20T00:38:34.199448Z",
          "iopub.execute_input": "2025-05-20T00:38:34.19977Z",
          "iopub.status.idle": "2025-05-20T00:38:34.203524Z",
          "shell.execute_reply.started": "2025-05-20T00:38:34.19974Z",
          "shell.execute_reply": "2025-05-20T00:38:34.202592Z"
        },
        "id": "kN-l0GWPV4Bt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchvision torchaudio protenix"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-20T00:38:37.543554Z",
          "iopub.execute_input": "2025-05-20T00:38:37.543838Z",
          "iopub.status.idle": "2025-05-20T00:38:40.348929Z",
          "shell.execute_reply.started": "2025-05-20T00:38:37.543817Z",
          "shell.execute_reply": "2025-05-20T00:38:40.347794Z"
        },
        "id": "PwP8uXe2V4Bt",
        "outputId": "0062d5f6-b3bf-41a6-b667-f2af07850b40"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Found existing installation: torch 2.1.2+cu118\nUninstalling torch-2.1.2+cu118:\n  Successfully uninstalled torch-2.1.2+cu118\n\u001b[33mWARNING: Skipping torchvision as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping torchaudio as it is not installed.\u001b[0m\u001b[33m\n\u001b[0mFound existing installation: protenix 0.4.6\nUninstalling protenix-0.4.6:\n  Successfully uninstalled protenix-0.4.6\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --extra-index-url https://download.pytorch.org/whl/cu118 torch==2.1.2+cu118"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-20T00:40:30.670965Z",
          "iopub.execute_input": "2025-05-20T00:40:30.671283Z",
          "iopub.status.idle": "2025-05-20T00:41:24.781006Z",
          "shell.execute_reply.started": "2025-05-20T00:40:30.671258Z",
          "shell.execute_reply": "2025-05-20T00:41:24.780095Z"
        },
        "id": "xBI5o3awV4Bu",
        "outputId": "46e52629-f220-4845-c49a-83a013196590"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118\nCollecting torch==2.1.2+cu118\n  Using cached https://download.pytorch.org/whl/cu118/torch-2.1.2%2Bcu118-cp310-cp310-linux_x86_64.whl (2325.9 MB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2+cu118) (3.17.0)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2+cu118) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2+cu118) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2+cu118) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2+cu118) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2+cu118) (2024.12.0)\nRequirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2+cu118) (2.1.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.2+cu118) (3.0.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.2+cu118) (1.3.0)\nInstalling collected packages: torch\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\neasyocr 1.7.2 requires torchvision>=0.5, which is not installed.\nfastai 2.7.18 requires torchvision>=0.11, which is not installed.\nprotenix 0.4.6 requires deepspeed>=0.15.1, which is not installed.\nprotenix 0.4.6 requires icecream, which is not installed.\nprotenix 0.4.6 requires ipdb, which is not installed.\nprotenix 0.4.6 requires modelcif==0.7, which is not installed.\nprotenix 0.4.6 requires py3Dmol, which is not installed.\nprotenix 0.4.6 requires scikit-learn-extra, which is not installed.\ntimm 1.0.12 requires torchvision, which is not installed.\nprotenix 0.4.6 requires biopython==1.83, but you have biopython 1.85 which is incompatible.\nprotenix 0.4.6 requires matplotlib==3.9.2, but you have matplotlib 3.7.5 which is incompatible.\nprotenix 0.4.6 requires numpy==1.26.3, but you have numpy 1.26.4 which is incompatible.\nprotenix 0.4.6 requires protobuf==3.20.2, but you have protobuf 3.20.3 which is incompatible.\nprotenix 0.4.6 requires torch==2.3.1, but you have torch 2.1.2+cu118 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed torch-2.1.2+cu118\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --no-deps protenix==0.4.6 einops tqdm gemmi biotite==1.0.1"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-20T00:38:45.039711Z",
          "iopub.execute_input": "2025-05-20T00:38:45.039999Z",
          "iopub.status.idle": "2025-05-20T00:38:46.429731Z",
          "shell.execute_reply.started": "2025-05-20T00:38:45.039976Z",
          "shell.execute_reply": "2025-05-20T00:38:46.428876Z"
        },
        "id": "grvOqzo5V4Bu",
        "outputId": "f30f1e4e-f382-4d7b-e198-6d732ecabc2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting protenix==0.4.6\n  Using cached protenix-0.4.6-py3-none-any.whl.metadata (1.1 kB)\nRequirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\nRequirement already satisfied: gemmi in /usr/local/lib/python3.10/dist-packages (0.7.1)\nRequirement already satisfied: biotite==1.0.1 in /usr/local/lib/python3.10/dist-packages (1.0.1)\nUsing cached protenix-0.4.6-py3-none-any.whl (441 kB)\nInstalling collected packages: protenix\nSuccessfully installed protenix-0.4.6\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install requirements"
      ],
      "metadata": {
        "id": "rnVW8LfbV4Bu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if MODEL_TYPE=='protenix' and VALIDATION:\n",
        "    !pip install biopython\n",
        "    !pip install ml-collections\n",
        "    !pip install rdkit\n",
        "!export PROTENIX_DATA_ROOT_DIR=/kaggle/input/protenix-checkpoints"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-20T00:37:01.642772Z",
          "iopub.execute_input": "2025-05-20T00:37:01.643053Z",
          "iopub.status.idle": "2025-05-20T00:37:11.777624Z",
          "shell.execute_reply.started": "2025-05-20T00:37:01.643031Z",
          "shell.execute_reply": "2025-05-20T00:37:11.776639Z"
        },
        "id": "WMOLpdXRV4Bv",
        "outputId": "912f6b0b-fbb0-4b67-c426-ed37b962cc00"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: biopython in /usr/local/lib/python3.10/dist-packages (1.85)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->biopython) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->biopython) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->biopython) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->biopython) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->biopython) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->biopython) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->biopython) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->biopython) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->biopython) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->biopython) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->biopython) (2024.2.0)\nRequirement already satisfied: ml-collections in /usr/local/lib/python3.10/dist-packages (1.1.0)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from ml-collections) (1.4.0)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from ml-collections) (6.0.2)\nRequirement already satisfied: rdkit in /usr/local/lib/python3.10/dist-packages (2025.3.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.26.4)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (11.0.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->rdkit) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->rdkit) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->rdkit) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->rdkit) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->rdkit) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->rdkit) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rdkit) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rdkit) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->rdkit) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->rdkit) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->rdkit) (2024.2.0)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir /af3-dev\n",
        "! ln -s /kaggle/input/protenix-checkpoints /af3-dev/release_data\n",
        "! ls /af3-dev/release_data/"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-20T00:38:19.696549Z",
          "iopub.execute_input": "2025-05-20T00:38:19.696888Z",
          "iopub.status.idle": "2025-05-20T00:38:20.039511Z",
          "shell.execute_reply.started": "2025-05-20T00:38:19.696865Z",
          "shell.execute_reply": "2025-05-20T00:38:20.038672Z"
        },
        "id": "IWtgdocIV4Bv",
        "outputId": "3ddeab09-ee48-4b69-9ac9-ff5201276840"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "mkdir: cannot create directory ‚Äò/af3-dev‚Äô: File exists\nln: failed to create symbolic link '/af3-dev/release_data/protenix-checkpoints': Read-only file system\ncomponents.v20240608.cif\t\tmodel_v0.2.0.pt\ncomponents.v20240608.cif.rdkit_mol.pkl\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper scripts"
      ],
      "metadata": {
        "id": "79WPpnhiV4Bv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import Bio\n",
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "import pandas as pd\n",
        "from Bio.PDB import Atom, Model, Chain, Residue, Structure, PDBParser\n",
        "from Bio import SeqIO\n",
        "import os, sys\n",
        "import re\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "time0=time.time()\n",
        "\n",
        "print('IMPORT OK !!!!')"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2025-05-20T00:41:39.764526Z",
          "iopub.execute_input": "2025-05-20T00:41:39.764808Z",
          "iopub.status.idle": "2025-05-20T00:41:41.686387Z",
          "shell.execute_reply.started": "2025-05-20T00:41:39.764784Z",
          "shell.execute_reply": "2025-05-20T00:41:41.685621Z"
        },
        "trusted": true,
        "id": "xzOihvfbV4Bv",
        "outputId": "a676f71b-c704-468a-a4c6-5f21ef6f03ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "IMPORT OK !!!!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from Bio.PDB import PDBParser\n",
        "\n",
        "# üß± ÌôòÍ≤Ω ÏÑ§Ï†ï\n",
        "PYTHON = sys.executable\n",
        "print('PYTHON', PYTHON)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-20T00:58:50.711324Z",
          "iopub.execute_input": "2025-05-20T00:58:50.711817Z",
          "iopub.status.idle": "2025-05-20T00:58:50.716931Z",
          "shell.execute_reply.started": "2025-05-20T00:58:50.711788Z",
          "shell.execute_reply": "2025-05-20T00:58:50.716257Z"
        },
        "id": "u1eRz4ZOV4Bw",
        "outputId": "9462b807-3279-4901-8b4c-73b786fd3970"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "PYTHON /usr/bin/python3\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "RHONET_DIR = '/kaggle/input/data-for-demo-for-rhofold-plus-with-kaggle-msa/RhoFold-main'\n",
        "USALIGN = '/kaggle/working/USalign'\n",
        "\n",
        "os.system('cp /kaggle/input/usalign/USalign /kaggle/working/')\n",
        "os.system('chmod u+x /kaggle/working/USalign')  # sudoÎäî ÌïÑÏöî ÏóÜÏùå\n",
        "sys.path.append(RHONET_DIR)\n",
        "\n",
        "DATA_KAGGLE_DIR = '/kaggle/input/stanford-rna-3d-folding'\n",
        "\n",
        "# üßæ dotdict\n",
        "class dotdict(dict):\n",
        "    __setattr__ = dict.__setitem__\n",
        "    __delattr__ = dict.__delitem__\n",
        "\n",
        "    def __getattr__(self, name):\n",
        "        try:\n",
        "            return self[name]\n",
        "        except KeyError:\n",
        "            raise AttributeError(name)\n",
        "\n",
        "# üìä ÏãúÍ∞ÅÌôîÏö©\n",
        "def set_aspect_equal(ax):\n",
        "    x_limits = ax.get_xlim()\n",
        "    y_limits = ax.get_ylim()\n",
        "    z_limits = ax.get_zlim()\n",
        "    x_middle, y_middle, z_middle = np.mean(x_limits), np.mean(y_limits), np.mean(z_limits)\n",
        "    max_range = max(\n",
        "        x_limits[1] - x_limits[0],\n",
        "        y_limits[1] - y_limits[0],\n",
        "        z_limits[1] - z_limits[0]\n",
        "    ) / 2.0\n",
        "    ax.set_xlim(x_middle - max_range, x_middle + max_range)\n",
        "    ax.set_ylim(y_middle - max_range, y_middle + max_range)\n",
        "    ax.set_zlim(z_middle - max_range, z_middle + max_range)\n",
        "\n",
        "# üìå ÏòàÏ∏° output ‚Üí DataFrame Î≥ÄÌôò\n",
        "def parse_output_to_df(output, seq, target_id):\n",
        "    if isinstance(output, torch.Tensor):\n",
        "        output = output.detach().cpu().numpy()\n",
        "    output = output.squeeze()  # [L, 3]\n",
        "    chain_data = []\n",
        "    for i, res in enumerate(seq):\n",
        "        x, y, z = output[i]\n",
        "        chain_data.append(dict(\n",
        "            ID=target_id,\n",
        "            resname=res,\n",
        "            resid=i + 1,\n",
        "            x_1=round(x, 3),\n",
        "            y_1=round(y, 3),\n",
        "            z_1=round(z, 3)\n",
        "        ))\n",
        "    return [pd.DataFrame(chain_data)]\n",
        "\n",
        "# üìå GT PDB ‚Üí DataFrame Î≥ÄÌôò\n",
        "def parse_pdb_to_df(pdb_file, target_id):\n",
        "    parser = PDBParser()\n",
        "    structure = parser.get_structure('', pdb_file)\n",
        "\n",
        "    df_list = []\n",
        "    for model in structure:\n",
        "        for chain in model:\n",
        "            chain_data = []\n",
        "            for residue in chain:\n",
        "                if residue.get_resname() in ['A', 'U', 'G', 'C']:\n",
        "                    if 'C1\\'' in residue:\n",
        "                        atom = residue['C1\\'']\n",
        "                        x, y, z = atom.get_coord()\n",
        "                        resid = residue.get_id()[1]\n",
        "                        chain_data.append(dict(\n",
        "                            ID=target_id + '_' + str(resid),\n",
        "                            resname=residue.get_resname(),\n",
        "                            resid=resid,\n",
        "                            x_1=x,\n",
        "                            y_1=y,\n",
        "                            z_1=z\n",
        "                        ))\n",
        "            if chain_data:\n",
        "                df_list.append(pd.DataFrame(chain_data))\n",
        "    return df_list\n",
        "\n",
        "# üìå Ï¢åÌëú ‚Üí PDB ÎùºÏù∏ ÏÉùÏÑ±\n",
        "def write_target_line(atom_name, atom_serial, residue_name, chain_id,\n",
        "                      residue_num, x_coord, y_coord, z_coord,\n",
        "                      occupancy=1.0, b_factor=0.0, atom_type='C'):\n",
        "    return (\n",
        "        f'ATOM  {atom_serial:>5d}  {atom_name:<5s} {residue_name:<3s} '\n",
        "        f'{residue_num:>3d}    {x_coord:>8.3f}{y_coord:>8.3f}{z_coord:>8.3f}'\n",
        "        f'{occupancy:>6.2f}{b_factor:>6.2f}           {atom_type}\\n'\n",
        "    )\n",
        "\n",
        "# üìå DataFrame ‚Üí PDB Ï†ÄÏû•\n",
        "def write_xyz_to_pdb(df, pdb_file, xyz_id=1):\n",
        "    resolved_cnt = 0\n",
        "    with open(pdb_file, 'w') as f:\n",
        "        for _, row in df.iterrows():\n",
        "            x, y, z = row[f'x_{xyz_id}'], row[f'y_{xyz_id}'], row[f'z_{xyz_id}']\n",
        "            if x > -1e17 and y > -1e17 and z > -1e17:\n",
        "                resolved_cnt += 1\n",
        "                f.write(write_target_line(\n",
        "                    atom_name=\"C1'\",\n",
        "                    atom_serial=int(row['resid']),\n",
        "                    residue_name=row['resname'],\n",
        "                    chain_id='0',\n",
        "                    residue_num=int(row['resid']),\n",
        "                    x_coord=x, y_coord=y, z_coord=z,\n",
        "                    atom_type='C'\n",
        "                ))\n",
        "    return resolved_cnt\n",
        "\n",
        "# üìå USalign ‚Üí TM-score Ï∂îÏ∂ú\n",
        "def parse_usalign_for_tm_score(output):\n",
        "    matches = re.findall(r'TM-score=\\s+([\\d.]+)', output)\n",
        "    if len(matches) < 2:\n",
        "        raise ValueError(\"TM-score Ï∂îÏ∂ú Ïã§Ìå®\")\n",
        "    return float(matches[1])\n",
        "\n",
        "# üìå USalign ‚Üí ÌöåÏ†ÑÌñâÎ†¨ Ï∂îÏ∂ú\n",
        "def parse_usalign_for_transform(output):\n",
        "    lines = output.splitlines()\n",
        "    matrix_lines = []\n",
        "    in_matrix = False\n",
        "    for line in lines:\n",
        "        if \"The rotation matrix to rotate Structure_1 to Structure_2\" in line:\n",
        "            in_matrix = True\n",
        "        elif in_matrix and re.match(r'^\\d+\\s+[-\\d.]+\\s+[-\\d.]+\\s+[-\\d.]+\\s+[-\\d.]+$', line):\n",
        "            matrix_lines.append(line)\n",
        "        elif in_matrix and not line.strip():\n",
        "            break\n",
        "    matrix = [list(map(float, l.split()[1:])) for l in matrix_lines]\n",
        "    return np.array(matrix)\n",
        "\n",
        "# üìå ÌèâÍ∞Ä Ï†ÑÏ≤¥ Ïã§Ìñâ\n",
        "def call_usalign(predict_df, truth_df, usalign_bin=USALIGN, verbose=True):\n",
        "    pred_pdb, gt_pdb = '~pred.pdb', '~truth.pdb'\n",
        "    write_xyz_to_pdb(predict_df, pred_pdb)\n",
        "    write_xyz_to_pdb(truth_df, gt_pdb)\n",
        "    cmd = f'{usalign_bin} {pred_pdb} {gt_pdb} -atom \" C1\\'\" -m -'\n",
        "    output = os.popen(cmd).read()\n",
        "    if verbose:\n",
        "        print(output)\n",
        "    return parse_usalign_for_tm_score(output), parse_usalign_for_transform(output)\n",
        "\n",
        "print('‚úÖ RNA Íµ¨Ï°∞ ÌèâÍ∞ÄÏö© HELPER Î°úÎî© ÏôÑÎ£å!')\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-20T01:01:01.310524Z",
          "iopub.execute_input": "2025-05-20T01:01:01.310815Z",
          "iopub.status.idle": "2025-05-20T01:01:01.37097Z",
          "shell.execute_reply.started": "2025-05-20T01:01:01.310793Z",
          "shell.execute_reply": "2025-05-20T01:01:01.370275Z"
        },
        "id": "3LcwvlamV4Bw",
        "outputId": "ead59682-b77d-42da-bad2-5aeb9b51bf87"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "‚úÖ RNA Íµ¨Ï°∞ ÌèâÍ∞ÄÏö© HELPER Î°úÎî© ÏôÑÎ£å!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "if MODEL_TYPE=='protenix':\n",
        "\n",
        "\n",
        "    from runner.batch_inference import get_default_runner\n",
        "    from runner.inference import update_inference_configs, InferenceRunner\n",
        "\n",
        "    from protenix.data.infer_data_pipeline import InferenceDataset\n",
        "\n",
        "    np.random.seed(0)\n",
        "    torch.random.manual_seed(0)\n",
        "    torch.cuda.manual_seed_all(0)\n",
        "\n",
        "    class DictDataset:\n",
        "        def __init__(\n",
        "            self,\n",
        "            seq_list: list,\n",
        "            dump_dir: str,\n",
        "            id_list: list = None,\n",
        "            use_msa: bool = False,\n",
        "        ) -> None:\n",
        "            assert isinstance(seq_list, list), \"seq_listÎäî Î¶¨Ïä§Ìä∏Ïó¨Ïïº Ìï©ÎãàÎã§\"\n",
        "            if id_list is not None:\n",
        "                assert len(seq_list) == len(id_list), \"seq_listÏôÄ id_list Í∏∏Ïù¥ Îã§Î¶Ñ\"\n",
        "\n",
        "            self.dump_dir = dump_dir\n",
        "            self.use_msa = use_msa\n",
        "\n",
        "            if id_list is None:\n",
        "                self.inputs = [{\n",
        "                    \"sequences\": [{\n",
        "                        \"rnaSequence\": {\n",
        "                            \"sequence\": seq,\n",
        "                            \"count\": 1\n",
        "                        }\n",
        "                    }],\n",
        "                    \"name\": f\"query_{idx}\"\n",
        "                } for idx, seq in enumerate(seq_list)]\n",
        "            else:\n",
        "                self.inputs = [{\n",
        "                    \"sequences\": [{\n",
        "                        \"rnaSequence\": {\n",
        "                            \"sequence\": seq,\n",
        "                            \"count\": 1\n",
        "                        }\n",
        "                    }],\n",
        "                    \"name\": id_\n",
        "                } for id_, seq in zip(id_list, seq_list)]\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.inputs)\n",
        "\n",
        "        def __getitem__(self, index):\n",
        "            # InferenceDatasetÏù¥ Ï†úÍ≥µÌïòÎäî Î∞©ÏãùÍ≥º Ïú†ÏÇ¨ÌïòÍ≤å Î∞òÌôò\n",
        "            from protenix.data.infer_data_pipeline import convert_input_to_data\n",
        "            input_json = self.inputs[index]\n",
        "            data, atom_array, error_message = convert_input_to_data(input_json, dump_dir=self.dump_dir, use_msa=self.use_msa)\n",
        "            return data, atom_array, error_message\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-20T01:31:43.160035Z",
          "iopub.execute_input": "2025-05-20T01:31:43.160427Z",
          "iopub.status.idle": "2025-05-20T01:31:43.16788Z",
          "shell.execute_reply.started": "2025-05-20T01:31:43.160396Z",
          "shell.execute_reply": "2025-05-20T01:31:43.167163Z"
        },
        "id": "xvkxAPyxV4Bx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "from distutils.util import strtobool  # Î¨∏ÏûêÏó¥ to bool Î≥ÄÌôò\n",
        "\n",
        "if MODEL_TYPE == 'protenix':\n",
        "\n",
        "    # configs import\n",
        "    from configs.configs_base import configs as configs_base\n",
        "    from configs.configs_data import data_configs\n",
        "    from configs.configs_inference import inference_configs\n",
        "    from protenix.config.config import parse_configs\n",
        "    from runner.inference import InferenceRunner\n",
        "\n",
        "    # ‚úÖ config ÍπäÏùÄ Î≥µÏÇ¨ ‚Üí ÏõêÎ≥∏ Î≥¥Ï°¥\n",
        "    base_config = copy.deepcopy(configs_base)\n",
        "\n",
        "    # ‚úÖ ÌôòÍ≤Ω Î≥ÄÏàò boolean Ï≤òÎ¶¨ Í∞úÏÑ†\n",
        "    base_config[\"use_deepspeed_evo_attention\"] = bool(strtobool(os.environ.get(\"USE_DEEPSPEED_EVO_ATTENTION\", \"false\")))\n",
        "\n",
        "    # ‚úÖ Î™®Îç∏, ÏÉòÌîåÎßÅ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÏàòÏ†ï\n",
        "    base_config[\"model\"][\"N_cycle\"] = 10\n",
        "    base_config[\"sample_diffusion\"][\"N_sample\"] = 5 if not VALIDATION else 1\n",
        "    base_config[\"sample_diffusion\"][\"N_step\"] = 200\n",
        "\n",
        "    # ‚úÖ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Í≤ΩÎ°ú ÏßÄÏ†ï\n",
        "    inference_configs['load_checkpoint_path'] = '/kaggle/input/protenix-checkpoints/model_v0.2.0.pt'\n",
        "\n",
        "    # ‚úÖ configs Î≥ëÌï© Î∞è ÌååÏã±\n",
        "    configs = {**base_config, **{\"data\": data_configs}, **inference_configs}\n",
        "    configs = parse_configs(\n",
        "        configs=configs,\n",
        "        fill_required_with_null=True,\n",
        "    )\n",
        "\n",
        "    # ‚úÖ Inference Runner Ï§ÄÎπÑ\n",
        "    runner = InferenceRunner(configs)\n",
        "\n",
        "print(\"‚úÖ Î™®Îç∏ Î°úÎî© ÏôÑÎ£å Ïó¨Î∂Ä:\", runner.model is not None)\n",
        "print(\"üìÇ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Í≤ΩÎ°ú:\", configs['load_checkpoint_path'])\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-20T01:31:48.16074Z",
          "iopub.execute_input": "2025-05-20T01:31:48.161046Z",
          "iopub.status.idle": "2025-05-20T01:31:53.730146Z",
          "shell.execute_reply.started": "2025-05-20T01:31:48.161025Z",
          "shell.execute_reply": "2025-05-20T01:31:53.729409Z"
        },
        "id": "cRcGb1BjV4Bx",
        "outputId": "40fac134-539c-4620-dc00-3cb89df3938c"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "train scheduler 16.0\ninference scheduler 16.0\nDiffusion Module has 16.0\n‚úÖ Î™®Îç∏ Î°úÎî© ÏôÑÎ£å Ïó¨Î∂Ä: True\nüìÇ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Í≤ΩÎ°ú: /kaggle/input/protenix-checkpoints/model_v0.2.0.pt\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "\n",
        "if VALIDATION:\n",
        "    LABEL_DF = pd.read_csv('/kaggle/input/stanford-rna-3d-folding/train_labels.csv')\n",
        "    LABEL_DF['target_id'] = LABEL_DF['ID'].apply(lambda x: '_'.join(x.split('_')[:-1]))\n",
        "    train_df = pd.read_csv('/kaggle/input/stanford-rna-3d-folding/train_sequences.csv')\n",
        "\n",
        "    # ‚úÖ Ïó¨Í∏∞ Ï∂îÍ∞Ä\n",
        "    def get_truth_df(target_id):\n",
        "        truth_df = LABEL_DF[LABEL_DF['target_id'] == target_id]\n",
        "        return truth_df.reset_index(drop=True)\n",
        "\n",
        "if MODEL_TYPE == 'protenix' and VALIDATION:\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "    time0 = time.time()\n",
        "\n",
        "    train_df['protenix_tm_score'] = None\n",
        "\n",
        "    # ‚¨áÔ∏è Ïó¨Í∏∞Í∞Ä ÏàòÏ†ïÎêòÎäî ÏúÑÏπò\n",
        "    dataset = DictDataset(\n",
        "        seq_list=train_df.sequence.tolist(),\n",
        "        dump_dir='output',\n",
        "        id_list=train_df.target_id.tolist(),\n",
        "        use_msa=False\n",
        "    )\n",
        "\n",
        "    num_data = len(dataset)\n",
        "\n",
        "    for i, seq in tqdm(enumerate(train_df.sequence), total=num_data):\n",
        "        if pd.notnull(train_df.loc[i, 'protenix_tm_score']):\n",
        "            continue\n",
        "        if len(seq) > 300:\n",
        "            continue\n",
        "\n",
        "        target_id = train_df.loc[i, 'target_id']\n",
        "        truth_df = get_truth_df(target_id)\n",
        "\n",
        "        if sum(~np.isnan(truth_df.x_1)) < 3:\n",
        "            continue\n",
        "\n",
        "        data, atom_array, data_error_message = dataset[i]\n",
        "        if data_error_message != '':\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            new_configs = update_inference_configs(configs, data[\"N_token\"].item())\n",
        "            runner.update_model_configs(new_configs)\n",
        "\n",
        "            prediction = runner.predict(data)\n",
        "            prediction = prediction['coordinate'][:, data['input_feature_dict']['atom_to_tokatom_idx'] == 12]\n",
        "\n",
        "            # Optional smoothing (TM-score Ìñ•ÏÉÅ Ïó¨ÏßÄ ÏûàÏùå)\n",
        "            # def smooth_coords(coords, window=3):\n",
        "            #     coords = coords.squeeze()\n",
        "            #     smoothed = np.copy(coords)\n",
        "            #     for j in range(1, len(coords)-1):\n",
        "            #         smoothed[j] = np.mean(coords[j-1:j+2], axis=0)\n",
        "            #     return smoothed\n",
        "            # prediction = smooth_coords(prediction[:1])\n",
        "\n",
        "            result = parse_output_to_df(prediction[:1], seq, target_id)[0]\n",
        "\n",
        "            tm_score, transform = call_usalign(result, truth_df, verbose=0)\n",
        "            train_df.loc[i, 'protenix_tm_score'] = tm_score\n",
        "\n",
        "        except Exception as e:\n",
        "            # Optional debug: print(f\"‚ö†Ô∏è {target_id} ÌèâÍ∞Ä Ïã§Ìå®: {e}\")\n",
        "            continue\n",
        "\n",
        "        if (time.time() - time0) > (12 * 3600 - 360):\n",
        "            print(\"‚ö†Ô∏è Ïã§Ìñâ ÏãúÍ∞Ñ ÌïúÎèÑ ÎèÑÎã¨Î°ú Ï§ëÎã®\")\n",
        "            break\n",
        "\n",
        "    # ‚úÖ Í≤∞Í≥º Ï†ÄÏû•\n",
        "    train_df.to_csv('/kaggle/working/tm_scores.csv', index=False)\n",
        "\n",
        "    # ‚úÖ ÌÜµÍ≥Ñ Ï∂úÎ†• Î∞è ÏãúÍ∞ÅÌôî\n",
        "    print(\"üìà ÌèâÍ∑† TM-score:\", train_df['protenix_tm_score'].astype(float).mean())\n",
        "    train_df['protenix_tm_score'].astype(float).hist(bins=50)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-20T01:31:58.466379Z",
          "iopub.execute_input": "2025-05-20T01:31:58.466683Z",
          "iopub.status.idle": "2025-05-20T01:31:58.726442Z",
          "shell.execute_reply.started": "2025-05-20T01:31:58.46666Z",
          "shell.execute_reply": "2025-05-20T01:31:58.725206Z"
        },
        "id": "bYlnfH74V4By",
        "outputId": "dc471b81-53a8-4403-f01d-ea30f822dbd1"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "  0%|          | 0/844 [00:00<?, ?it/s]\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-d23be3f808a8>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matom_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_error_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata_error_message\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-2ac652b2ebf6>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;31m# InferenceDatasetÏù¥ Ï†úÍ≥µÌïòÎäî Î∞©ÏãùÍ≥º Ïú†ÏÇ¨ÌïòÍ≤å Î∞òÌôò\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mprotenix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_data_pipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_input_to_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0minput_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matom_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_input_to_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdump_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_msa\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'convert_input_to_data' from 'protenix.data.infer_data_pipeline' (/usr/local/lib/python3.10/dist-packages/protenix/data/infer_data_pipeline.py)"
          ],
          "ename": "ImportError",
          "evalue": "cannot import name 'convert_input_to_data' from 'protenix.data.infer_data_pipeline' (/usr/local/lib/python3.10/dist-packages/protenix/data/infer_data_pipeline.py)",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "if MODEL_TYPE=='protenix' and not VALIDATION:\n",
        "    test_df=pd.read_csv('/kaggle/input/stanford-rna-3d-folding/test_sequences.csv')\n",
        "    import warnings\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "    dataset = DictDataset(test_df.sequence, dump_dir='output', id_list=test_df.target_id, use_msa=False)\n",
        "    num_data = len(dataset)\n",
        "    for i, seq in tqdm(enumerate(test_df.sequence),total=num_data):\n",
        "        try:\n",
        "            data, atom_array, data_error_message=dataset[i]\n",
        "            target_id = data[\"sample_name\"]\n",
        "            assert target_id==test_df.target_id[i]\n",
        "            assert data_error_message==''\n",
        "\n",
        "            new_configs = update_inference_configs(configs, data[\"N_token\"].item())\n",
        "            runner.update_model_configs(new_configs)\n",
        "            prediction = runner.predict(data)\n",
        "            prediction=prediction['coordinate'][:,data['input_feature_dict']['atom_to_tokatom_idx']==12]\n",
        "\n",
        "            result = parse_output_to_df(prediction, seq, target_id)[0]\n",
        "        except:\n",
        "            target_id==test_df.target_id[i]\n",
        "            print('Failed to predict', target_id)\n",
        "            result=pd.DataFrame(columns=['ID', 'resname', 'resid',\n",
        "                                         'x_1', 'y_1', 'z_1',\n",
        "                                         'x_2', 'y_2', 'z_2',\n",
        "                                         'x_3', 'y_3', 'z_3',\n",
        "                                         'x_4', 'y_4', 'z_4',\n",
        "                                         'x_5', 'y_5', 'z_5'],\n",
        "                                         data=[[target_id, x, j+1] + [0.0]*15 for j, x in enumerate(seq)])\n",
        "\n",
        "        result['ID']=result.apply(lambda x: x.ID + '_' + str(x.resid), axis=1)\n",
        "        result.to_csv('submission.csv', index=False, mode='a', header=(i==0))\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    display(pd.read_csv('submission.csv'))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-28T16:47:17.57656Z",
          "iopub.status.idle": "2025-03-28T16:47:17.576943Z",
          "shell.execute_reply": "2025-03-28T16:47:17.576784Z"
        },
        "id": "BhBUhEvJV4By"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}