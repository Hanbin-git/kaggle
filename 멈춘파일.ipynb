{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 12058286,
          "sourceType": "datasetVersion",
          "datasetId": 7589403
        }
      ],
      "dockerImageVersionId": 31041,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "notebookef924bbad8",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hanbin-git/kaggle/blob/main/%EB%A9%88%EC%B6%98%ED%8C%8C%EC%9D%BC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "IcftcFesUuHu"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "biniroun_car_image_path = kagglehub.dataset_download('biniroun/car-image')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "zswLbcF0UuHw"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# 필수 라이브러리 설치 (Kaggle Notebook 기본적으로 torch, timm 포함됨)\n",
        "!pip install timm --quiet\n",
        "!pip install albumentations --quiet\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-04T16:28:24.660597Z",
          "iopub.execute_input": "2025-06-04T16:28:24.660896Z",
          "iopub.status.idle": "2025-06-04T16:28:30.718545Z",
          "shell.execute_reply.started": "2025-06-04T16:28:24.660871Z",
          "shell.execute_reply": "2025-06-04T16:28:30.717517Z"
        },
        "id": "Do6rK3XlUuHw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Device 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"✅ Using device: {device}\")\n",
        "\n",
        "# 모델 정의 (예: EfficientNet-B5)\n",
        "import timm\n",
        "model = timm.create_model('tf_efficientnet_b5', pretrained=True, num_classes=396)\n",
        "\n",
        "# ✅ 멀티 GPU 적용 (DataParallel)\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(f\"✅ Using {torch.cuda.device_count()} GPUs with DataParallel\")\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "# ✅ 모델을 device로 이동\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-04T16:28:46.033148Z",
          "iopub.execute_input": "2025-06-04T16:28:46.033715Z",
          "iopub.status.idle": "2025-06-04T16:28:51.082231Z",
          "shell.execute_reply.started": "2025-06-04T16:28:46.033685Z",
          "shell.execute_reply": "2025-06-04T16:28:51.081344Z"
        },
        "id": "ZkbG4JeIUuHx",
        "outputId": "79c3bc63-ff12-4f68-9af9-f0821055c775"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "✅ Using device: cuda\n✅ Using 2 GPUs with DataParallel\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# /kaggle/input/car-image 내부 확인\n",
        "print(\"✅ /kaggle/input/car-image 폴더 내용:\", os.listdir('/kaggle/input/car-image'))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-04T16:29:46.840754Z",
          "iopub.execute_input": "2025-06-04T16:29:46.841488Z",
          "iopub.status.idle": "2025-06-04T16:29:46.8473Z",
          "shell.execute_reply.started": "2025-06-04T16:29:46.841461Z",
          "shell.execute_reply": "2025-06-04T16:29:46.846613Z"
        },
        "id": "MIokiNj5UuHy",
        "outputId": "748200ae-d726-4779-e6ce-410f15e9206e"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "✅ /kaggle/input/car-image 폴더 내용: ['sample_submission.csv', 'test.csv', 'test', 'train']\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Train 이미지 경로 확인\n",
        "import glob\n",
        "\n",
        "train_files = glob.glob('/kaggle/input/car-image/train/*/*.jpg')\n",
        "print(f\"✅ Train 이미지 수: {len(train_files)}\")\n",
        "print(\"샘플:\", train_files[:3])\n",
        "\n",
        "test_files = glob.glob('/kaggle/input/car-image/test/*.jpg')\n",
        "print(f\"✅ Test 이미지 수: {len(test_files)}\")\n",
        "print(\"샘플:\", test_files[:3])\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "sample_submission = pd.read_csv('/kaggle/input/car-image/sample_submission.csv')\n",
        "print(sample_submission.head())\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-04T16:29:49.519886Z",
          "iopub.execute_input": "2025-06-04T16:29:49.520505Z",
          "iopub.status.idle": "2025-06-04T16:29:50.798225Z",
          "shell.execute_reply.started": "2025-06-04T16:29:49.52048Z",
          "shell.execute_reply": "2025-06-04T16:29:50.797546Z"
        },
        "id": "hh0hUK-1UuHy",
        "outputId": "41bada5a-4bd6-4f0b-f3b1-b4e9b8a86d12"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "✅ Train 이미지 수: 33137\n샘플: ['/kaggle/input/car-image/train/3시리즈_G20_2019_2022/3시리즈_G20_2019_2022_0002.jpg', '/kaggle/input/car-image/train/3시리즈_G20_2019_2022/3시리즈_G20_2019_2022_0016.jpg', '/kaggle/input/car-image/train/3시리즈_G20_2019_2022/3시리즈_G20_2019_2022_0037.jpg']\n✅ Test 이미지 수: 8258\n샘플: ['/kaggle/input/car-image/test/TEST_04038.jpg', '/kaggle/input/car-image/test/TEST_07075.jpg', '/kaggle/input/car-image/test/TEST_04342.jpg']\n           ID  1시리즈_F20_2013_2015  1시리즈_F20_2016_2019  1시리즈_F40_2020_2024  \\\n0  TEST_00000                   1                 0.0                 0.0   \n1  TEST_00001                   1                 0.0                 0.0   \n2  TEST_00002                   1                 0.0                 0.0   \n3  TEST_00003                   1                 0.0                 0.0   \n4  TEST_00004                   1                 0.0                 0.0   \n\n   2008_2015_2017  2시리즈_그란쿠페_F44_2020_2024  2시리즈_액티브_투어러_F45_2019_2021  \\\n0             0.0                      0.0                         0.0   \n1             0.0                      0.0                         0.0   \n2             0.0                      0.0                         0.0   \n3             0.0                      0.0                         0.0   \n4             0.0                      0.0                         0.0   \n\n   2시리즈_액티브_투어러_U06_2022_2024  3008_2세대_2018_2023  3시리즈_E90_2005_2012  ...  \\\n0                         0.0                 0.0                 0.0  ...   \n1                         0.0                 0.0                 0.0  ...   \n2                         0.0                 0.0                 0.0  ...   \n3                         0.0                 0.0                 0.0  ...   \n4                         0.0                 0.0                 0.0  ...   \n\n   티볼리_에어_2021_2022  파나메라_2010_2016  파나메라_971_2017_2023  파사트_GT_B8_2018_2022  \\\n0               0.0             0.0                 0.0                  0.0   \n1               0.0             0.0                 0.0                  0.0   \n2               0.0             0.0                 0.0                  0.0   \n3               0.0             0.0                 0.0                  0.0   \n4               0.0             0.0                 0.0                  0.0   \n\n   파일럿_3세대_2016_2018  팰리세이드_2019_2022  팰리세이드_LX3_2025  프리우스_4세대_2016_2018  \\\n0                0.0              0.0             0.0                 0.0   \n1                0.0              0.0             0.0                 0.0   \n2                0.0              0.0             0.0                 0.0   \n3                0.0              0.0             0.0                 0.0   \n4                0.0              0.0             0.0                 0.0   \n\n   프리우스_4세대_2019_2022  프리우스_C_2018_2020  \n0                 0.0               0.0  \n1                 0.0               0.0  \n2                 0.0               0.0  \n3                 0.0               0.0  \n4                 0.0               0.0  \n\n[5 rows x 397 columns]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# label encoding 필요\n",
        "import os\n",
        "\n",
        "# 1️⃣ Train 디렉토리 경로\n",
        "train_dir = '/kaggle/input/car-image/train'\n",
        "\n",
        "# 2️⃣ 클래스명 리스트 (폴더명 기준 → 정렬)\n",
        "class_names = sorted(os.listdir(train_dir))\n",
        "\n",
        "# 3️⃣ 클래스 수 확인\n",
        "num_classes = len(class_names)\n",
        "print(f\"✅ 클래스 수: {num_classes}\")\n",
        "print(\"샘플 클래스명:\", class_names[:5])\n",
        "\n",
        "# 4️⃣ 클래스명 → label (index) 매핑\n",
        "class_to_idx = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
        "idx_to_class = {idx: class_name for class_name, idx in class_to_idx.items()}\n",
        "\n",
        "# 5️⃣ 샘플 출력 확인\n",
        "print(\"\\n✅ class_to_idx 샘플:\")\n",
        "for i, (k, v) in enumerate(class_to_idx.items()):\n",
        "    print(f\"{k} --> {v}\")\n",
        "    if i >= 4:\n",
        "        break\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-04T16:29:54.111784Z",
          "iopub.execute_input": "2025-06-04T16:29:54.11206Z",
          "iopub.status.idle": "2025-06-04T16:29:54.119575Z",
          "shell.execute_reply.started": "2025-06-04T16:29:54.112041Z",
          "shell.execute_reply": "2025-06-04T16:29:54.118808Z"
        },
        "id": "nzHCxuh7UuHy",
        "outputId": "cc2a7c35-3828-4dd3-b411-865ae2be5805"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "✅ 클래스 수: 396\n샘플 클래스명: ['1시리즈_F20_2013_2015', '1시리즈_F20_2016_2019', '1시리즈_F40_2020_2024', '2008_2015_2017', '2시리즈_그란쿠페_F44_2020_2024']\n\n✅ class_to_idx 샘플:\n1시리즈_F20_2013_2015 --> 0\n1시리즈_F20_2016_2019 --> 1\n1시리즈_F40_2020_2024 --> 2\n2008_2015_2017 --> 3\n2시리즈_그란쿠페_F44_2020_2024 --> 4\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "import torch.nn as nn\n",
        "\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, use_aspect=False, use_color=False, num_classes=396):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.use_aspect = use_aspect\n",
        "        self.use_color = use_color\n",
        "\n",
        "        self.backbone = timm.create_model('efficientnet_b5', pretrained=True, num_classes=0)\n",
        "        self.backbone_out_features = self.backbone.num_features\n",
        "\n",
        "        aux_dim = 0\n",
        "        if self.use_aspect:\n",
        "            aux_dim += 1\n",
        "        if self.use_color:\n",
        "            aux_dim += 3\n",
        "\n",
        "        self.fc = nn.Linear(self.backbone_out_features + aux_dim, num_classes)\n",
        "\n",
        "    def forward(self, image, aspect_ratio=None, color_mean=None):\n",
        "        x = self.backbone(image)\n",
        "\n",
        "        aux_list = []\n",
        "        if self.use_aspect:\n",
        "            aux_list.append(aspect_ratio)\n",
        "        if self.use_color:\n",
        "            aux_list.append(color_mean)\n",
        "\n",
        "        if aux_list:\n",
        "            aux_features = torch.cat(aux_list, dim=1)\n",
        "            x = torch.cat([x, aux_features], dim=1)\n",
        "\n",
        "        out = self.fc(x)\n",
        "        return out\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-04T16:29:59.039689Z",
          "iopub.execute_input": "2025-06-04T16:29:59.040287Z",
          "iopub.status.idle": "2025-06-04T16:29:59.046556Z",
          "shell.execute_reply.started": "2025-06-04T16:29:59.040264Z",
          "shell.execute_reply": "2025-06-04T16:29:59.045614Z"
        },
        "id": "L80tw3wZUuHz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 전체 JPG 파일 불러오기 (Train)\n",
        "# ✅ Kaggle 경로\n",
        "file_list = glob.glob('/kaggle/input/car-image/train/*/*.jpg')\n",
        "\n",
        "# ✅ 클래스명 추출\n",
        "def extract_class_name_jpg(path):\n",
        "    return os.path.basename(os.path.dirname(path))\n",
        "\n",
        "class_names = sorted(set(extract_class_name_jpg(f) for f in file_list))\n",
        "class_to_idx = {cls: idx for idx, cls in enumerate(class_names)}\n",
        "\n",
        "print(f\"✅ 클래스 수: {len(class_to_idx)}\")  # 396개 나와야 정상\n",
        "\n",
        "# 라벨 생성\n",
        "labels = [class_to_idx[extract_class_name_jpg(f)] for f in file_list]\n",
        "\n",
        "train_files, val_files = train_test_split(\n",
        "    file_list, test_size=0.1, stratify=labels, random_state=42\n",
        ")\n",
        "\n",
        "# Train / Val Split\n",
        "print(f\"✅ Train 파일 수: {len(train_files)}\")\n",
        "print(f\"✅ Val 파일 수: {len(val_files)}\")\n",
        "\n",
        "# Transform 정의\n",
        "# ✅ Train Transform\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((384, 384)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# ✅ Val Transform\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((384, 384)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 확장형 Dataset 버전\n",
        "class CarImageDataset(Dataset):\n",
        "    def __init__(self, file_list, class_to_idx, transform=None, use_aspect=False, use_color=False):\n",
        "        self.file_list = file_list\n",
        "        self.class_to_idx = class_to_idx\n",
        "        self.transform = transform\n",
        "        self.use_aspect = use_aspect\n",
        "        self.use_color = use_color\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.file_list[idx]\n",
        "\n",
        "        # 🚗 Load Image (RGB 고정)\n",
        "        image_pil = Image.open(path).convert(\"RGB\")\n",
        "\n",
        "        # 🚗 Feature: Aspect Ratio\n",
        "        width, height = image_pil.size\n",
        "        aspect_ratio = torch.tensor([width / height], dtype=torch.float32)\n",
        "\n",
        "        # 🚗 Feature: Dominant Color (mean RGB)\n",
        "        image_np = np.array(image_pil)\n",
        "        color_mean = image_np.mean(axis=(0, 1)) / 255.0  # Normalize to 0~1\n",
        "        color_mean = torch.tensor(color_mean, dtype=torch.float32)\n",
        "\n",
        "        # 🚗 Transform\n",
        "        if self.transform:\n",
        "            image = self.transform(image_pil)\n",
        "        else:\n",
        "            image = transforms.ToTensor()(image_pil)\n",
        "\n",
        "        # 🚗 Label\n",
        "        class_name = extract_class_name_jpg(path)\n",
        "        label = self.class_to_idx[class_name]\n",
        "\n",
        "        # 🚗 Return mode\n",
        "        if self.use_aspect and self.use_color:\n",
        "            return image, aspect_ratio, color_mean, label\n",
        "        elif self.use_aspect:\n",
        "            return image, aspect_ratio, label\n",
        "        elif self.use_color:\n",
        "            return image, color_mean, label\n",
        "        else:\n",
        "            return image, label\n",
        "\n",
        "# ✅ 전략 설정\n",
        "USE_ASPECT = True    # 전략 B\n",
        "USE_COLOR = False\n",
        "\n",
        "# ✅ Dataset 정의 (전략 설정 값으로!)\n",
        "train_dataset = CarImageDataset(train_files, class_to_idx, train_transform, use_aspect=USE_ASPECT, use_color=USE_COLOR)\n",
        "val_dataset = CarImageDataset(val_files, class_to_idx, val_transform, use_aspect=USE_ASPECT, use_color=USE_COLOR)\n",
        "\n",
        "# ✅ DataLoader 정의\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
        "\n",
        "# ✅ Model 정의 (전략 설정 값으로!)\n",
        "model = CustomModel(use_aspect=USE_ASPECT, use_color=USE_COLOR, num_classes=len(class_to_idx))\n",
        "model = model.to(device)\n",
        "\n",
        "# ✅ 테스트 출력\n",
        "sample = next(iter(train_loader))\n",
        "images, labels = sample[0], sample[-1]\n",
        "print(f\"✅ Batch 이미지 shape: {images.shape}\")\n",
        "print(f\"✅ Batch label shape: {labels.shape}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-04T16:30:19.041965Z",
          "iopub.execute_input": "2025-06-04T16:30:19.042549Z",
          "iopub.status.idle": "2025-06-04T16:30:21.514479Z",
          "shell.execute_reply.started": "2025-06-04T16:30:19.042523Z",
          "shell.execute_reply": "2025-06-04T16:30:21.513283Z"
        },
        "id": "fLYGKL0cUuH0",
        "outputId": "e1496d11-da4d-4592-98e2-23791921a06a"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "✅ 클래스 수: 396\n✅ Train 파일 수: 29823\n✅ Val 파일 수: 3314\n✅ Batch 이미지 shape: torch.Size([16, 3, 384, 384])\n✅ Batch label shape: torch.Size([16])\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습용 optimizer / criterion 정의\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "\n",
        "# 학습 epoch 설정\n",
        "num_epochs = 10\n",
        "\n",
        "# 학습 loop 실행 (내가 준 학습 loop 그대로 복붙 가능)\n",
        "\n",
        "# ✅ labels 재정의 (반드시 Fold 전에!)\n",
        "labels = [class_to_idx[extract_class_name_jpg(f)] for f in file_list]\n",
        "\n",
        "# ✅ Fold loop 시작\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(file_list, labels)):\n",
        "    print(f\"\\n==============================\")\n",
        "    print(f\"🚀 Fold {fold + 1} / {n_splits}\")\n",
        "    print(f\"==============================\\n\")\n",
        "\n",
        "    # fold별 train/val 진행\n",
        "    ...\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-04T16:34:39.440759Z",
          "iopub.execute_input": "2025-06-04T16:34:39.441516Z",
          "iopub.status.idle": "2025-06-04T16:34:39.515498Z",
          "shell.execute_reply.started": "2025-06-04T16:34:39.441494Z",
          "shell.execute_reply": "2025-06-04T16:34:39.514764Z"
        },
        "id": "e-Z_qzhMUuH2",
        "outputId": "53a0ae90-cfd0-4156-f1d1-8a9309c49f56"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\n==============================\n🚀 Fold 1 / 5\n==============================\n\n\n==============================\n🚀 Fold 2 / 5\n==============================\n\n\n==============================\n🚀 Fold 3 / 5\n==============================\n\n\n==============================\n🚀 Fold 4 / 5\n==============================\n\n\n==============================\n🚀 Fold 5 / 5\n==============================\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# ✅ device 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"✅ Using device: {device}\")\n",
        "\n",
        "# ✅ 전략 설정\n",
        "USE_ASPECT = True\n",
        "USE_COLOR = False\n",
        "\n",
        "# ✅ Fold 설정\n",
        "n_splits = 5\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "# ✅ Fold loop 시작\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(file_list, labels)):\n",
        "    print(f\"\\n==============================\")\n",
        "    print(f\"🚀 Fold {fold + 1} / {n_splits}\")\n",
        "    print(f\"==============================\\n\")\n",
        "\n",
        "    # ✅ fold별 데이터 split\n",
        "    train_files = [file_list[i] for i in train_idx]\n",
        "    val_files = [file_list[i] for i in val_idx]\n",
        "\n",
        "    # ✅ fold별 Dataset & DataLoader\n",
        "    train_dataset = CarImageDataset(train_files, class_to_idx, train_transform, use_aspect=USE_ASPECT, use_color=USE_COLOR)\n",
        "    val_dataset = CarImageDataset(val_files, class_to_idx, val_transform, use_aspect=USE_ASPECT, use_color=USE_COLOR)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=12, shuffle=True, num_workers=2, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=12, shuffle=False, num_workers=2, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
        "\n",
        "    # ✅ Model 생성 (fold마다 fresh model 생성)\n",
        "    model = CustomModel(use_aspect=USE_ASPECT, use_color=USE_COLOR, num_classes=len(class_to_idx))\n",
        "    model = model.to(device)\n",
        "\n",
        "    # ✅ Loss / Optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "\n",
        "    # ✅ EarlyStopping 변수\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 3\n",
        "    patience_counter = 0\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    # ✅ 학습 epoch 설정\n",
        "    num_epochs = 10\n",
        "\n",
        "    # ✅ epoch loop 시작\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        print(f\"\\n🚀 Fold {fold + 1} | Epoch {epoch}/{num_epochs}\")\n",
        "\n",
        "        # ✅ Clear GPU cache → 메모리 누수 방지\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # === Train ===\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "\n",
        "        loop = tqdm(train_loader, desc=f\"Train Fold {fold + 1} | Epoch {epoch}\", leave=False)\n",
        "        for batch in loop:\n",
        "            if USE_ASPECT and USE_COLOR:\n",
        "                images, aspect_ratio, color_mean, labels = batch\n",
        "                images, aspect_ratio, color_mean, labels = images.to(device), aspect_ratio.to(device), color_mean.to(device), labels.to(device)\n",
        "                outputs = model(images, aspect_ratio, color_mean)\n",
        "            elif USE_ASPECT:\n",
        "                images, aspect_ratio, labels = batch\n",
        "                images, aspect_ratio, labels = images.to(device), aspect_ratio.to(device), labels.to(device)\n",
        "                outputs = model(images, aspect_ratio)\n",
        "            elif USE_COLOR:\n",
        "                images, color_mean, labels = batch\n",
        "                images, color_mean, labels = images.to(device), color_mean.to(device), labels.to(device)\n",
        "                outputs = model(images, color_mean=color_mean)\n",
        "            else:\n",
        "                images, labels = batch\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * images.size(0)\n",
        "            train_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "        train_acc = train_correct / len(train_loader.dataset)\n",
        "\n",
        "        # === Val ===\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "\n",
        "        val_loop = tqdm(val_loader, desc=f\"Valid Fold {fold + 1} | Epoch {epoch}\", leave=False)\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loop:\n",
        "                if USE_ASPECT and USE_COLOR:\n",
        "                    images, aspect_ratio, color_mean, labels = batch\n",
        "                    images, aspect_ratio, color_mean, labels = images.to(device), aspect_ratio.to(device), color_mean.to(device), labels.to(device)\n",
        "                    outputs = model(images, aspect_ratio, color_mean)\n",
        "                elif USE_ASPECT:\n",
        "                    images, aspect_ratio, labels = batch\n",
        "                    images, aspect_ratio, labels = images.to(device), aspect_ratio.to(device), labels.to(device)\n",
        "                    outputs = model(images, aspect_ratio)\n",
        "                elif USE_COLOR:\n",
        "                    images, color_mean, labels = batch\n",
        "                    images, color_mean, labels = images.to(device), color_mean.to(device), labels.to(device)\n",
        "                    outputs = model(images, color_mean=color_mean)\n",
        "                else:\n",
        "                    images, labels = batch\n",
        "                    images, labels = images.to(device), labels.to(device)\n",
        "                    outputs = model(images)\n",
        "\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item() * images.size(0)\n",
        "                val_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "                val_loop.set_postfix(loss=loss.item())\n",
        "\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        val_acc = val_correct / len(val_loader.dataset)\n",
        "\n",
        "        # === (선택) Validation 끝난 후에도 clear 가능\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # === 로그 출력 ===\n",
        "        print(f\"✅ Fold {fold + 1} | Epoch {epoch} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"✅ Fold {fold + 1} | Epoch {epoch} | Val   Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        # === EarlyStopping ===\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            save_path = f\"/kaggle/working/EffNetB5_B_fold{fold + 1}_best_model.pth\"\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(f\"📦 Best model saved: {save_path}\")\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"⚠️ EarlyStopping patience: {patience_counter}/{patience}\")\n",
        "            if patience_counter >= patience:\n",
        "                print(\"⛔ Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "    # ✅ fold 끝난 후 best model load\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    print(f\"\\n✅ Fold {fold + 1} | Best model loaded (Val Loss: {best_val_loss:.4f})\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-04T16:34:43.760242Z",
          "iopub.execute_input": "2025-06-04T16:34:43.760771Z",
          "execution_failed": "2025-06-04T23:41:57.744Z"
        },
        "id": "lgqr9ZVOUuH3",
        "outputId": "a60c6943-02ea-4b8b-f87a-e6ceb5543f5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "✅ Using device: cuda\n\n==============================\n🚀 Fold 1 / 5\n==============================\n\n\n🚀 Fold 1 | Epoch 1/10\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "                                                                                        \r",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "✅ Fold 1 | Epoch 1 | Train Loss: 2.1797 | Train Acc: 0.5318\n✅ Fold 1 | Epoch 1 | Val   Loss: 0.4839 | Val Acc: 0.8502\n📦 Best model saved: /kaggle/working/EffNetB5_B_fold1_best_model.pth\n\n🚀 Fold 1 | Epoch 2/10\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "                                                                                        \r",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "✅ Fold 1 | Epoch 2 | Train Loss: 0.3871 | Train Acc: 0.8835\n✅ Fold 1 | Epoch 2 | Val   Loss: 0.2834 | Val Acc: 0.9092\n📦 Best model saved: /kaggle/working/EffNetB5_B_fold1_best_model.pth\n\n🚀 Fold 1 | Epoch 3/10\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "                                                                                        \r",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "✅ Fold 1 | Epoch 3 | Train Loss: 0.2630 | Train Acc: 0.9138\n✅ Fold 1 | Epoch 3 | Val   Loss: 0.2506 | Val Acc: 0.9214\n📦 Best model saved: /kaggle/working/EffNetB5_B_fold1_best_model.pth\n\n🚀 Fold 1 | Epoch 4/10\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "                                                                                         \r",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "✅ Fold 1 | Epoch 4 | Train Loss: 0.2068 | Train Acc: 0.9322\n✅ Fold 1 | Epoch 4 | Val   Loss: 0.2239 | Val Acc: 0.9301\n📦 Best model saved: /kaggle/working/EffNetB5_B_fold1_best_model.pth\n\n🚀 Fold 1 | Epoch 5/10\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "                                                                                         \r",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "✅ Fold 1 | Epoch 5 | Train Loss: 0.1786 | Train Acc: 0.9392\n✅ Fold 1 | Epoch 5 | Val   Loss: 0.1779 | Val Acc: 0.9434\n📦 Best model saved: /kaggle/working/EffNetB5_B_fold1_best_model.pth\n\n🚀 Fold 1 | Epoch 6/10\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "                                                                                         \r",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "✅ Fold 1 | Epoch 6 | Train Loss: 0.1537 | Train Acc: 0.9484\n✅ Fold 1 | Epoch 6 | Val   Loss: 0.2321 | Val Acc: 0.9353\n⚠️ EarlyStopping patience: 1/3\n\n🚀 Fold 1 | Epoch 7/10\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "                                                                                         \r",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "✅ Fold 1 | Epoch 7 | Train Loss: 0.1393 | Train Acc: 0.9527\n✅ Fold 1 | Epoch 7 | Val   Loss: 0.2020 | Val Acc: 0.9375\n⚠️ EarlyStopping patience: 2/3\n\n🚀 Fold 1 | Epoch 8/10\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "                                                                                          \r",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "✅ Fold 1 | Epoch 8 | Train Loss: 0.1237 | Train Acc: 0.9579\n✅ Fold 1 | Epoch 8 | Val   Loss: 0.2061 | Val Acc: 0.9422\n⚠️ EarlyStopping patience: 3/3\n⛔ Early stopping triggered.\n\n✅ Fold 1 | Best model loaded (Val Loss: 0.1779)\n\n==============================\n🚀 Fold 2 / 5\n==============================\n\n\n🚀 Fold 2 | Epoch 1/10\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "                                                                                       \r",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "✅ Fold 2 | Epoch 1 | Train Loss: 2.0047 | Train Acc: 0.5672\n✅ Fold 2 | Epoch 1 | Val   Loss: 0.4567 | Val Acc: 0.8639\n📦 Best model saved: /kaggle/working/EffNetB5_B_fold2_best_model.pth\n\n🚀 Fold 2 | Epoch 2/10\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "                                                                                        \r",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "✅ Fold 2 | Epoch 2 | Train Loss: 0.3846 | Train Acc: 0.8806\n✅ Fold 2 | Epoch 2 | Val   Loss: 0.3050 | Val Acc: 0.9018\n📦 Best model saved: /kaggle/working/EffNetB5_B_fold2_best_model.pth\n\n🚀 Fold 2 | Epoch 3/10\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "                                                                                        \r",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "✅ Fold 2 | Epoch 3 | Train Loss: 0.2636 | Train Acc: 0.9155\n✅ Fold 2 | Epoch 3 | Val   Loss: 0.2639 | Val Acc: 0.9160\n📦 Best model saved: /kaggle/working/EffNetB5_B_fold2_best_model.pth\n\n🚀 Fold 2 | Epoch 4/10\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "                                                                                         \r",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "✅ Fold 2 | Epoch 4 | Train Loss: 0.2098 | Train Acc: 0.9310\n✅ Fold 2 | Epoch 4 | Val   Loss: 0.2074 | Val Acc: 0.9345\n📦 Best model saved: /kaggle/working/EffNetB5_B_fold2_best_model.pth\n\n🚀 Fold 2 | Epoch 5/10\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "                                                                                         \r",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "✅ Fold 2 | Epoch 5 | Train Loss: 0.1793 | Train Acc: 0.9400\n✅ Fold 2 | Epoch 5 | Val   Loss: 0.2055 | Val Acc: 0.9335\n📦 Best model saved: /kaggle/working/EffNetB5_B_fold2_best_model.pth\n\n🚀 Fold 2 | Epoch 6/10\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Train Fold 2 | Epoch 6:  88%|████████▊ | 1953/2210 [20:49<02:44,  1.56it/s, loss=0.271]  ",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import timm\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# ✅ 고정 경로 (Kaggle)\n",
        "TEST_DIR = \"/kaggle/input/car-image/test\"\n",
        "SAMPLE_SUB_PATH = \"/kaggle/input/car-image/sample_submission.csv\"\n",
        "NUM_CLASSES = 396\n",
        "\n",
        "# ✅ 샘플 제출 파일에서 클래스명 추출\n",
        "sample = pd.read_csv(SAMPLE_SUB_PATH)\n",
        "column_names = sample.columns.tolist()[1:]  # 'ID' 제외\n",
        "\n",
        "# ✅ Transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((384, 384)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# ✅ 테스트용 Dataset\n",
        "class TestJPGDataset(Dataset):\n",
        "    def __init__(self, img_root, transform=None):\n",
        "        self.file_list = []\n",
        "        for file in os.listdir(img_root):\n",
        "            if file.endswith('.jpg'):\n",
        "                self.file_list.append(os.path.join(img_root, file))\n",
        "        self.file_list.sort()  # 반드시 정렬\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.file_list[idx]\n",
        "        image = Image.open(path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        fname = os.path.basename(path).replace(\".jpg\", \"\")\n",
        "        return image, fname\n",
        "\n",
        "# ✅ DataLoader\n",
        "test_dataset = TestJPGDataset(TEST_DIR, transform)\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=32,   # Inference는 32~64 사용해도 괜찮음\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        "    prefetch_factor=2\n",
        ")\n",
        "\n",
        "# ✅ 디바이스 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"✅ Using device: {device}\")\n",
        "\n",
        "# ✅ 전략 B Inference (fold 없이 단일 best model 사용 예시)\n",
        "EXP_NAME = \"B\"\n",
        "\n",
        "# ✅ 모델 경로 (Kaggle → working에서 불러오기 예시)\n",
        "MODEL_PATH = f\"/kaggle/working/EffNetB5_{EXP_NAME}_best_model.pth\"\n",
        "\n",
        "# ✅ CustomModel 불러와서 사용해야 함!\n",
        "# (너가 학습 때 쓴 CustomModel 동일하게 넣어야 함)\n",
        "\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, use_aspect=False, use_color=False, num_classes=396):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.use_aspect = use_aspect\n",
        "        self.use_color = use_color\n",
        "\n",
        "        self.backbone = timm.create_model('efficientnet_b5', pretrained=False, num_classes=0)\n",
        "        self.backbone_out_features = self.backbone.num_features\n",
        "\n",
        "        aux_dim = 0\n",
        "        if self.use_aspect:\n",
        "            aux_dim += 1\n",
        "        if self.use_color:\n",
        "            aux_dim += 3\n",
        "\n",
        "        self.fc = nn.Linear(self.backbone_out_features + aux_dim, num_classes)\n",
        "\n",
        "    def forward(self, image, aspect_ratio=None, color_mean=None):\n",
        "        x = self.backbone(image)\n",
        "\n",
        "        aux_list = []\n",
        "        if self.use_aspect:\n",
        "            aux_list.append(aspect_ratio)\n",
        "        if self.use_color:\n",
        "            aux_list.append(color_mean)\n",
        "\n",
        "        if aux_list:\n",
        "            aux_features = torch.cat(aux_list, dim=1)\n",
        "            x = torch.cat([x, aux_features], dim=1)\n",
        "\n",
        "        out = self.fc(x)\n",
        "        return out\n",
        "\n",
        "# ✅ 모델 생성 & 로드\n",
        "USE_ASPECT = True    # 전략 B\n",
        "USE_COLOR = False\n",
        "\n",
        "model = CustomModel(use_aspect=USE_ASPECT, use_color=USE_COLOR, num_classes=NUM_CLASSES)\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# ✅ 추론 시작\n",
        "all_probs = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, names in tqdm(test_loader, desc=f\"🚀 Inference EXP {EXP_NAME}\"):\n",
        "        imgs = imgs.to(device)\n",
        "\n",
        "        # 전략 B이므로 aspect_ratio 필요 없음 → 그냥 image만 사용\n",
        "        outputs = model(imgs, aspect_ratio=None)\n",
        "        probs = F.softmax(outputs, dim=1)\n",
        "        all_probs.append(probs.cpu().numpy())\n",
        "\n",
        "# ✅ 결과 정리\n",
        "all_probs = np.concatenate(all_probs, axis=0)\n",
        "\n",
        "results = []\n",
        "for idx, path in enumerate(test_dataset.file_list):\n",
        "    fname = os.path.basename(path).replace(\".jpg\", \"\")\n",
        "    row = {\"ID\": fname}\n",
        "    row.update({class_name: all_probs[idx, i] for i, class_name in enumerate(column_names)})\n",
        "    results.append(row)\n",
        "\n",
        "submission_df = pd.DataFrame(results)\n",
        "submission_df = submission_df[[\"ID\"] + column_names]\n",
        "\n",
        "# ✅ 파일 저장 (Kaggle working에 저장!)\n",
        "SAVE_SUBMISSION_PATH = f\"/kaggle/working/submission_B.csv\"\n",
        "submission_df.to_csv(SAVE_SUBMISSION_PATH, index=False)\n",
        "\n",
        "print(f\"\\n✅ Submission 저장 완료: {SAVE_SUBMISSION_PATH}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-06-04T16:28:06.019Z"
        },
        "id": "nVHsCsH4UuH4"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}