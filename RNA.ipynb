{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 87793,
          "databundleVersionId": 12276181,
          "sourceType": "competition"
        },
        {
          "sourceId": 10855324,
          "sourceType": "datasetVersion",
          "datasetId": 6742586
        },
        {
          "sourceId": 11118830,
          "sourceType": "datasetVersion",
          "datasetId": 6933267
        },
        {
          "sourceId": 224830487,
          "sourceType": "kernelVersion"
        }
      ],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Randomness",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hanbin-git/kaggle/blob/main/RNA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "Bk4B5NutV4Br"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "stanford_rna_3d_folding_path = kagglehub.competition_download('stanford-rna-3d-folding')\n",
        "metric_usalign_path = kagglehub.dataset_download('metric/usalign')\n",
        "geraseva_protenix_checkpoints_path = kagglehub.dataset_download('geraseva/protenix-checkpoints')\n",
        "metric_ribonanza_tm_score_path = kagglehub.notebook_output_download('metric/ribonanza-tm-score')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "VomZYivsV4Bs"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_TYPE='protenix'\n",
        "VALIDATION=True"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-20T00:38:34.199448Z",
          "iopub.execute_input": "2025-05-20T00:38:34.19977Z",
          "iopub.status.idle": "2025-05-20T00:38:34.203524Z",
          "shell.execute_reply.started": "2025-05-20T00:38:34.19974Z",
          "shell.execute_reply": "2025-05-20T00:38:34.202592Z"
        },
        "id": "kN-l0GWPV4Bt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchvision torchaudio protenix"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-20T00:38:37.543554Z",
          "iopub.execute_input": "2025-05-20T00:38:37.543838Z",
          "iopub.status.idle": "2025-05-20T00:38:40.348929Z",
          "shell.execute_reply.started": "2025-05-20T00:38:37.543817Z",
          "shell.execute_reply": "2025-05-20T00:38:40.347794Z"
        },
        "id": "PwP8uXe2V4Bt",
        "outputId": "0062d5f6-b3bf-41a6-b667-f2af07850b40"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Found existing installation: torch 2.1.2+cu118\nUninstalling torch-2.1.2+cu118:\n  Successfully uninstalled torch-2.1.2+cu118\n\u001b[33mWARNING: Skipping torchvision as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping torchaudio as it is not installed.\u001b[0m\u001b[33m\n\u001b[0mFound existing installation: protenix 0.4.6\nUninstalling protenix-0.4.6:\n  Successfully uninstalled protenix-0.4.6\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --extra-index-url https://download.pytorch.org/whl/cu118 torch==2.1.2+cu118"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-20T00:40:30.670965Z",
          "iopub.execute_input": "2025-05-20T00:40:30.671283Z",
          "iopub.status.idle": "2025-05-20T00:41:24.781006Z",
          "shell.execute_reply.started": "2025-05-20T00:40:30.671258Z",
          "shell.execute_reply": "2025-05-20T00:41:24.780095Z"
        },
        "id": "xBI5o3awV4Bu",
        "outputId": "46e52629-f220-4845-c49a-83a013196590"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118\nCollecting torch==2.1.2+cu118\n  Using cached https://download.pytorch.org/whl/cu118/torch-2.1.2%2Bcu118-cp310-cp310-linux_x86_64.whl (2325.9 MB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2+cu118) (3.17.0)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2+cu118) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2+cu118) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2+cu118) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2+cu118) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2+cu118) (2024.12.0)\nRequirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2+cu118) (2.1.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.2+cu118) (3.0.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.2+cu118) (1.3.0)\nInstalling collected packages: torch\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\neasyocr 1.7.2 requires torchvision>=0.5, which is not installed.\nfastai 2.7.18 requires torchvision>=0.11, which is not installed.\nprotenix 0.4.6 requires deepspeed>=0.15.1, which is not installed.\nprotenix 0.4.6 requires icecream, which is not installed.\nprotenix 0.4.6 requires ipdb, which is not installed.\nprotenix 0.4.6 requires modelcif==0.7, which is not installed.\nprotenix 0.4.6 requires py3Dmol, which is not installed.\nprotenix 0.4.6 requires scikit-learn-extra, which is not installed.\ntimm 1.0.12 requires torchvision, which is not installed.\nprotenix 0.4.6 requires biopython==1.83, but you have biopython 1.85 which is incompatible.\nprotenix 0.4.6 requires matplotlib==3.9.2, but you have matplotlib 3.7.5 which is incompatible.\nprotenix 0.4.6 requires numpy==1.26.3, but you have numpy 1.26.4 which is incompatible.\nprotenix 0.4.6 requires protobuf==3.20.2, but you have protobuf 3.20.3 which is incompatible.\nprotenix 0.4.6 requires torch==2.3.1, but you have torch 2.1.2+cu118 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed torch-2.1.2+cu118\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --no-deps protenix==0.4.6 einops tqdm gemmi biotite==1.0.1"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-20T00:38:45.039711Z",
          "iopub.execute_input": "2025-05-20T00:38:45.039999Z",
          "iopub.status.idle": "2025-05-20T00:38:46.429731Z",
          "shell.execute_reply.started": "2025-05-20T00:38:45.039976Z",
          "shell.execute_reply": "2025-05-20T00:38:46.428876Z"
        },
        "id": "grvOqzo5V4Bu",
        "outputId": "f30f1e4e-f382-4d7b-e198-6d732ecabc2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting protenix==0.4.6\n  Using cached protenix-0.4.6-py3-none-any.whl.metadata (1.1 kB)\nRequirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\nRequirement already satisfied: gemmi in /usr/local/lib/python3.10/dist-packages (0.7.1)\nRequirement already satisfied: biotite==1.0.1 in /usr/local/lib/python3.10/dist-packages (1.0.1)\nUsing cached protenix-0.4.6-py3-none-any.whl (441 kB)\nInstalling collected packages: protenix\nSuccessfully installed protenix-0.4.6\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install requirements"
      ],
      "metadata": {
        "id": "rnVW8LfbV4Bu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if MODEL_TYPE=='protenix' and VALIDATION:\n",
        "    !pip install biopython\n",
        "    !pip install ml-collections\n",
        "    !pip install rdkit\n",
        "!export PROTENIX_DATA_ROOT_DIR=/kaggle/input/protenix-checkpoints"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-20T00:37:01.642772Z",
          "iopub.execute_input": "2025-05-20T00:37:01.643053Z",
          "iopub.status.idle": "2025-05-20T00:37:11.777624Z",
          "shell.execute_reply.started": "2025-05-20T00:37:01.643031Z",
          "shell.execute_reply": "2025-05-20T00:37:11.776639Z"
        },
        "id": "WMOLpdXRV4Bv",
        "outputId": "912f6b0b-fbb0-4b67-c426-ed37b962cc00"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: biopython in /usr/local/lib/python3.10/dist-packages (1.85)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->biopython) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->biopython) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->biopython) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->biopython) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->biopython) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->biopython) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->biopython) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->biopython) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->biopython) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->biopython) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->biopython) (2024.2.0)\nRequirement already satisfied: ml-collections in /usr/local/lib/python3.10/dist-packages (1.1.0)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from ml-collections) (1.4.0)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from ml-collections) (6.0.2)\nRequirement already satisfied: rdkit in /usr/local/lib/python3.10/dist-packages (2025.3.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.26.4)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (11.0.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->rdkit) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->rdkit) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->rdkit) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->rdkit) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->rdkit) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->rdkit) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rdkit) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rdkit) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->rdkit) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->rdkit) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->rdkit) (2024.2.0)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir /af3-dev\n",
        "! ln -s /kaggle/input/protenix-checkpoints /af3-dev/release_data\n",
        "! ls /af3-dev/release_data/"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-20T00:38:19.696549Z",
          "iopub.execute_input": "2025-05-20T00:38:19.696888Z",
          "iopub.status.idle": "2025-05-20T00:38:20.039511Z",
          "shell.execute_reply.started": "2025-05-20T00:38:19.696865Z",
          "shell.execute_reply": "2025-05-20T00:38:20.038672Z"
        },
        "id": "IWtgdocIV4Bv",
        "outputId": "3ddeab09-ee48-4b69-9ac9-ff5201276840"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "mkdir: cannot create directory ‘/af3-dev’: File exists\nln: failed to create symbolic link '/af3-dev/release_data/protenix-checkpoints': Read-only file system\ncomponents.v20240608.cif\t\tmodel_v0.2.0.pt\ncomponents.v20240608.cif.rdkit_mol.pkl\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper scripts"
      ],
      "metadata": {
        "id": "79WPpnhiV4Bv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import Bio\n",
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "import pandas as pd\n",
        "from Bio.PDB import Atom, Model, Chain, Residue, Structure, PDBParser\n",
        "from Bio import SeqIO\n",
        "import os, sys\n",
        "import re\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "time0=time.time()\n",
        "\n",
        "print('IMPORT OK !!!!')"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2025-05-20T00:41:39.764526Z",
          "iopub.execute_input": "2025-05-20T00:41:39.764808Z",
          "iopub.status.idle": "2025-05-20T00:41:41.686387Z",
          "shell.execute_reply.started": "2025-05-20T00:41:39.764784Z",
          "shell.execute_reply": "2025-05-20T00:41:41.685621Z"
        },
        "trusted": true,
        "id": "xzOihvfbV4Bv",
        "outputId": "a676f71b-c704-468a-a4c6-5f21ef6f03ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "IMPORT OK !!!!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from Bio.PDB import PDBParser\n",
        "\n",
        "# 🧱 환경 설정\n",
        "PYTHON = sys.executable\n",
        "print('PYTHON', PYTHON)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-20T00:58:50.711324Z",
          "iopub.execute_input": "2025-05-20T00:58:50.711817Z",
          "iopub.status.idle": "2025-05-20T00:58:50.716931Z",
          "shell.execute_reply.started": "2025-05-20T00:58:50.711788Z",
          "shell.execute_reply": "2025-05-20T00:58:50.716257Z"
        },
        "id": "u1eRz4ZOV4Bw",
        "outputId": "9462b807-3279-4901-8b4c-73b786fd3970"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "PYTHON /usr/bin/python3\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "RHONET_DIR = '/kaggle/input/data-for-demo-for-rhofold-plus-with-kaggle-msa/RhoFold-main'\n",
        "USALIGN = '/kaggle/working/USalign'\n",
        "\n",
        "os.system('cp /kaggle/input/usalign/USalign /kaggle/working/')\n",
        "os.system('chmod u+x /kaggle/working/USalign')  # sudo는 필요 없음\n",
        "sys.path.append(RHONET_DIR)\n",
        "\n",
        "DATA_KAGGLE_DIR = '/kaggle/input/stanford-rna-3d-folding'\n",
        "\n",
        "# 🧾 dotdict\n",
        "class dotdict(dict):\n",
        "    __setattr__ = dict.__setitem__\n",
        "    __delattr__ = dict.__delitem__\n",
        "\n",
        "    def __getattr__(self, name):\n",
        "        try:\n",
        "            return self[name]\n",
        "        except KeyError:\n",
        "            raise AttributeError(name)\n",
        "\n",
        "# 📊 시각화용\n",
        "def set_aspect_equal(ax):\n",
        "    x_limits = ax.get_xlim()\n",
        "    y_limits = ax.get_ylim()\n",
        "    z_limits = ax.get_zlim()\n",
        "    x_middle, y_middle, z_middle = np.mean(x_limits), np.mean(y_limits), np.mean(z_limits)\n",
        "    max_range = max(\n",
        "        x_limits[1] - x_limits[0],\n",
        "        y_limits[1] - y_limits[0],\n",
        "        z_limits[1] - z_limits[0]\n",
        "    ) / 2.0\n",
        "    ax.set_xlim(x_middle - max_range, x_middle + max_range)\n",
        "    ax.set_ylim(y_middle - max_range, y_middle + max_range)\n",
        "    ax.set_zlim(z_middle - max_range, z_middle + max_range)\n",
        "\n",
        "# 📌 예측 output → DataFrame 변환\n",
        "def parse_output_to_df(output, seq, target_id):\n",
        "    if isinstance(output, torch.Tensor):\n",
        "        output = output.detach().cpu().numpy()\n",
        "    output = output.squeeze()  # [L, 3]\n",
        "    chain_data = []\n",
        "    for i, res in enumerate(seq):\n",
        "        x, y, z = output[i]\n",
        "        chain_data.append(dict(\n",
        "            ID=target_id,\n",
        "            resname=res,\n",
        "            resid=i + 1,\n",
        "            x_1=round(x, 3),\n",
        "            y_1=round(y, 3),\n",
        "            z_1=round(z, 3)\n",
        "        ))\n",
        "    return [pd.DataFrame(chain_data)]\n",
        "\n",
        "# 📌 GT PDB → DataFrame 변환\n",
        "def parse_pdb_to_df(pdb_file, target_id):\n",
        "    parser = PDBParser()\n",
        "    structure = parser.get_structure('', pdb_file)\n",
        "\n",
        "    df_list = []\n",
        "    for model in structure:\n",
        "        for chain in model:\n",
        "            chain_data = []\n",
        "            for residue in chain:\n",
        "                if residue.get_resname() in ['A', 'U', 'G', 'C']:\n",
        "                    if 'C1\\'' in residue:\n",
        "                        atom = residue['C1\\'']\n",
        "                        x, y, z = atom.get_coord()\n",
        "                        resid = residue.get_id()[1]\n",
        "                        chain_data.append(dict(\n",
        "                            ID=target_id + '_' + str(resid),\n",
        "                            resname=residue.get_resname(),\n",
        "                            resid=resid,\n",
        "                            x_1=x,\n",
        "                            y_1=y,\n",
        "                            z_1=z\n",
        "                        ))\n",
        "            if chain_data:\n",
        "                df_list.append(pd.DataFrame(chain_data))\n",
        "    return df_list\n",
        "\n",
        "# 📌 좌표 → PDB 라인 생성\n",
        "def write_target_line(atom_name, atom_serial, residue_name, chain_id,\n",
        "                      residue_num, x_coord, y_coord, z_coord,\n",
        "                      occupancy=1.0, b_factor=0.0, atom_type='C'):\n",
        "    return (\n",
        "        f'ATOM  {atom_serial:>5d}  {atom_name:<5s} {residue_name:<3s} '\n",
        "        f'{residue_num:>3d}    {x_coord:>8.3f}{y_coord:>8.3f}{z_coord:>8.3f}'\n",
        "        f'{occupancy:>6.2f}{b_factor:>6.2f}           {atom_type}\\n'\n",
        "    )\n",
        "\n",
        "# 📌 DataFrame → PDB 저장\n",
        "def write_xyz_to_pdb(df, pdb_file, xyz_id=1):\n",
        "    resolved_cnt = 0\n",
        "    with open(pdb_file, 'w') as f:\n",
        "        for _, row in df.iterrows():\n",
        "            x, y, z = row[f'x_{xyz_id}'], row[f'y_{xyz_id}'], row[f'z_{xyz_id}']\n",
        "            if x > -1e17 and y > -1e17 and z > -1e17:\n",
        "                resolved_cnt += 1\n",
        "                f.write(write_target_line(\n",
        "                    atom_name=\"C1'\",\n",
        "                    atom_serial=int(row['resid']),\n",
        "                    residue_name=row['resname'],\n",
        "                    chain_id='0',\n",
        "                    residue_num=int(row['resid']),\n",
        "                    x_coord=x, y_coord=y, z_coord=z,\n",
        "                    atom_type='C'\n",
        "                ))\n",
        "    return resolved_cnt\n",
        "\n",
        "# 📌 USalign → TM-score 추출\n",
        "def parse_usalign_for_tm_score(output):\n",
        "    matches = re.findall(r'TM-score=\\s+([\\d.]+)', output)\n",
        "    if len(matches) < 2:\n",
        "        raise ValueError(\"TM-score 추출 실패\")\n",
        "    return float(matches[1])\n",
        "\n",
        "# 📌 USalign → 회전행렬 추출\n",
        "def parse_usalign_for_transform(output):\n",
        "    lines = output.splitlines()\n",
        "    matrix_lines = []\n",
        "    in_matrix = False\n",
        "    for line in lines:\n",
        "        if \"The rotation matrix to rotate Structure_1 to Structure_2\" in line:\n",
        "            in_matrix = True\n",
        "        elif in_matrix and re.match(r'^\\d+\\s+[-\\d.]+\\s+[-\\d.]+\\s+[-\\d.]+\\s+[-\\d.]+$', line):\n",
        "            matrix_lines.append(line)\n",
        "        elif in_matrix and not line.strip():\n",
        "            break\n",
        "    matrix = [list(map(float, l.split()[1:])) for l in matrix_lines]\n",
        "    return np.array(matrix)\n",
        "\n",
        "# 📌 평가 전체 실행\n",
        "def call_usalign(predict_df, truth_df, usalign_bin=USALIGN, verbose=True):\n",
        "    pred_pdb, gt_pdb = '~pred.pdb', '~truth.pdb'\n",
        "    write_xyz_to_pdb(predict_df, pred_pdb)\n",
        "    write_xyz_to_pdb(truth_df, gt_pdb)\n",
        "    cmd = f'{usalign_bin} {pred_pdb} {gt_pdb} -atom \" C1\\'\" -m -'\n",
        "    output = os.popen(cmd).read()\n",
        "    if verbose:\n",
        "        print(output)\n",
        "    return parse_usalign_for_tm_score(output), parse_usalign_for_transform(output)\n",
        "\n",
        "print('✅ RNA 구조 평가용 HELPER 로딩 완료!')\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-20T01:01:01.310524Z",
          "iopub.execute_input": "2025-05-20T01:01:01.310815Z",
          "iopub.status.idle": "2025-05-20T01:01:01.37097Z",
          "shell.execute_reply.started": "2025-05-20T01:01:01.310793Z",
          "shell.execute_reply": "2025-05-20T01:01:01.370275Z"
        },
        "id": "3LcwvlamV4Bw",
        "outputId": "ead59682-b77d-42da-bad2-5aeb9b51bf87"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "✅ RNA 구조 평가용 HELPER 로딩 완료!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "if MODEL_TYPE=='protenix':\n",
        "\n",
        "\n",
        "    from runner.batch_inference import get_default_runner\n",
        "    from runner.inference import update_inference_configs, InferenceRunner\n",
        "\n",
        "    from protenix.data.infer_data_pipeline import InferenceDataset\n",
        "\n",
        "    np.random.seed(0)\n",
        "    torch.random.manual_seed(0)\n",
        "    torch.cuda.manual_seed_all(0)\n",
        "\n",
        "    class DictDataset:\n",
        "        def __init__(\n",
        "            self,\n",
        "            seq_list: list,\n",
        "            dump_dir: str,\n",
        "            id_list: list = None,\n",
        "            use_msa: bool = False,\n",
        "        ) -> None:\n",
        "            assert isinstance(seq_list, list), \"seq_list는 리스트여야 합니다\"\n",
        "            if id_list is not None:\n",
        "                assert len(seq_list) == len(id_list), \"seq_list와 id_list 길이 다름\"\n",
        "\n",
        "            self.dump_dir = dump_dir\n",
        "            self.use_msa = use_msa\n",
        "\n",
        "            if id_list is None:\n",
        "                self.inputs = [{\n",
        "                    \"sequences\": [{\n",
        "                        \"rnaSequence\": {\n",
        "                            \"sequence\": seq,\n",
        "                            \"count\": 1\n",
        "                        }\n",
        "                    }],\n",
        "                    \"name\": f\"query_{idx}\"\n",
        "                } for idx, seq in enumerate(seq_list)]\n",
        "            else:\n",
        "                self.inputs = [{\n",
        "                    \"sequences\": [{\n",
        "                        \"rnaSequence\": {\n",
        "                            \"sequence\": seq,\n",
        "                            \"count\": 1\n",
        "                        }\n",
        "                    }],\n",
        "                    \"name\": id_\n",
        "                } for id_, seq in zip(id_list, seq_list)]\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.inputs)\n",
        "\n",
        "        def __getitem__(self, index):\n",
        "            # InferenceDataset이 제공하는 방식과 유사하게 반환\n",
        "            from protenix.data.infer_data_pipeline import convert_input_to_data\n",
        "            input_json = self.inputs[index]\n",
        "            data, atom_array, error_message = convert_input_to_data(input_json, dump_dir=self.dump_dir, use_msa=self.use_msa)\n",
        "            return data, atom_array, error_message\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-20T01:31:43.160035Z",
          "iopub.execute_input": "2025-05-20T01:31:43.160427Z",
          "iopub.status.idle": "2025-05-20T01:31:43.16788Z",
          "shell.execute_reply.started": "2025-05-20T01:31:43.160396Z",
          "shell.execute_reply": "2025-05-20T01:31:43.167163Z"
        },
        "id": "xvkxAPyxV4Bx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "from distutils.util import strtobool  # 문자열 to bool 변환\n",
        "\n",
        "if MODEL_TYPE == 'protenix':\n",
        "\n",
        "    # configs import\n",
        "    from configs.configs_base import configs as configs_base\n",
        "    from configs.configs_data import data_configs\n",
        "    from configs.configs_inference import inference_configs\n",
        "    from protenix.config.config import parse_configs\n",
        "    from runner.inference import InferenceRunner\n",
        "\n",
        "    # ✅ config 깊은 복사 → 원본 보존\n",
        "    base_config = copy.deepcopy(configs_base)\n",
        "\n",
        "    # ✅ 환경 변수 boolean 처리 개선\n",
        "    base_config[\"use_deepspeed_evo_attention\"] = bool(strtobool(os.environ.get(\"USE_DEEPSPEED_EVO_ATTENTION\", \"false\")))\n",
        "\n",
        "    # ✅ 모델, 샘플링 하이퍼파라미터 수정\n",
        "    base_config[\"model\"][\"N_cycle\"] = 10\n",
        "    base_config[\"sample_diffusion\"][\"N_sample\"] = 5 if not VALIDATION else 1\n",
        "    base_config[\"sample_diffusion\"][\"N_step\"] = 200\n",
        "\n",
        "    # ✅ 체크포인트 경로 지정\n",
        "    inference_configs['load_checkpoint_path'] = '/kaggle/input/protenix-checkpoints/model_v0.2.0.pt'\n",
        "\n",
        "    # ✅ configs 병합 및 파싱\n",
        "    configs = {**base_config, **{\"data\": data_configs}, **inference_configs}\n",
        "    configs = parse_configs(\n",
        "        configs=configs,\n",
        "        fill_required_with_null=True,\n",
        "    )\n",
        "\n",
        "    # ✅ Inference Runner 준비\n",
        "    runner = InferenceRunner(configs)\n",
        "\n",
        "print(\"✅ 모델 로딩 완료 여부:\", runner.model is not None)\n",
        "print(\"📂 체크포인트 경로:\", configs['load_checkpoint_path'])\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-20T01:31:48.16074Z",
          "iopub.execute_input": "2025-05-20T01:31:48.161046Z",
          "iopub.status.idle": "2025-05-20T01:31:53.730146Z",
          "shell.execute_reply.started": "2025-05-20T01:31:48.161025Z",
          "shell.execute_reply": "2025-05-20T01:31:53.729409Z"
        },
        "id": "cRcGb1BjV4Bx",
        "outputId": "40fac134-539c-4620-dc00-3cb89df3938c"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "train scheduler 16.0\ninference scheduler 16.0\nDiffusion Module has 16.0\n✅ 모델 로딩 완료 여부: True\n📂 체크포인트 경로: /kaggle/input/protenix-checkpoints/model_v0.2.0.pt\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "\n",
        "if VALIDATION:\n",
        "    LABEL_DF = pd.read_csv('/kaggle/input/stanford-rna-3d-folding/train_labels.csv')\n",
        "    LABEL_DF['target_id'] = LABEL_DF['ID'].apply(lambda x: '_'.join(x.split('_')[:-1]))\n",
        "    train_df = pd.read_csv('/kaggle/input/stanford-rna-3d-folding/train_sequences.csv')\n",
        "\n",
        "    # ✅ 여기 추가\n",
        "    def get_truth_df(target_id):\n",
        "        truth_df = LABEL_DF[LABEL_DF['target_id'] == target_id]\n",
        "        return truth_df.reset_index(drop=True)\n",
        "\n",
        "if MODEL_TYPE == 'protenix' and VALIDATION:\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "    time0 = time.time()\n",
        "\n",
        "    train_df['protenix_tm_score'] = None\n",
        "\n",
        "    # ⬇️ 여기가 수정되는 위치\n",
        "    dataset = DictDataset(\n",
        "        seq_list=train_df.sequence.tolist(),\n",
        "        dump_dir='output',\n",
        "        id_list=train_df.target_id.tolist(),\n",
        "        use_msa=False\n",
        "    )\n",
        "\n",
        "    num_data = len(dataset)\n",
        "\n",
        "    for i, seq in tqdm(enumerate(train_df.sequence), total=num_data):\n",
        "        if pd.notnull(train_df.loc[i, 'protenix_tm_score']):\n",
        "            continue\n",
        "        if len(seq) > 300:\n",
        "            continue\n",
        "\n",
        "        target_id = train_df.loc[i, 'target_id']\n",
        "        truth_df = get_truth_df(target_id)\n",
        "\n",
        "        if sum(~np.isnan(truth_df.x_1)) < 3:\n",
        "            continue\n",
        "\n",
        "        data, atom_array, data_error_message = dataset[i]\n",
        "        if data_error_message != '':\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            new_configs = update_inference_configs(configs, data[\"N_token\"].item())\n",
        "            runner.update_model_configs(new_configs)\n",
        "\n",
        "            prediction = runner.predict(data)\n",
        "            prediction = prediction['coordinate'][:, data['input_feature_dict']['atom_to_tokatom_idx'] == 12]\n",
        "\n",
        "            # Optional smoothing (TM-score 향상 여지 있음)\n",
        "            # def smooth_coords(coords, window=3):\n",
        "            #     coords = coords.squeeze()\n",
        "            #     smoothed = np.copy(coords)\n",
        "            #     for j in range(1, len(coords)-1):\n",
        "            #         smoothed[j] = np.mean(coords[j-1:j+2], axis=0)\n",
        "            #     return smoothed\n",
        "            # prediction = smooth_coords(prediction[:1])\n",
        "\n",
        "            result = parse_output_to_df(prediction[:1], seq, target_id)[0]\n",
        "\n",
        "            tm_score, transform = call_usalign(result, truth_df, verbose=0)\n",
        "            train_df.loc[i, 'protenix_tm_score'] = tm_score\n",
        "\n",
        "        except Exception as e:\n",
        "            # Optional debug: print(f\"⚠️ {target_id} 평가 실패: {e}\")\n",
        "            continue\n",
        "\n",
        "        if (time.time() - time0) > (12 * 3600 - 360):\n",
        "            print(\"⚠️ 실행 시간 한도 도달로 중단\")\n",
        "            break\n",
        "\n",
        "    # ✅ 결과 저장\n",
        "    train_df.to_csv('/kaggle/working/tm_scores.csv', index=False)\n",
        "\n",
        "    # ✅ 통계 출력 및 시각화\n",
        "    print(\"📈 평균 TM-score:\", train_df['protenix_tm_score'].astype(float).mean())\n",
        "    train_df['protenix_tm_score'].astype(float).hist(bins=50)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-20T01:31:58.466379Z",
          "iopub.execute_input": "2025-05-20T01:31:58.466683Z",
          "iopub.status.idle": "2025-05-20T01:31:58.726442Z",
          "shell.execute_reply.started": "2025-05-20T01:31:58.46666Z",
          "shell.execute_reply": "2025-05-20T01:31:58.725206Z"
        },
        "id": "bYlnfH74V4By",
        "outputId": "dc471b81-53a8-4403-f01d-ea30f822dbd1"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "  0%|          | 0/844 [00:00<?, ?it/s]\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-d23be3f808a8>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matom_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_error_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata_error_message\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-2ac652b2ebf6>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;31m# InferenceDataset이 제공하는 방식과 유사하게 반환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mprotenix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_data_pipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_input_to_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0minput_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matom_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_input_to_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdump_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_msa\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'convert_input_to_data' from 'protenix.data.infer_data_pipeline' (/usr/local/lib/python3.10/dist-packages/protenix/data/infer_data_pipeline.py)"
          ],
          "ename": "ImportError",
          "evalue": "cannot import name 'convert_input_to_data' from 'protenix.data.infer_data_pipeline' (/usr/local/lib/python3.10/dist-packages/protenix/data/infer_data_pipeline.py)",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "if MODEL_TYPE=='protenix' and not VALIDATION:\n",
        "    test_df=pd.read_csv('/kaggle/input/stanford-rna-3d-folding/test_sequences.csv')\n",
        "    import warnings\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "    dataset = DictDataset(test_df.sequence, dump_dir='output', id_list=test_df.target_id, use_msa=False)\n",
        "    num_data = len(dataset)\n",
        "    for i, seq in tqdm(enumerate(test_df.sequence),total=num_data):\n",
        "        try:\n",
        "            data, atom_array, data_error_message=dataset[i]\n",
        "            target_id = data[\"sample_name\"]\n",
        "            assert target_id==test_df.target_id[i]\n",
        "            assert data_error_message==''\n",
        "\n",
        "            new_configs = update_inference_configs(configs, data[\"N_token\"].item())\n",
        "            runner.update_model_configs(new_configs)\n",
        "            prediction = runner.predict(data)\n",
        "            prediction=prediction['coordinate'][:,data['input_feature_dict']['atom_to_tokatom_idx']==12]\n",
        "\n",
        "            result = parse_output_to_df(prediction, seq, target_id)[0]\n",
        "        except:\n",
        "            target_id==test_df.target_id[i]\n",
        "            print('Failed to predict', target_id)\n",
        "            result=pd.DataFrame(columns=['ID', 'resname', 'resid',\n",
        "                                         'x_1', 'y_1', 'z_1',\n",
        "                                         'x_2', 'y_2', 'z_2',\n",
        "                                         'x_3', 'y_3', 'z_3',\n",
        "                                         'x_4', 'y_4', 'z_4',\n",
        "                                         'x_5', 'y_5', 'z_5'],\n",
        "                                         data=[[target_id, x, j+1] + [0.0]*15 for j, x in enumerate(seq)])\n",
        "\n",
        "        result['ID']=result.apply(lambda x: x.ID + '_' + str(x.resid), axis=1)\n",
        "        result.to_csv('submission.csv', index=False, mode='a', header=(i==0))\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    display(pd.read_csv('submission.csv'))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-28T16:47:17.57656Z",
          "iopub.status.idle": "2025-03-28T16:47:17.576943Z",
          "shell.execute_reply": "2025-03-28T16:47:17.576784Z"
        },
        "id": "BhBUhEvJV4By"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}