{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 87793,
          "databundleVersionId": 12276181,
          "isSourceIdPinned": false,
          "sourceType": "competition"
        },
        {
          "sourceId": 10855324,
          "sourceType": "datasetVersion",
          "datasetId": 6742586
        },
        {
          "sourceId": 11118830,
          "sourceType": "datasetVersion",
          "datasetId": 6933267
        }
      ],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Randomness",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hanbin-git/kaggle/blob/main/Randomness.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "26EIi-yOM4qq"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "stanford_rna_3d_folding_path = kagglehub.competition_download('stanford-rna-3d-folding')\n",
        "metric_usalign_path = kagglehub.dataset_download('metric/usalign')\n",
        "geraseva_protenix_checkpoints_path = kagglehub.dataset_download('geraseva/protenix-checkpoints')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "iXUp6PBYM4qt"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_TYPE='protenix'\n",
        "VALIDATION=False"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-22T03:03:06.302003Z",
          "iopub.execute_input": "2025-05-22T03:03:06.302434Z",
          "iopub.status.idle": "2025-05-22T03:03:06.306119Z",
          "shell.execute_reply.started": "2025-05-22T03:03:06.302408Z",
          "shell.execute_reply": "2025-05-22T03:03:06.305066Z"
        },
        "id": "NhbFUJyMM4qu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip uninstall -y torch torchvision torchaudio protenix"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-22T03:00:09.754928Z",
          "iopub.execute_input": "2025-05-22T03:00:09.755211Z",
          "iopub.status.idle": "2025-05-22T03:00:12.010812Z",
          "shell.execute_reply.started": "2025-05-22T03:00:09.755188Z",
          "shell.execute_reply": "2025-05-22T03:00:12.00997Z"
        },
        "id": "TtRTt-daM4qu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version\n",
        "\n",
        "!nvidia-smi\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-22T03:00:16.514977Z",
          "iopub.execute_input": "2025-05-22T03:00:16.515266Z",
          "iopub.status.idle": "2025-05-22T03:00:16.858914Z",
          "shell.execute_reply.started": "2025-05-22T03:00:16.515243Z",
          "shell.execute_reply": "2025-05-22T03:00:16.858071Z"
        },
        "id": "shifMQIJM4qu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --extra-index-url https://download.pytorch.org/whl/cu118 torch==2.3.1+cu118"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-22T03:00:20.726437Z",
          "iopub.execute_input": "2025-05-22T03:00:20.726747Z",
          "iopub.status.idle": "2025-05-22T03:00:52.395048Z",
          "shell.execute_reply.started": "2025-05-22T03:00:20.726721Z",
          "shell.execute_reply": "2025-05-22T03:00:52.393678Z"
        },
        "id": "qYHQMkqqM4qv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --no-deps protenix==0.4.6 einops tqdm gemmi biotite==1.0.1"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-22T03:01:25.66547Z",
          "iopub.execute_input": "2025-05-22T03:01:25.665809Z",
          "iopub.status.idle": "2025-05-22T03:01:27.064291Z",
          "shell.execute_reply.started": "2025-05-22T03:01:25.66578Z",
          "shell.execute_reply": "2025-05-22T03:01:27.06348Z"
        },
        "id": "hFzIH2upM4qv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install biopython ml-collections rdkit"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-22T03:01:32.4637Z",
          "iopub.execute_input": "2025-05-22T03:01:32.464006Z",
          "iopub.status.idle": "2025-05-22T03:01:35.82879Z",
          "shell.execute_reply.started": "2025-05-22T03:01:32.463983Z",
          "shell.execute_reply": "2025-05-22T03:01:35.827675Z"
        },
        "id": "xPT7i3kGM4qw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install requirements"
      ],
      "metadata": {
        "id": "WQMrVIpHM4qw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if MODEL_TYPE=='protenix' and VALIDATION:\n",
        "    !pip install biopython\n",
        "    !pip install ml-collections\n",
        "    !pip install rdkit\n",
        "!export PROTENIX_DATA_ROOT_DIR=/kaggle/input/protenix-checkpoints"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-22T03:01:59.632924Z",
          "iopub.execute_input": "2025-05-22T03:01:59.633237Z",
          "iopub.status.idle": "2025-05-22T03:02:10.028536Z",
          "shell.execute_reply.started": "2025-05-22T03:01:59.63321Z",
          "shell.execute_reply": "2025-05-22T03:02:10.027489Z"
        },
        "id": "xAhItjbVM4qx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir /af3-dev\n",
        "! ln -s /kaggle/input/protenix-checkpoints /af3-dev/release_data\n",
        "! ls /af3-dev/release_data/"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-22T03:02:39.633296Z",
          "iopub.execute_input": "2025-05-22T03:02:39.633596Z",
          "iopub.status.idle": "2025-05-22T03:02:39.974925Z",
          "shell.execute_reply.started": "2025-05-22T03:02:39.633571Z",
          "shell.execute_reply": "2025-05-22T03:02:39.974107Z"
        },
        "id": "44ZWMMD2M4qx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper scripts"
      ],
      "metadata": {
        "id": "KN0pJOVtM4qy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import Bio\n",
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "import pandas as pd\n",
        "from Bio.PDB import Atom, Model, Chain, Residue, Structure, PDBParser\n",
        "from Bio import SeqIO\n",
        "import os, sys\n",
        "import re\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "time0=time.time()\n",
        "\n",
        "print('IMPORT OK !!!!')"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2025-05-22T03:02:41.751735Z",
          "iopub.execute_input": "2025-05-22T03:02:41.752215Z",
          "iopub.status.idle": "2025-05-22T03:02:43.80947Z",
          "shell.execute_reply.started": "2025-05-22T03:02:41.752173Z",
          "shell.execute_reply": "2025-05-22T03:02:43.808266Z"
        },
        "trusted": true,
        "id": "tpOx782dM4qy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "PYTHON = sys.executable\n",
        "print('PYTHON',PYTHON)\n",
        "\n",
        "RHONET_DIR=\\\n",
        "'/kaggle/input/data-for-demo-for-rhofold-plus-with-kaggle-msa/RhoFold-main'\n",
        "#'<your downloaded rhofold repo>/RhoFold-main'\n",
        "\n",
        "USALIGN = \\\n",
        "'/kaggle/working//USalign'\n",
        "#'<your us align path>/USalign'\n",
        "\n",
        "os.system('cp /kaggle/input/usalign/USalign /kaggle/working/')\n",
        "os.system('sudo chmod u+x /kaggle/working//USalign')\n",
        "sys.path.append(RHONET_DIR)\n",
        "\n",
        "\n",
        "DATA_KAGGLE_DIR = '/kaggle/input/stanford-rna-3d-folding'\n",
        "\n",
        "\n",
        "# helper ----\n",
        "class dotdict(dict):\n",
        "\t__setattr__ = dict.__setitem__\n",
        "\t__delattr__ = dict.__delitem__\n",
        "\n",
        "\tdef __getattr__(self, name):\n",
        "\t\ttry:\n",
        "\t\t\treturn self[name]\n",
        "\t\texcept KeyError:\n",
        "\t\t\traise AttributeError(name)\n",
        "\n",
        "# visualisation helper ----\n",
        "def set_aspect_equal(ax):\n",
        "\tx_limits = ax.get_xlim()\n",
        "\ty_limits = ax.get_ylim()\n",
        "\tz_limits = ax.get_zlim()\n",
        "\n",
        "\t# Compute the mean of each axis\n",
        "\tx_middle = np.mean(x_limits)\n",
        "\ty_middle = np.mean(y_limits)\n",
        "\tz_middle = np.mean(z_limits)\n",
        "\n",
        "\t# Compute the max range across all axes\n",
        "\tmax_range = max(x_limits[1] - x_limits[0],\n",
        "\t\t\t\t\ty_limits[1] - y_limits[0],\n",
        "\t\t\t\t\tz_limits[1] - z_limits[0]) / 2.0\n",
        "\n",
        "\t# Set the new limits to ensure equal scaling\n",
        "\tax.set_xlim(x_middle - max_range, x_middle + max_range)\n",
        "\tax.set_ylim(y_middle - max_range, y_middle + max_range)\n",
        "\tax.set_zlim(z_middle - max_range, z_middle + max_range)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-20T07:33:53.178245Z",
          "iopub.execute_input": "2025-05-20T07:33:53.178532Z",
          "iopub.status.idle": "2025-05-20T07:33:53.203314Z",
          "shell.execute_reply.started": "2025-05-20T07:33:53.178509Z",
          "shell.execute_reply": "2025-05-20T07:33:53.202387Z"
        },
        "id": "gCnJoR6WM4qy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# xyz df helper --------------------\n",
        "def get_truth_df(target_id):\n",
        "    truth_df = LABEL_DF[LABEL_DF['target_id'] == target_id]\n",
        "    truth_df = truth_df.reset_index(drop=True)\n",
        "    return truth_df\n",
        "\n",
        "def parse_output_to_df(output, seq, target_id):\n",
        "    df = []\n",
        "    chain_data = []\n",
        "    for i, res in enumerate(seq):\n",
        "        d=dict(ID = target_id,\n",
        "                    resname=res,\n",
        "                    resid=i+1)\n",
        "        for n in range(len(output)):\n",
        "            d={**d, f'x_{n+1}': round(output[n,i,0].item(),3),\n",
        "                     f'y_{n+1}': round(output[n,i,1].item(),3),\n",
        "                     f'z_{n+1}': round(output[n,i,2].item(),3)}\n",
        "        chain_data.append(d)\n",
        "\n",
        "    if len(chain_data)!=0:\n",
        "        chain_df = pd.DataFrame(chain_data)\n",
        "        df.append(chain_df)\n",
        "        ##print(chain_df)\n",
        "    return df\n",
        "\n",
        "\n",
        "def parse_pdb_to_df(pdb_file, target_id):\n",
        "    parser = PDBParser()\n",
        "    structure = parser.get_structure('', pdb_file)\n",
        "\n",
        "    df = []\n",
        "    for model in structure:\n",
        "        for chain in model:\n",
        "            print(chain)\n",
        "            chain_data = []\n",
        "            for residue in chain:\n",
        "                # print(residue)\n",
        "                if residue.get_resname() in ['A', 'U', 'G', 'C']:\n",
        "                    # Check if the residue has a C1' atom\n",
        "                    if 'C1\\'' in residue:\n",
        "                        atom = residue['C1\\'']\n",
        "                        xyz = atom.get_coord()\n",
        "                        resname = residue.get_resname()\n",
        "                        resid = residue.get_id()[1]\n",
        "\n",
        "                        #todo detect discontinous: resid = prev_resid+1\n",
        "                        #ID\tresname\tresid\tx_1\ty_1\tz_1\n",
        "                        chain_data.append(dict(\n",
        "                            ID = target_id+'_'+str(resid),\n",
        "                            resname=resname,\n",
        "                            resid=resid,\n",
        "                            x_1=xyz[0],\n",
        "                            y_1=xyz[1],\n",
        "                            z_1=xyz[2],\n",
        "                        ))\n",
        "                        ##print(f\"Residue {resname} {resid}, Atom: {atom.get_name()}, xyz: {xyz}\")\n",
        "\n",
        "            if len(chain_data)!=0:\n",
        "                chain_df = pd.DataFrame(chain_data)\n",
        "                df.append(chain_df)\n",
        "                ##print(chain_df)\n",
        "    return df"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-20T07:33:55.874239Z",
          "iopub.execute_input": "2025-05-20T07:33:55.874515Z",
          "iopub.status.idle": "2025-05-20T07:33:55.882929Z",
          "shell.execute_reply.started": "2025-05-20T07:33:55.874492Z",
          "shell.execute_reply": "2025-05-20T07:33:55.88198Z"
        },
        "id": "hjBLfKsqM4qy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# usalign helper --------------------\n",
        "def write_target_line(\n",
        "    atom_name, atom_serial, residue_name, chain_id, residue_num, x_coord, y_coord, z_coord, occupancy=1.0, b_factor=0.0, atom_type='P'\n",
        "):\n",
        "    \"\"\"\n",
        "    Writes a single line of PDB format based on provided atom information.\n",
        "\n",
        "    Args:\n",
        "        atom_name (str): Name of the atom (e.g., \"N\", \"CA\").\n",
        "        atom_serial (int): Atom serial number.\n",
        "        residue_name (str): Residue name (e.g., \"ALA\").\n",
        "        chain_id (str): Chain identifier.\n",
        "        residue_num (int): Residue number.\n",
        "        x_coord (float): X coordinate.\n",
        "        y_coord (float): Y coordinate.\n",
        "        z_coord (float): Z coordinate.\n",
        "        occupancy (float, optional): Occupancy value (default: 1.0).\n",
        "        b_factor (float, optional): B-factor value (default: 0.0).\n",
        "\n",
        "    Returns:\n",
        "        str: A single line of PDB string.\n",
        "    \"\"\"\n",
        "    return f'ATOM  {atom_serial:>5d}  {atom_name:<5s} {residue_name:<3s} {residue_num:>3d}    {x_coord:>8.3f}{y_coord:>8.3f}{z_coord:>8.3f}{occupancy:>6.2f}{b_factor:>6.2f}           {atom_type}\\n'\n",
        "\n",
        "def write_xyz_to_pdb(df, pdb_file, xyz_id = 1):\n",
        "    resolved_cnt = 0\n",
        "    with open(pdb_file, 'w') as target_file:\n",
        "        for _, row in df.iterrows():\n",
        "            x_coord = row[f'x_{xyz_id}']\n",
        "            y_coord = row[f'y_{xyz_id}']\n",
        "            z_coord = row[f'z_{xyz_id}']\n",
        "\n",
        "            if x_coord > -1e17 and y_coord > -1e17 and z_coord > -1e17:\n",
        "                resolved_cnt += 1\n",
        "                target_line = write_target_line(\n",
        "                    atom_name=\"C1'\",\n",
        "                    atom_serial=int(row['resid']),\n",
        "                    residue_name=row['resname'],\n",
        "                    chain_id='0',\n",
        "                    residue_num=int(row['resid']),\n",
        "                    x_coord=x_coord,\n",
        "                    y_coord=y_coord,\n",
        "                    z_coord=z_coord,\n",
        "                    atom_type='C',\n",
        "                )\n",
        "                target_file.write(target_line)\n",
        "    return resolved_cnt\n",
        "\n",
        "\n",
        "def parse_usalign_for_tm_score(output):\n",
        "    # Extract TM-score based on length of reference structure (second)\n",
        "    tm_score_match = re.findall(r'TM-score=\\s+([\\d.]+)', output)[1]\n",
        "    if not tm_score_match:\n",
        "        raise ValueError('No TM score found')\n",
        "    return float(tm_score_match)\n",
        "\n",
        "def parse_usalign_for_transform(output):\n",
        "    # Locate the rotation matrix section\n",
        "    matrix_lines = []\n",
        "    found_matrix = False\n",
        "\n",
        "    for line in output.splitlines():\n",
        "        if \"The rotation matrix to rotate Structure_1 to Structure_2\" in line:\n",
        "            found_matrix = True\n",
        "        elif found_matrix and re.match(r'^\\d+\\s+[-\\d.]+\\s+[-\\d.]+\\s+[-\\d.]+\\s+[-\\d.]+$', line):\n",
        "            matrix_lines.append(line)\n",
        "        elif found_matrix and not line.strip():\n",
        "            break  # Stop parsing if an empty line is encountered after the matrix\n",
        "\n",
        "    # Parse the rotation matrix values\n",
        "    rotation_matrix = []\n",
        "    for line in matrix_lines:\n",
        "        parts = line.split()\n",
        "        row_values = list(map(float, parts[1:]))  # Skip the first column (index)\n",
        "        rotation_matrix.append(row_values)\n",
        "\n",
        "    return np.array(rotation_matrix)\n",
        "\n",
        "def call_usalign(predict_df, truth_df, verbose=1):\n",
        "    truth_pdb = '~truth.pdb'\n",
        "    predict_pdb = '~predict.pdb'\n",
        "    write_xyz_to_pdb(predict_df, predict_pdb, xyz_id=1)\n",
        "    write_xyz_to_pdb(truth_df, truth_pdb, xyz_id=1)\n",
        "\n",
        "    command = f'{USALIGN} {predict_pdb} {truth_pdb} -atom \" C1\\'\" -m -'\n",
        "    output = os.popen(command).read()\n",
        "    if verbose==1:\n",
        "        print(output)\n",
        "    tm_score = parse_usalign_for_tm_score(output)\n",
        "    transform = parse_usalign_for_transform(output)\n",
        "    return tm_score, transform\n",
        "\n",
        "print('HELPER OK!!!')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-20T07:33:58.690681Z",
          "iopub.execute_input": "2025-05-20T07:33:58.691184Z",
          "iopub.status.idle": "2025-05-20T07:33:58.704252Z",
          "shell.execute_reply.started": "2025-05-20T07:33:58.691136Z",
          "shell.execute_reply": "2025-05-20T07:33:58.703171Z"
        },
        "id": "tz-d8WJsM4qz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if MODEL_TYPE=='protenix':\n",
        "\n",
        "\n",
        "    from runner.batch_inference import get_default_runner\n",
        "    from runner.inference import update_inference_configs, InferenceRunner\n",
        "\n",
        "    from protenix.data.infer_data_pipeline import InferenceDataset\n",
        "\n",
        "    np.random.seed(0)\n",
        "    torch.random.manual_seed(0)\n",
        "    torch.cuda.manual_seed_all(0)\n",
        "\n",
        "    class DictDataset(InferenceDataset):\n",
        "        def __init__(\n",
        "            self,\n",
        "            seq_list: list,\n",
        "            dump_dir: str,\n",
        "            id_list: list = None,\n",
        "            use_msa: bool = False,\n",
        "        ) -> None:\n",
        "\n",
        "            self.dump_dir = dump_dir\n",
        "            self.use_msa = use_msa\n",
        "            if isinstance(id_list,type(None)):\n",
        "                self.inputs = [{\"sequences\":\n",
        "                                [{\"rnaSequence\":\n",
        "                                  {\"sequence\": seq,\n",
        "                                   \"count\": 1}}],\n",
        "                                \"name\": \"query\"} for seq in seq_list]\n",
        "            else:\n",
        "                self.inputs = [{\"sequences\":\n",
        "                                [{\"rnaSequence\":\n",
        "                                  {\"sequence\": seq,\n",
        "                                   \"count\": 1}}],\n",
        "                                \"name\": i} for i, seq in zip(id_list,seq_list)]\n",
        "\n",
        "\n",
        "if MODEL_TYPE=='protenix':\n",
        "\n",
        "    from configs.configs_base import configs as configs_base\n",
        "    from configs.configs_data import data_configs\n",
        "    from configs.configs_inference import inference_configs\n",
        "    from protenix.config.config import parse_configs\n",
        "\n",
        "    configs_base[\"use_deepspeed_evo_attention\"] = (\n",
        "    os.environ.get(\"USE_DEEPSPEED_EVO_ATTENTION\", False) == \"true\")\n",
        "    configs_base[\"model\"][\"N_cycle\"] = 10 #10\n",
        "    configs_base[\"sample_diffusion\"][\"N_sample\"] = (1 if VALIDATION else 10)\n",
        "    configs_base[\"sample_diffusion\"][\"N_step\"] = 200\n",
        "    inference_configs['load_checkpoint_path']='/kaggle/input/protenix-checkpoints/model_v0.2.0.pt'\n",
        "    configs = {**configs_base, **{\"data\": data_configs}, **inference_configs}\n",
        "\n",
        "    configs = parse_configs(\n",
        "            configs=configs,\n",
        "            fill_required_with_null=True,\n",
        "        )\n",
        "\n",
        "    runner=InferenceRunner(configs)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-20T07:34:03.193185Z",
          "iopub.execute_input": "2025-05-20T07:34:03.193507Z",
          "iopub.status.idle": "2025-05-20T07:34:03.201541Z",
          "shell.execute_reply.started": "2025-05-20T07:34:03.193477Z",
          "shell.execute_reply": "2025-05-20T07:34:03.200865Z"
        },
        "id": "8f9EAV2cM4qz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if VALIDATION:\n",
        "    LABEL_DF = pd.read_csv('/kaggle/input/stanford-rna-3d-folding/train_labels.csv')\n",
        "    LABEL_DF['target_id'] = LABEL_DF['ID'].apply(lambda x: '_'.join(x.split('_')[:-1]))\n",
        "    train_df=pd.read_csv('/kaggle/input/stanford-rna-3d-folding/train_sequences.csv')\n",
        "\n",
        "\n",
        "\n",
        "if MODEL_TYPE=='protenix' and VALIDATION:\n",
        "    import warnings\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "    train_df['protenix_tm_score']=None\n",
        "    dataset = DictDataset(train_df.sequence, dump_dir='output', id_list=train_df.target_id, use_msa=False)\n",
        "    num_data = len(dataset)\n",
        "    for i, seq in tqdm(enumerate(train_df.sequence),total=num_data):\n",
        "        if train_df.loc[i,'protenix_tm_score']!=None:\n",
        "            continue\n",
        "        if len(seq)>300:\n",
        "            continue\n",
        "        target_id = train_df.loc[i,'target_id']\n",
        "        truth_df = get_truth_df(target_id)\n",
        "        if sum(~np.isnan(truth_df.x_1))<3:\n",
        "            continue\n",
        "        data, atom_array, data_error_message=dataset[i]\n",
        "        if data_error_message!='':\n",
        "            continue\n",
        "        new_configs = update_inference_configs(configs, data[\"N_token\"].item())\n",
        "        runner.update_model_configs(new_configs)\n",
        "        prediction = runner.predict(data)\n",
        "        coords = prediction['coordinate'][:, data['input_feature_dict']['atom_to_tokatom_idx'] == 12]  # shape: [N, L, 3]\n",
        "        coords = coords.mean(dim=0, keepdim=True)  # shape: [1, L, 3]\n",
        "        coords = savgol_smooth_coords(coords)  # 선택적으로 smoothing 적용\n",
        "        result = parse_output_to_df(coords, seq, target_id)[0]\n",
        "        try:\n",
        "            tm_score, transform = call_usalign(result, truth_df, verbose=0)\n",
        "            train_df.loc[i,'protenix_tm_score']=tm_score\n",
        "        except:\n",
        "            pass\n",
        "        if (time.time()-time0)>(12*3600-360):\n",
        "            break\n",
        "    train_df.to_csv('tm_scores.csv', index=False)\n",
        "    print(train_df.protenix_tm_score.mean())\n",
        "    display(train_df.protenix_tm_score.hist())\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "6wZIz0ifM4qz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if MODEL_TYPE=='protenix' and not VALIDATION:\n",
        "    test_df=pd.read_csv('/kaggle/input/stanford-rna-3d-folding/test_sequences.csv')\n",
        "    import warnings\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "    dataset = DictDataset(test_df.sequence, dump_dir='output', id_list=test_df.target_id, use_msa=False)\n",
        "    num_data = len(dataset)\n",
        "    for i, seq in tqdm(enumerate(test_df.sequence),total=num_data):\n",
        "        try:\n",
        "            data, atom_array, data_error_message=dataset[i]\n",
        "            target_id = data[\"sample_name\"]\n",
        "            assert target_id==test_df.target_id[i]\n",
        "            assert data_error_message==''\n",
        "\n",
        "            new_configs = update_inference_configs(configs, data[\"N_token\"].item())\n",
        "            runner.update_model_configs(new_configs)\n",
        "            prediction = runner.predict(data)\n",
        "            coords = prediction['coordinate'][:, data['input_feature_dict']['atom_to_tokatom_idx'] == 12]  # [N, L, 3]\n",
        "            coords = coords.mean(dim=0, keepdim=True)  # 앙상블 평균 → [1, L, 3]\n",
        "            coords = savgol_smooth_coords(coords)  # 선택적 smoothing 적용\n",
        "            result = parse_output_to_df(coords, seq, target_id)[0]  # 최종 결과\n",
        "        except:\n",
        "            target_id==test_df.target_id[i]\n",
        "            print('Failed to predict', target_id)\n",
        "            result=pd.DataFrame(columns=['ID', 'resname', 'resid',\n",
        "                                         'x_1', 'y_1', 'z_1',\n",
        "                                         'x_2', 'y_2', 'z_2',\n",
        "                                         'x_3', 'y_3', 'z_3',\n",
        "                                         'x_4', 'y_4', 'z_4',\n",
        "                                         'x_5', 'y_5', 'z_5'],\n",
        "                                         data=[[target_id, x, j+1] + [0.0]*15 for j, x in enumerate(seq)])\n",
        "\n",
        "        result['ID']=result.apply(lambda x: x.ID + '_' + str(x.resid), axis=1)\n",
        "        result.to_csv('submission.csv', index=False, mode='a', header=(i==0))\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    display(pd.read_csv('submission.csv'))\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "tS2-8Df2M4q0"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}